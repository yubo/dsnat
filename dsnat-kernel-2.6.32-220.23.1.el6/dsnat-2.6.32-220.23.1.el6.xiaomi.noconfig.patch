diff -uprN linux-2.6.32-220.23.1.el6.x86_64.bak/include/linux/ip_vs.h linux-2.6.32-220.23.1.el6.x86_64/include/linux/ip_vs.h
--- linux-2.6.32-220.23.1.el6.x86_64.bak/include/linux/ip_vs.h	2012-06-12 21:31:32.000000000 +0800
+++ linux-2.6.32-220.23.1.el6.x86_64/include/linux/ip_vs.h	2013-03-15 18:15:38.000000000 +0800
@@ -1,6 +1,9 @@
 /*
  *      IP Virtual Server
  *      data structure and functionality definitions
+ *
+ * Changes:
+ *   Yu Bo        <yubo@xiaomi.com>
  */
 
 #ifndef _IP_VS_H
@@ -17,29 +20,29 @@
 /*
  *      Virtual Service Flags
  */
-#define IP_VS_SVC_F_PERSISTENT	0x0001		/* persistent port */
-#define IP_VS_SVC_F_HASHED	0x0002		/* hashed entry */
-#define IP_VS_SVC_F_ONEPACKET	0x0004		/* one-packet scheduling */
+#define IP_VS_SVC_F_PERSISTENT	0x0001	/* persistent port */
+#define IP_VS_SVC_F_HASHED	0x0002	/* hashed entry */
+#define IP_VS_SVC_F_ONEPACKET	0x0004	/* one-packet scheduling */
 
 /*
  *      Destination Server Flags
  */
-#define IP_VS_DEST_F_AVAILABLE	0x0001		/* server is available */
-#define IP_VS_DEST_F_OVERLOAD	0x0002		/* server is overloaded */
+#define IP_VS_DEST_F_AVAILABLE	0x0001	/* server is available */
+#define IP_VS_DEST_F_OVERLOAD	0x0002	/* server is overloaded */
 
 /*
  *      IPVS sync daemon states
  */
-#define IP_VS_STATE_NONE	0x0000		/* daemon is stopped */
-#define IP_VS_STATE_MASTER	0x0001		/* started as master */
-#define IP_VS_STATE_BACKUP	0x0002		/* started as backup */
+#define IP_VS_STATE_NONE	0x0000	/* daemon is stopped */
+#define IP_VS_STATE_MASTER	0x0001	/* started as master */
+#define IP_VS_STATE_BACKUP	0x0002	/* started as backup */
 
 /*
  *      IPVS socket options
  */
-#define IP_VS_BASE_CTL		(64+1024+64)		/* base */
+#define IP_VS_BASE_CTL		(64+1024+64)	/* base */
 
-#define IP_VS_SO_SET_NONE	IP_VS_BASE_CTL		/* just peek */
+#define IP_VS_SO_SET_NONE	IP_VS_BASE_CTL	/* just peek */
 #define IP_VS_SO_SET_INSERT	(IP_VS_BASE_CTL+1)
 #define IP_VS_SO_SET_ADD	(IP_VS_BASE_CTL+2)
 #define IP_VS_SO_SET_EDIT	(IP_VS_BASE_CTL+3)
@@ -55,7 +58,9 @@
 #define IP_VS_SO_SET_RESTORE    (IP_VS_BASE_CTL+13)
 #define IP_VS_SO_SET_SAVE       (IP_VS_BASE_CTL+14)
 #define IP_VS_SO_SET_ZERO	(IP_VS_BASE_CTL+15)
-#define IP_VS_SO_SET_MAX	IP_VS_SO_SET_ZERO
+#define IP_VS_SO_SET_ADDLADDR	(IP_VS_BASE_CTL+16)
+#define IP_VS_SO_SET_DELLADDR	(IP_VS_BASE_CTL+17)
+#define IP_VS_SO_SET_MAX	IP_VS_SO_SET_DELLADDR
 
 #define IP_VS_SO_GET_VERSION	IP_VS_BASE_CTL
 #define IP_VS_SO_GET_INFO	(IP_VS_BASE_CTL+1)
@@ -65,183 +70,206 @@
 #define IP_VS_SO_GET_DEST	(IP_VS_BASE_CTL+5)	/* not used now */
 #define IP_VS_SO_GET_TIMEOUT	(IP_VS_BASE_CTL+6)
 #define IP_VS_SO_GET_DAEMON	(IP_VS_BASE_CTL+7)
-#define IP_VS_SO_GET_MAX	IP_VS_SO_GET_DAEMON
-
+#define IP_VS_SO_GET_LADDRS	(IP_VS_BASE_CTL+8)
+#define IP_VS_SO_GET_MAX	IP_VS_SO_GET_LADDRS
 
 /*
  *      IPVS Connection Flags
  */
-#define IP_VS_CONN_F_FWD_MASK	0x0007		/* mask for the fwd methods */
-#define IP_VS_CONN_F_MASQ	0x0000		/* masquerading/NAT */
-#define IP_VS_CONN_F_LOCALNODE	0x0001		/* local node */
-#define IP_VS_CONN_F_TUNNEL	0x0002		/* tunneling */
-#define IP_VS_CONN_F_DROUTE	0x0003		/* direct routing */
-#define IP_VS_CONN_F_BYPASS	0x0004		/* cache bypass */
-#define IP_VS_CONN_F_SYNC	0x0020		/* entry created by sync */
-#define IP_VS_CONN_F_HASHED	0x0040		/* hashed entry */
-#define IP_VS_CONN_F_NOOUTPUT	0x0080		/* no output packets */
-#define IP_VS_CONN_F_INACTIVE	0x0100		/* not established */
-#define IP_VS_CONN_F_OUT_SEQ	0x0200		/* must do output seq adjust */
-#define IP_VS_CONN_F_IN_SEQ	0x0400		/* must do input seq adjust */
-#define IP_VS_CONN_F_SEQ_MASK	0x0600		/* in/out sequence mask */
-#define IP_VS_CONN_F_NO_CPORT	0x0800		/* no client port set yet */
-#define IP_VS_CONN_F_TEMPLATE	0x1000		/* template, not connection */
-#define IP_VS_CONN_F_ONE_PACKET	0x2000		/* forward only one packet */
+#define IP_VS_CONN_F_FWD_MASK	0x0007	/* mask for the fwd methods */
+#define IP_VS_CONN_F_MASQ	0x0000	/* masquerading/NAT */
+#define IP_VS_CONN_F_LOCALNODE	0x0001	/* local node */
+#define IP_VS_CONN_F_TUNNEL	0x0002	/* tunneling */
+#define IP_VS_CONN_F_DROUTE	0x0003	/* direct routing */
+#define IP_VS_CONN_F_BYPASS	0x0004	/* cache bypass */
+#define IP_VS_CONN_F_FULLNAT	0x0005	/* full nat */
+#define IP_VS_CONN_F_SYNC	0x0020	/* entry created by sync */
+#define IP_VS_CONN_F_HASHED	0x0040	/* hashed entry */
+#define IP_VS_CONN_F_NOOUTPUT	0x0080	/* no output packets */
+#define IP_VS_CONN_F_INACTIVE	0x0100	/* not established */
+#define IP_VS_CONN_F_OUT_SEQ	0x0200	/* must do output seq adjust */
+#define IP_VS_CONN_F_IN_SEQ	0x0400	/* must do input seq adjust */
+#define IP_VS_CONN_F_SEQ_MASK	0x0600	/* in/out sequence mask */
+#define IP_VS_CONN_F_NO_CPORT	0x0800	/* no client port set yet */
+#define IP_VS_CONN_F_TEMPLATE	0x1000	/* template, not connection */
+#define IP_VS_CONN_F_ONE_PACKET	0x2000	/* forward only one packet */
+#define IP_VS_CONN_F_CIP_INSERTED 0x4000	/* client ip address has inserted */
+#define IP_VS_CONN_F_SYNPROXY	0x8000	/* syn proxy flag */
+#define IP_VS_CONN_F_DSNAT	0x010000
+
 
 #define IP_VS_SCHEDNAME_MAXLEN	16
 #define IP_VS_IFNAME_MAXLEN	16
 
-
 /*
  *	The struct ip_vs_service_user and struct ip_vs_dest_user are
  *	used to set IPVS rules through setsockopt.
  */
 struct ip_vs_service_user {
 	/* virtual service addresses */
-	__u16		protocol;
-	__be32			addr;		/* virtual ip address */
-	__be16			port;
-	__u32		fwmark;		/* firwall mark of service */
+	__u16 protocol;
+	__be32 addr;		/* virtual ip address */
+	__be16 port;
+	__u32 fwmark;		/* firwall mark of service */
 
 	/* virtual service options */
-	char			sched_name[IP_VS_SCHEDNAME_MAXLEN];
-	unsigned		flags;		/* virtual service flags */
-	unsigned		timeout;	/* persistent timeout in sec */
-	__be32			netmask;	/* persistent netmask */
+	char sched_name[IP_VS_SCHEDNAME_MAXLEN];
+	unsigned flags;		/* virtual service flags */
+	unsigned timeout;	/* persistent timeout in sec */
+	__be32 netmask;		/* persistent netmask */
 };
 
-
 struct ip_vs_dest_user {
 	/* destination server address */
-	__be32			addr;
-	__be16			port;
+	__be32 addr;
+	__be16 port;
 
 	/* real server options */
-	unsigned		conn_flags;	/* connection flags */
-	int			weight;		/* destination weight */
+	unsigned conn_flags;	/* connection flags */
+	int weight;		/* destination weight */
 
 	/* thresholds for active connections */
-	__u32		u_threshold;	/* upper threshold */
-	__u32		l_threshold;	/* lower threshold */
+	__u32 u_threshold;	/* upper threshold */
+	__u32 l_threshold;	/* lower threshold */
 };
 
+struct ip_vs_laddr_user {
+	__be32 addr;		/* ipv4 address */
+};
 
 /*
  *	IPVS statistics object (for user space)
  */
-struct ip_vs_stats_user
-{
-	__u32                   conns;          /* connections scheduled */
-	__u32                   inpkts;         /* incoming packets */
-	__u32                   outpkts;        /* outgoing packets */
-	__u64                   inbytes;        /* incoming bytes */
-	__u64                   outbytes;       /* outgoing bytes */
-
-	__u32			cps;		/* current connection rate */
-	__u32			inpps;		/* current in packet rate */
-	__u32			outpps;		/* current out packet rate */
-	__u32			inbps;		/* current in byte rate */
-	__u32			outbps;		/* current out byte rate */
+struct ip_vs_stats_user {
+	__u64 conns;		/* connections scheduled */
+	__u64 inpkts;		/* incoming packets */
+	__u64 outpkts;		/* outgoing packets */
+	__u64 inbytes;		/* incoming bytes */
+	__u64 outbytes;		/* outgoing bytes */
+
+	__u32 cps;		/* current connection rate */
+	__u32 inpps;		/* current in packet rate */
+	__u32 outpps;		/* current out packet rate */
+	__u32 inbps;		/* current in byte rate */
+	__u32 outbps;		/* current out byte rate */
 };
 
-
 /* The argument to IP_VS_SO_GET_INFO */
 struct ip_vs_getinfo {
 	/* version number */
-	unsigned int		version;
+	unsigned int version;
 
 	/* size of connection hash table */
-	unsigned int		size;
+	unsigned int size;
 
 	/* number of virtual services */
-	unsigned int		num_services;
+	unsigned int num_services;
 };
 
-
 /* The argument to IP_VS_SO_GET_SERVICE */
 struct ip_vs_service_entry {
 	/* which service: user fills in these */
-	__u16		protocol;
-	__be32			addr;		/* virtual address */
-	__be16			port;
-	__u32		fwmark;		/* firwall mark of service */
+	__u16 protocol;
+	__be32 addr;		/* virtual address */
+	__be16 port;
+	__u32 fwmark;		/* firwall mark of service */
 
 	/* service options */
-	char			sched_name[IP_VS_SCHEDNAME_MAXLEN];
-	unsigned		flags;          /* virtual service flags */
-	unsigned		timeout;	/* persistent timeout */
-	__be32			netmask;	/* persistent netmask */
+	char sched_name[IP_VS_SCHEDNAME_MAXLEN];
+	unsigned flags;		/* virtual service flags */
+	unsigned timeout;	/* persistent timeout */
+	__be32 netmask;		/* persistent netmask */
 
 	/* number of real servers */
-	unsigned int		num_dests;
+	unsigned int num_dests;
+
+	/* number of local address */
+	unsigned int num_laddrs;
 
 	/* statistics */
 	struct ip_vs_stats_user stats;
 };
 
-
 struct ip_vs_dest_entry {
-	__be32			addr;		/* destination address */
-	__be16			port;
-	unsigned		conn_flags;	/* connection flags */
-	int			weight;		/* destination weight */
-
-	__u32		u_threshold;	/* upper threshold */
-	__u32		l_threshold;	/* lower threshold */
-
-	__u32		activeconns;	/* active connections */
-	__u32		inactconns;	/* inactive connections */
-	__u32		persistconns;	/* persistent connections */
+	__be32 addr;		/* destination address */
+	__be16 port;
+	unsigned conn_flags;	/* connection flags */
+	int weight;		/* destination weight */
+
+	__u32 u_threshold;	/* upper threshold */
+	__u32 l_threshold;	/* lower threshold */
+
+	__u32 activeconns;	/* active connections */
+	__u32 inactconns;	/* inactive connections */
+	__u32 persistconns;	/* persistent connections */
 
 	/* statistics */
 	struct ip_vs_stats_user stats;
 };
 
+struct ip_vs_laddr_entry {
+	__be32 addr;		/* ipv4 address */
+
+	__u64 port_conflict;	/* conflict counts */
+	__u32 conn_counts;	/* current connects */
+};
 
 /* The argument to IP_VS_SO_GET_DESTS */
 struct ip_vs_get_dests {
 	/* which service: user fills in these */
-	__u16		protocol;
-	__be32			addr;		/* virtual address */
-	__be16			port;
-	__u32		fwmark;		/* firwall mark of service */
+	__u16 protocol;
+	__be32 addr;		/* virtual address */
+	__be16 port;
+	__u32 fwmark;		/* firwall mark of service */
 
 	/* number of real servers */
-	unsigned int		num_dests;
+	unsigned int num_dests;
 
 	/* the real servers */
-	struct ip_vs_dest_entry	entrytable[0];
+	struct ip_vs_dest_entry entrytable[0];
 };
 
+/* The argument to IP_VS_SO_GET_LADDRS */
+struct ip_vs_get_laddrs {
+	/* which service: user fills in these */
+	__u16 protocol;
+	__be32 addr;		/* virtual address */
+	__be16 port;
+	__u32 fwmark;		/* firwall mark of service */
+
+	/* number of local address */
+	unsigned int num_laddrs;
+
+	/* the real servers */
+	struct ip_vs_laddr_entry entrytable[0];
+};
 
 /* The argument to IP_VS_SO_GET_SERVICES */
 struct ip_vs_get_services {
 	/* number of virtual services */
-	unsigned int		num_services;
+	unsigned int num_services;
 
 	/* service table */
 	struct ip_vs_service_entry entrytable[0];
 };
 
-
 /* The argument to IP_VS_SO_GET_TIMEOUT */
 struct ip_vs_timeout_user {
-	int			tcp_timeout;
-	int			tcp_fin_timeout;
-	int			udp_timeout;
+	int tcp_timeout;
+	int tcp_fin_timeout;
+	int udp_timeout;
 };
 
-
 /* The argument to IP_VS_SO_GET_DAEMON */
 struct ip_vs_daemon_user {
 	/* sync daemon state (master/backup) */
-	int			state;
+	int state;
 
 	/* multicast interface name */
-	char			mcast_ifn[IP_VS_IFNAME_MAXLEN];
+	char mcast_ifn[IP_VS_IFNAME_MAXLEN];
 
 	/* SyncID we belong to */
-	int			syncid;
+	int syncid;
 };
 
 /*
@@ -260,32 +288,40 @@ struct ip_vs_flags {
 	__be32 mask;
 };
 
-/* Generic Netlink command attributes */
-enum {
-	IPVS_CMD_UNSPEC = 0,
 
-	IPVS_CMD_NEW_SERVICE,		/* add service */
-	IPVS_CMD_SET_SERVICE,		/* modify service */
-	IPVS_CMD_DEL_SERVICE,		/* delete service */
-	IPVS_CMD_GET_SERVICE,		/* get service info */
-
-	IPVS_CMD_NEW_DEST,		/* add destination */
-	IPVS_CMD_SET_DEST,		/* modify destination */
-	IPVS_CMD_DEL_DEST,		/* delete destination */
-	IPVS_CMD_GET_DEST,		/* get destination info */
-
-	IPVS_CMD_NEW_DAEMON,		/* start sync daemon */
-	IPVS_CMD_DEL_DAEMON,		/* stop sync daemon */
-	IPVS_CMD_GET_DAEMON,		/* get sync daemon status */
 
-	IPVS_CMD_SET_CONFIG,		/* set config settings */
-	IPVS_CMD_GET_CONFIG,		/* get config settings */
 
-	IPVS_CMD_SET_INFO,		/* only used in GET_INFO reply */
-	IPVS_CMD_GET_INFO,		/* get general IPVS info */
 
-	IPVS_CMD_ZERO,			/* zero all counters and stats */
-	IPVS_CMD_FLUSH,			/* flush services and dests */
+/* Generic Netlink command attributes */
+enum {
+	IPVS_CMD_UNSPEC = 0,
+
+	IPVS_CMD_NEW_SERVICE,	/* add service */
+	IPVS_CMD_SET_SERVICE,	/* modify service */
+	IPVS_CMD_DEL_SERVICE,	/* delete service */
+	IPVS_CMD_GET_SERVICE,	/* get service info */
+
+	IPVS_CMD_NEW_DEST,	/* add destination */
+	IPVS_CMD_SET_DEST,	/* modify destination */
+	IPVS_CMD_DEL_DEST,	/* delete destination */
+	IPVS_CMD_GET_DEST,	/* get destination info */
+
+	IPVS_CMD_NEW_DAEMON,	/* start sync daemon */
+	IPVS_CMD_DEL_DAEMON,	/* stop sync daemon */
+	IPVS_CMD_GET_DAEMON,	/* get sync daemon status */
+
+	IPVS_CMD_SET_CONFIG,	/* set config settings */
+	IPVS_CMD_GET_CONFIG,	/* get config settings */
+
+	IPVS_CMD_SET_INFO,	/* only used in GET_INFO reply */
+	IPVS_CMD_GET_INFO,	/* get general IPVS info */
+
+	IPVS_CMD_ZERO,		/* zero all counters and stats */
+	IPVS_CMD_FLUSH,		/* flush services and dests */
+
+	IPVS_CMD_NEW_LADDR,	/* add local address */
+	IPVS_CMD_DEL_LADDR,	/* del local address */
+	IPVS_CMD_GET_LADDR,	/* dump local address */
 
 	__IPVS_CMD_MAX,
 };
@@ -295,12 +331,13 @@ enum {
 /* Attributes used in the first level of commands */
 enum {
 	IPVS_CMD_ATTR_UNSPEC = 0,
-	IPVS_CMD_ATTR_SERVICE,		/* nested service attribute */
-	IPVS_CMD_ATTR_DEST,		/* nested destination attribute */
-	IPVS_CMD_ATTR_DAEMON,		/* nested sync daemon attribute */
+	IPVS_CMD_ATTR_SERVICE,	/* nested service attribute */
+	IPVS_CMD_ATTR_DEST,	/* nested destination attribute */
+	IPVS_CMD_ATTR_DAEMON,	/* nested sync daemon attribute */
 	IPVS_CMD_ATTR_TIMEOUT_TCP,	/* TCP connection timeout */
 	IPVS_CMD_ATTR_TIMEOUT_TCP_FIN,	/* TCP FIN wait timeout */
 	IPVS_CMD_ATTR_TIMEOUT_UDP,	/* UDP timeout */
+	IPVS_CMD_ATTR_LADDR,	/* nested local address attribute */
 	__IPVS_CMD_ATTR_MAX,
 };
 
@@ -313,18 +350,18 @@ enum {
  */
 enum {
 	IPVS_SVC_ATTR_UNSPEC = 0,
-	IPVS_SVC_ATTR_AF,		/* address family */
-	IPVS_SVC_ATTR_PROTOCOL,		/* virtual service protocol */
-	IPVS_SVC_ATTR_ADDR,		/* virtual service address */
-	IPVS_SVC_ATTR_PORT,		/* virtual service port */
-	IPVS_SVC_ATTR_FWMARK,		/* firewall mark of service */
+	IPVS_SVC_ATTR_AF,	/* address family */
+	IPVS_SVC_ATTR_PROTOCOL,	/* virtual service protocol */
+	IPVS_SVC_ATTR_ADDR,	/* virtual service address */
+	IPVS_SVC_ATTR_PORT,	/* virtual service port */
+	IPVS_SVC_ATTR_FWMARK,	/* firewall mark of service */
 
 	IPVS_SVC_ATTR_SCHED_NAME,	/* name of scheduler */
-	IPVS_SVC_ATTR_FLAGS,		/* virtual service flags */
-	IPVS_SVC_ATTR_TIMEOUT,		/* persistent timeout */
-	IPVS_SVC_ATTR_NETMASK,		/* persistent netmask */
+	IPVS_SVC_ATTR_FLAGS,	/* virtual service flags */
+	IPVS_SVC_ATTR_TIMEOUT,	/* persistent timeout */
+	IPVS_SVC_ATTR_NETMASK,	/* persistent netmask */
 
-	IPVS_SVC_ATTR_STATS,		/* nested attribute for service stats */
+	IPVS_SVC_ATTR_STATS,	/* nested attribute for service stats */
 	__IPVS_SVC_ATTR_MAX,
 };
 
@@ -337,11 +374,11 @@ enum {
  */
 enum {
 	IPVS_DEST_ATTR_UNSPEC = 0,
-	IPVS_DEST_ATTR_ADDR,		/* real server address */
-	IPVS_DEST_ATTR_PORT,		/* real server port */
+	IPVS_DEST_ATTR_ADDR,	/* real server address */
+	IPVS_DEST_ATTR_PORT,	/* real server port */
 
 	IPVS_DEST_ATTR_FWD_METHOD,	/* forwarding method */
-	IPVS_DEST_ATTR_WEIGHT,		/* destination weight */
+	IPVS_DEST_ATTR_WEIGHT,	/* destination weight */
 
 	IPVS_DEST_ATTR_U_THRESH,	/* upper threshold */
 	IPVS_DEST_ATTR_L_THRESH,	/* lower threshold */
@@ -350,20 +387,35 @@ enum {
 	IPVS_DEST_ATTR_INACT_CONNS,	/* inactive connections */
 	IPVS_DEST_ATTR_PERSIST_CONNS,	/* persistent connections */
 
-	IPVS_DEST_ATTR_STATS,		/* nested attribute for dest stats */
+	IPVS_DEST_ATTR_STATS,	/* nested attribute for dest stats */
 	__IPVS_DEST_ATTR_MAX,
 };
 
 #define IPVS_DEST_ATTR_MAX (__IPVS_DEST_ATTR_MAX - 1)
 
 /*
+ *  * Attirbutes used to describe a local address
+ *   *
+ *    */
+
+enum {
+	IPVS_LADDR_ATTR_UNSPEC = 0,
+	IPVS_LADDR_ATTR_ADDR,
+	IPVS_LADDR_ATTR_PORT_CONFLICT,
+	IPVS_LADDR_ATTR_CONN_COUNTS,
+	__IPVS_LADDR_ATTR_MAX,
+};
+
+#define IPVS_LADDR_ATTR_MAX (__IPVS_LADDR_ATTR_MAX - 1)
+
+/*
  * Attributes describing a sync daemon
  *
  * Used inside nested attribute IPVS_CMD_ATTR_DAEMON
  */
 enum {
 	IPVS_DAEMON_ATTR_UNSPEC = 0,
-	IPVS_DAEMON_ATTR_STATE,		/* sync daemon state (master/backup) */
+	IPVS_DAEMON_ATTR_STATE,	/* sync daemon state (master/backup) */
 	IPVS_DAEMON_ATTR_MCAST_IFN,	/* multicast interface name */
 	IPVS_DAEMON_ATTR_SYNC_ID,	/* SyncID we belong to */
 	__IPVS_DAEMON_ATTR_MAX,
@@ -378,17 +430,17 @@ enum {
  */
 enum {
 	IPVS_STATS_ATTR_UNSPEC = 0,
-	IPVS_STATS_ATTR_CONNS,		/* connections scheduled */
-	IPVS_STATS_ATTR_INPKTS,		/* incoming packets */
+	IPVS_STATS_ATTR_CONNS,	/* connections scheduled */
+	IPVS_STATS_ATTR_INPKTS,	/* incoming packets */
 	IPVS_STATS_ATTR_OUTPKTS,	/* outgoing packets */
 	IPVS_STATS_ATTR_INBYTES,	/* incoming bytes */
 	IPVS_STATS_ATTR_OUTBYTES,	/* outgoing bytes */
 
-	IPVS_STATS_ATTR_CPS,		/* current connection rate */
-	IPVS_STATS_ATTR_INPPS,		/* current in packet rate */
-	IPVS_STATS_ATTR_OUTPPS,		/* current out packet rate */
-	IPVS_STATS_ATTR_INBPS,		/* current in byte rate */
-	IPVS_STATS_ATTR_OUTBPS,		/* current out byte rate */
+	IPVS_STATS_ATTR_CPS,	/* current connection rate */
+	IPVS_STATS_ATTR_INPPS,	/* current in packet rate */
+	IPVS_STATS_ATTR_OUTPPS,	/* current out packet rate */
+	IPVS_STATS_ATTR_INBPS,	/* current in byte rate */
+	IPVS_STATS_ATTR_OUTBPS,	/* current out byte rate */
 	__IPVS_STATS_ATTR_MAX,
 };
 
@@ -397,11 +449,11 @@ enum {
 /* Attributes used in response to IPVS_CMD_GET_INFO command */
 enum {
 	IPVS_INFO_ATTR_UNSPEC = 0,
-	IPVS_INFO_ATTR_VERSION,		/* IPVS version number */
+	IPVS_INFO_ATTR_VERSION,	/* IPVS version number */
 	IPVS_INFO_ATTR_CONN_TAB_SIZE,	/* size of connection hash table */
 	__IPVS_INFO_ATTR_MAX,
 };
 
 #define IPVS_INFO_ATTR_MAX (__IPVS_INFO_ATTR_MAX - 1)
 
-#endif	/* _IP_VS_H */
+#endif				/* _IP_VS_H */
diff -uprN linux-2.6.32-220.23.1.el6.x86_64.bak/include/net/ip_vs.h linux-2.6.32-220.23.1.el6.x86_64/include/net/ip_vs.h
--- linux-2.6.32-220.23.1.el6.x86_64.bak/include/net/ip_vs.h	2009-12-03 11:51:21.000000000 +0800
+++ linux-2.6.32-220.23.1.el6.x86_64/include/net/ip_vs.h	2013-03-18 19:48:40.000000000 +0800
@@ -6,25 +6,25 @@
 #ifndef _NET_IP_VS_H
 #define _NET_IP_VS_H
 
-#include <linux/ip_vs.h>                /* definitions shared with userland */
+#include <linux/ip_vs.h>	/* definitions shared with userland */
 
 /* old ipvsadm versions still include this file directly */
 #ifdef __KERNEL__
 
-#include <asm/types.h>                  /* for __uXX types */
+#include <asm/types.h>		/* for __uXX types */
 
-#include <linux/sysctl.h>               /* for ctl_path */
-#include <linux/list.h>                 /* for struct list_head */
-#include <linux/spinlock.h>             /* for struct rwlock_t */
-#include <asm/atomic.h>                 /* for struct atomic_t */
+#include <linux/sysctl.h>	/* for ctl_path */
+#include <linux/list.h>		/* for struct list_head */
+#include <linux/spinlock.h>	/* for struct rwlock_t */
+#include <asm/atomic.h>		/* for struct atomic_t */
 #include <linux/compiler.h>
 #include <linux/timer.h>
 
 #include <net/checksum.h>
-#include <linux/netfilter.h>		/* for union nf_inet_addr */
+#include <linux/netfilter.h>	/* for union nf_inet_addr */
 #include <linux/ip.h>
-#include <linux/ipv6.h>			/* for struct ipv6hdr */
-#include <net/ipv6.h>			/* for ipv6_addr_copy */
+#include <linux/ipv6.h>		/* for struct ipv6hdr */
+#include <net/ipv6.h>		/* for ipv6_addr_copy */
 
 struct ip_vs_iphdr {
 	int len;
@@ -62,13 +62,15 @@ static inline void ip_vs_addr_copy(int a
 		ipv6_addr_copy(&dst->in6, &src->in6);
 	else
 #endif
-	dst->ip = src->ip;
+		dst->ip = src->ip;
 }
 
 static inline int ip_vs_addr_equal(int af, const union nf_inet_addr *a,
 				   const union nf_inet_addr *b)
 {
+
 #ifdef CONFIG_IP_VS_IPV6
+	af &= ~IP_VS_CONN_F_DSNAT;
 	if (af == AF_INET6)
 		return ipv6_addr_equal(&a->in6, &b->in6);
 #endif
@@ -140,7 +142,7 @@ static inline const char *ip_vs_dbg_addr
 		    net_ratelimit())					\
 			pp->debug_packet(pp, skb, ofs, msg);		\
 	} while (0)
-#else	/* NO DEBUGGING at ALL */
+#else				/* NO DEBUGGING at ALL */
 #define IP_VS_DBG_BUF(level, msg...)  do {} while (0)
 #define IP_VS_ERR_BUF(msg...)  do {} while (0)
 #define IP_VS_DBG(level, msg...)  do {} while (0)
@@ -178,13 +180,19 @@ static inline const char *ip_vs_dbg_addr
 
 #define	IP_VS_WAIT_WHILE(expr)	while (expr) { cpu_relax(); }
 
-
 /*
  *      The port number of FTP service (in network order).
  */
 #define FTPPORT  cpu_to_be16(21)
 #define FTPDATA  cpu_to_be16(20)
 
+
+#define IP_VS_DSNAT_RS_ADDR	cpu_to_be32((1<<24)+1)
+#define IP_VS_DSNAT_RS_PORT	cpu_to_be16(1)
+
+
+
+
 /*
  *      TCP State Values
  */
@@ -225,38 +233,37 @@ enum {
  *      Only used in the VS/NAT.
  */
 struct ip_vs_seq {
-	__u32			init_seq;	/* Add delta from this seq */
-	__u32			delta;		/* Delta in sequence numbers */
-	__u32			previous_delta;	/* Delta in sequence numbers
-						   before last resized pkt */
+	__u32 init_seq;		/* Add delta from this seq */
+	__u32 delta;		/* Delta in sequence numbers */
+	__u32 previous_delta;	/* Delta in sequence numbers
+				   before last resized pkt */
+	__u32 fdata_seq;	/* sequence of first data packet */
 };
 
-
 /*
  *	IPVS statistics objects
  */
 struct ip_vs_estimator {
-	struct list_head	list;
-
-	u64			last_inbytes;
-	u64			last_outbytes;
-	u32			last_conns;
-	u32			last_inpkts;
-	u32			last_outpkts;
+	struct list_head list;
 
-	u32			cps;
-	u32			inpps;
-	u32			outpps;
-	u32			inbps;
-	u32			outbps;
+	u64 last_inbytes;
+	u64 last_outbytes;
+	u64 last_conns;
+	u64 last_inpkts;
+	u64 last_outpkts;
+
+	u32 cps;
+	u32 inpps;
+	u32 outpps;
+	u64 inbps;
+	u64 outbps;
 };
 
-struct ip_vs_stats
-{
-	struct ip_vs_stats_user	ustats;         /* statistics */
-	struct ip_vs_estimator	est;		/* estimator */
+struct ip_vs_stats {
+	struct ip_vs_stats_user ustats;	/* statistics */
+	struct ip_vs_estimator est;	/* estimator */
 
-	spinlock_t              lock;           /* spin lock */
+	spinlock_t lock;	/* spin lock */
 };
 
 struct dst_entry;
@@ -266,123 +273,179 @@ struct ip_vs_app;
 struct sk_buff;
 
 struct ip_vs_protocol {
-	struct ip_vs_protocol	*next;
-	char			*name;
-	u16			protocol;
-	u16			num_states;
-	int			dont_defrag;
-	atomic_t		appcnt;		/* counter of proto app incs */
-	int			*timeout_table;	/* protocol timeout table */
-
-	void (*init)(struct ip_vs_protocol *pp);
-
-	void (*exit)(struct ip_vs_protocol *pp);
-
-	int (*conn_schedule)(int af, struct sk_buff *skb,
-			     struct ip_vs_protocol *pp,
-			     int *verdict, struct ip_vs_conn **cpp);
+	struct ip_vs_protocol *next;
+	char *name;
+	u16 protocol;
+	u16 num_states;
+	int dont_defrag;
+	atomic_t appcnt;	/* counter of proto app incs */
+	int *timeout_table;	/* protocol timeout table */
+
+	void (*init) (struct ip_vs_protocol * pp);
+
+	void (*exit) (struct ip_vs_protocol * pp);
+
+	int (*conn_schedule) (int af, struct sk_buff * skb,
+			      struct ip_vs_protocol * pp,
+			      int *verdict, struct ip_vs_conn ** cpp);
 
 	struct ip_vs_conn *
-	(*conn_in_get)(int af,
-		       const struct sk_buff *skb,
-		       struct ip_vs_protocol *pp,
-		       const struct ip_vs_iphdr *iph,
-		       unsigned int proto_off,
-		       int inverse);
+	    (*conn_in_get) (int af,
+			    const struct sk_buff * skb,
+			    struct ip_vs_protocol * pp,
+			    const struct ip_vs_iphdr * iph,
+			    unsigned int proto_off, int inverse, int *res_dir);
 
 	struct ip_vs_conn *
-	(*conn_out_get)(int af,
-			const struct sk_buff *skb,
-			struct ip_vs_protocol *pp,
-			const struct ip_vs_iphdr *iph,
-			unsigned int proto_off,
-			int inverse);
+	    (*conn_out_get) (int af,
+			     const struct sk_buff * skb,
+			     struct ip_vs_protocol * pp,
+			     const struct ip_vs_iphdr * iph,
+			     unsigned int proto_off, int inverse, int *res_dir);
+
+	int (*snat_handler) (struct sk_buff * skb,
+			     struct ip_vs_protocol * pp,
+			     struct ip_vs_conn * cp);
 
-	int (*snat_handler)(struct sk_buff *skb,
-			    struct ip_vs_protocol *pp, struct ip_vs_conn *cp);
+	int (*dnat_handler) (struct sk_buff * skb,
+			     struct ip_vs_protocol * pp,
+			     struct ip_vs_conn * cp);
 
-	int (*dnat_handler)(struct sk_buff *skb,
-			    struct ip_vs_protocol *pp, struct ip_vs_conn *cp);
+	int (*fnat_in_handler) (struct sk_buff ** skb_p,
+				struct ip_vs_protocol * pp,
+				struct ip_vs_conn * cp);
 
-	int (*csum_check)(int af, struct sk_buff *skb,
-			  struct ip_vs_protocol *pp);
+	int (*fnat_out_handler) (struct sk_buff * skb,
+				 struct ip_vs_protocol * pp,
+				 struct ip_vs_conn * cp);
 
-	const char *(*state_name)(int state);
+	int (*csum_check) (int af, struct sk_buff * skb,
+			   struct ip_vs_protocol * pp);
 
-	int (*state_transition)(struct ip_vs_conn *cp, int direction,
-				const struct sk_buff *skb,
-				struct ip_vs_protocol *pp);
+	const char *(*state_name) (int state);
 
-	int (*register_app)(struct ip_vs_app *inc);
+	int (*state_transition) (struct ip_vs_conn * cp, int direction,
+				 const struct sk_buff * skb,
+				 struct ip_vs_protocol * pp);
 
-	void (*unregister_app)(struct ip_vs_app *inc);
+	int (*register_app) (struct ip_vs_app * inc);
 
-	int (*app_conn_bind)(struct ip_vs_conn *cp);
+	void (*unregister_app) (struct ip_vs_app * inc);
 
-	void (*debug_packet)(struct ip_vs_protocol *pp,
-			     const struct sk_buff *skb,
-			     int offset,
-			     const char *msg);
+	int (*app_conn_bind) (struct ip_vs_conn * cp);
 
-	void (*timeout_change)(struct ip_vs_protocol *pp, int flags);
+	void (*debug_packet) (struct ip_vs_protocol * pp,
+			      const struct sk_buff * skb,
+			      int offset, const char *msg);
 
-	int (*set_state_timeout)(struct ip_vs_protocol *pp, char *sname, int to);
+	void (*timeout_change) (struct ip_vs_protocol * pp, int flags);
+
+	int (*set_state_timeout) (struct ip_vs_protocol * pp, char *sname,
+				  int to);
+
+	void (*conn_expire_handler) (struct ip_vs_protocol * pp,
+				     struct ip_vs_conn * cp);
 };
 
-extern struct ip_vs_protocol * ip_vs_proto_get(unsigned short proto);
+extern struct ip_vs_protocol *ip_vs_proto_get(unsigned short proto);
+
+/*
+ *      Connection Index Flags
+ */
+#define IP_VS_CIDX_F_OUT2IN     0x0001	/* packet director, OUTside2INside */
+#define IP_VS_CIDX_F_IN2OUT     0x0002	/* packet director, INside2OUTside */
+#define IP_VS_CIDX_F_DIR_MASK	0x0003	/* packet director mask */
+
+/*
+ *      Connection index in HASH TABLE, each connection has two index
+ */
+struct ip_vs_conn_idx {
+	struct list_head c_list;	/* hashed list heads */
+
+	u16 af;			/* address family */
+	__u16 protocol;		/* Which protocol (TCP/UDP) */
+	union nf_inet_addr s_addr;	/* source address */
+	union nf_inet_addr d_addr;	/* destination address */
+	__be16 s_port;		/* source port */
+	__be16 d_port;		/* destination port */
+
+	struct ip_vs_conn *cp;	/* point to connection */
+	volatile __u16 flags;	/* status flags */
+};
 
 /*
  *	IP_VS structure allocated for each dynamically scheduled connection
  */
 struct ip_vs_conn {
-	struct list_head        c_list;         /* hashed list heads */
+	struct ip_vs_conn_idx *in_idx;	/* client-vs hash index */
+	struct ip_vs_conn_idx *out_idx;	/* rs-vs hash index */
 
 	/* Protocol, addresses and port numbers */
-	u16                      af;		/* address family */
-	union nf_inet_addr       caddr;          /* client address */
-	union nf_inet_addr       vaddr;          /* virtual address */
-	union nf_inet_addr       daddr;          /* destination address */
-	__be16                   cport;
-	__be16                   vport;
-	__be16                   dport;
-	__u16                   protocol;       /* Which protocol (TCP/UDP) */
+	u16 af;			/* address family */
+	__u16 protocol;		/* Which protocol (TCP/UDP) */
+	union nf_inet_addr caddr;	/* client address */
+	union nf_inet_addr vaddr;	/* virtual address */
+	union nf_inet_addr laddr;	/* local address */
+	union nf_inet_addr daddr;	/* destination address */
+	__be16 cport;
+	__be16 vport;
+	__be16 lport;
+	__be16 dport;
 
 	/* counter and timer */
-	atomic_t		refcnt;		/* reference count */
-	struct timer_list	timer;		/* Expiration timer */
-	volatile unsigned long	timeout;	/* timeout */
+	atomic_t refcnt;	/* reference count */
+	struct timer_list timer;	/* Expiration timer */
+	volatile unsigned long timeout;	/* timeout */
 
 	/* Flags and state transition */
-	spinlock_t              lock;           /* lock for state transition */
-	volatile __u16          flags;          /* status flags */
-	volatile __u16          state;          /* state info */
-	volatile __u16          old_state;      /* old state, to be used for
-						 * state transition triggerd
-						 * synchronization
-						 */
-
+	spinlock_t lock;	/* lock for state transition */
+	volatile __u16 flags;	/* status flags */
+	volatile __u16 state;	/* state info */
+	volatile __u16 old_state;	/* old state, to be used for
+					 * state transition triggerd
+					 * synchronization
+					 */
 	/* Control members */
-	struct ip_vs_conn       *control;       /* Master control connection */
-	atomic_t                n_control;      /* Number of controlled ones */
-	struct ip_vs_dest       *dest;          /* real server */
-	atomic_t                in_pkts;        /* incoming packet counter */
+	struct ip_vs_conn *control;	/* Master control connection */
+	atomic_t n_control;	/* Number of controlled ones */
+	struct ip_vs_dest *dest;	/* real server */
+	struct ip_vs_laddr *local;	/* local address */
+	atomic_t in_pkts;	/* incoming packet counter */
+
+	/* for fullnat */
+	struct ip_vs_seq fnat_seq;
 
 	/* packet transmitter for different forwarding methods.  If it
 	   mangles the packet, it must return NF_DROP or better NF_STOLEN,
 	   otherwise this must be changed to a sk_buff **.
 	 */
-	int (*packet_xmit)(struct sk_buff *skb, struct ip_vs_conn *cp,
-			   struct ip_vs_protocol *pp);
+	int (*packet_xmit) (struct sk_buff * skb, struct ip_vs_conn * cp,
+			    struct ip_vs_protocol * pp);
 
 	/* Note: we can group the following members into a structure,
 	   in order to save more space, and the following members are
 	   only used in VS/NAT anyway */
-	struct ip_vs_app        *app;           /* bound ip_vs_app object */
-	void                    *app_data;      /* Application private data */
-	struct ip_vs_seq        in_seq;         /* incoming seq. struct */
-	struct ip_vs_seq        out_seq;        /* outgoing seq. struct */
-};
+	struct ip_vs_app *app;	/* bound ip_vs_app object */
+	void *app_data;		/* Application private data */
+	struct ip_vs_seq in_seq;	/* incoming seq. struct */
+	struct ip_vs_seq out_seq;	/* outgoing seq. struct */
 
+	/* syn-proxy related members
+	 */
+	struct ip_vs_seq syn_proxy_seq;	/* seq. used in syn proxy */
+	struct sk_buff_head ack_skb;	/* ack skb, save in step2 */
+	struct sk_buff *syn_skb;	/* saved rs syn packet */
+	atomic_t syn_retry_max;	/* syn retransmition max count */
+
+	/* add for stopping ack storm */
+	__u32 last_seq;		/* seq of the last ack packet */
+	__u32 last_ack_seq;	/* ack seq of the last ack packet */
+	atomic_t dup_ack_cnt;	/* count of repeated ack packets */
+
+	/* for RST */
+	__u32 rs_end_seq;	/* end seq(seq+datalen) of the last ack packet from rs */
+	__u32 rs_ack_seq;	/* ack seq of the last ack packet from rs */
+};
 
 /*
  *	Extended internal versions of struct ip_vs_service_user and
@@ -394,190 +457,294 @@ struct ip_vs_conn {
  */
 struct ip_vs_service_user_kern {
 	/* virtual service addresses */
-	u16			af;
-	u16			protocol;
-	union nf_inet_addr	addr;		/* virtual ip address */
-	u16			port;
-	u32			fwmark;		/* firwall mark of service */
+	u16 af;
+	u16 protocol;
+	union nf_inet_addr addr;	/* virtual ip address */
+	u16 port;
+	u32 fwmark;		/* firwall mark of service */
 
 	/* virtual service options */
-	char			*sched_name;
-	unsigned		flags;		/* virtual service flags */
-	unsigned		timeout;	/* persistent timeout in sec */
-	u32			netmask;	/* persistent netmask */
+	char *sched_name;
+	unsigned flags;		/* virtual service flags */
+	unsigned timeout;	/* persistent timeout in sec */
+	u32 netmask;		/* persistent netmask */
 };
 
-
 struct ip_vs_dest_user_kern {
 	/* destination server address */
-	union nf_inet_addr	addr;
-	u16			port;
+	union nf_inet_addr addr;
+	u16 port;
 
 	/* real server options */
-	unsigned		conn_flags;	/* connection flags */
-	int			weight;		/* destination weight */
+	unsigned conn_flags;	/* connection flags */
+	int weight;		/* destination weight */
 
 	/* thresholds for active connections */
-	u32			u_threshold;	/* upper threshold */
-	u32			l_threshold;	/* lower threshold */
+	u32 u_threshold;	/* upper threshold */
+	u32 l_threshold;	/* lower threshold */
+};
+
+struct ip_vs_laddr_user_kern {
+	union nf_inet_addr addr;	/* ip address */
 };
 
 
 /*
+ * dsnat 
+ */
+struct ip_vs_dsnat {
+	struct ip_vs_service *svc[IPPROTO_MAX];
+	char iniface[IFNAMSIZ], outiface[IFNAMSIZ];
+	unsigned char iniface_mask[IFNAMSIZ], outiface_mask[IFNAMSIZ];	
+};
+
+
+
+
+/*
  *	The information about the virtual service offered to the net
  *	and the forwarding entries
  */
 struct ip_vs_service {
-	struct list_head	s_list;   /* for normal service table */
-	struct list_head	f_list;   /* for fwmark-based service table */
-	atomic_t		refcnt;   /* reference counter */
-	atomic_t		usecnt;   /* use counter */
-
-	u16			af;       /* address family */
-	__u16			protocol; /* which protocol (TCP/UDP) */
-	union nf_inet_addr	addr;	  /* IP address for virtual service */
-	__be16			port;	  /* port number for the service */
-	__u32                   fwmark;   /* firewall mark of the service */
-	unsigned		flags;	  /* service status flags */
-	unsigned		timeout;  /* persistent timeout in ticks */
-	__be32			netmask;  /* grouping granularity */
-
-	struct list_head	destinations;  /* real server d-linked list */
-	__u32			num_dests;     /* number of servers */
-	struct ip_vs_stats      stats;         /* statistics for the service */
-	struct ip_vs_app	*inc;	  /* bind conns to this app inc */
+	struct list_head s_list;	/* for normal service table */
+	struct list_head f_list;	/* for fwmark-based service table */
+	atomic_t refcnt;	/* reference counter */
+	atomic_t usecnt;	/* use counter */
+
+	u16 af;			/* address family */
+	__u16 protocol;		/* which protocol (TCP/UDP) */
+	union nf_inet_addr addr;	/* IP address for virtual service */
+	__be16 port;		/* port number for the service */
+	__u32 fwmark;		/* firewall mark of the service */
+	unsigned flags;		/* service status flags */
+	unsigned timeout;	/* persistent timeout in ticks */
+	__be32 netmask;		/* grouping granularity */
+
+	/* for realservers list */
+	struct list_head destinations;	/* real server d-linked list */
+	__u32 num_dests;	/* number of servers */
+
+	/* for local ip address list, now only used in FULL NAT model */
+	struct list_head laddr_list;	/* local ip address list */
+	rwlock_t laddr_lock;	/* lock for protect curr_laddr */
+	__u32 num_laddrs;	/* number of local ip address */
+	struct list_head *curr_laddr;	/* laddr data list head */
+
+	struct ip_vs_stats stats;	/* statistics for the service */
+	struct ip_vs_app *inc;	/* bind conns to this app inc */
 
 	/* for scheduling */
-	struct ip_vs_scheduler	*scheduler;    /* bound scheduler object */
-	rwlock_t		sched_lock;    /* lock sched_data */
-	void			*sched_data;   /* scheduler application data */
+	struct ip_vs_scheduler *scheduler;	/* bound scheduler object */
+	rwlock_t sched_lock;	/* lock sched_data */
+	void *sched_data;	/* scheduler application data */
 };
 
-
 /*
  *	The real server destination forwarding entry
  *	with ip address, port number, and so on.
  */
 struct ip_vs_dest {
-	struct list_head	n_list;   /* for the dests in the service */
-	struct list_head	d_list;   /* for table with all the dests */
+	struct list_head n_list;	/* for the dests in the service */
+	struct list_head d_list;	/* for table with all the dests */
 
-	u16			af;		/* address family */
-	union nf_inet_addr	addr;		/* IP address of the server */
-	__be16			port;		/* port number of the server */
-	volatile unsigned	flags;		/* dest status flags */
-	atomic_t		conn_flags;	/* flags to copy to conn */
-	atomic_t		weight;		/* server weight */
+	u16 af;			/* address family */
+	union nf_inet_addr addr;	/* IP address of the server */
+	__be16 port;		/* port number of the server */
+	volatile unsigned flags;	/* dest status flags */
+	atomic_t conn_flags;	/* flags to copy to conn */
+	atomic_t weight;	/* server weight */
 
-	atomic_t		refcnt;		/* reference counter */
-	struct ip_vs_stats      stats;          /* statistics */
+	atomic_t refcnt;	/* reference counter */
+	struct ip_vs_stats stats;	/* statistics */
 
 	/* connection counters and thresholds */
-	atomic_t		activeconns;	/* active connections */
-	atomic_t		inactconns;	/* inactive connections */
-	atomic_t		persistconns;	/* persistent connections */
-	__u32			u_threshold;	/* upper threshold */
-	__u32			l_threshold;	/* lower threshold */
+	atomic_t activeconns;	/* active connections */
+	atomic_t inactconns;	/* inactive connections */
+	atomic_t persistconns;	/* persistent connections */
+	__u32 u_threshold;	/* upper threshold */
+	__u32 l_threshold;	/* lower threshold */
 
 	/* for destination cache */
-	spinlock_t		dst_lock;	/* lock of dst_cache */
-	struct dst_entry	*dst_cache;	/* destination cache entry */
-	u32			dst_rtos;	/* RT_TOS(tos) for dst */
+	spinlock_t dst_lock;	/* lock of dst_cache */
+	struct dst_entry *dst_cache;	/* destination cache entry */
+	u32 dst_rtos;		/* RT_TOS(tos) for dst */
 
 	/* for virtual service */
-	struct ip_vs_service	*svc;		/* service it belongs to */
-	__u16			protocol;	/* which protocol (TCP/UDP) */
-	union nf_inet_addr	vaddr;		/* virtual IP address */
-	__be16			vport;		/* virtual port number */
-	__u32			vfwmark;	/* firewall mark of service */
+	struct ip_vs_service *svc;	/* service it belongs to */
+	__u16 protocol;		/* which protocol (TCP/UDP) */
+	union nf_inet_addr vaddr;	/* virtual IP address */
+	__be16 vport;		/* virtual port number */
+	__u32 vfwmark;		/* firewall mark of service */
 };
 
+/*
+ *	Local ip address object, now only used in FULL NAT model
+ */
+struct ip_vs_laddr {
+	struct list_head n_list;	/* for the local address in the service */
+	u16 af;			/* address family */
+	union nf_inet_addr addr;	/* ip address */
+	atomic64_t port;	/* port counts */
+	atomic_t refcnt;	/* reference count */
+
+	atomic64_t port_conflict;	/* conflict counts */
+	atomic_t conn_counts;	/* connects counts */
+};
 
 /*
  *	The scheduler object
  */
 struct ip_vs_scheduler {
-	struct list_head	n_list;		/* d-linked list head */
-	char			*name;		/* scheduler name */
-	atomic_t		refcnt;		/* reference counter */
-	struct module		*module;	/* THIS_MODULE/NULL */
+	struct list_head n_list;	/* d-linked list head */
+	char *name;		/* scheduler name */
+	atomic_t refcnt;	/* reference counter */
+	struct module *module;	/* THIS_MODULE/NULL */
 
 	/* scheduler initializing service */
-	int (*init_service)(struct ip_vs_service *svc);
+	int (*init_service) (struct ip_vs_service * svc);
 	/* scheduling service finish */
-	int (*done_service)(struct ip_vs_service *svc);
+	int (*done_service) (struct ip_vs_service * svc);
 	/* scheduler updating service */
-	int (*update_service)(struct ip_vs_service *svc);
+	int (*update_service) (struct ip_vs_service * svc);
 
 	/* selecting a server from the given service */
-	struct ip_vs_dest* (*schedule)(struct ip_vs_service *svc,
-				       const struct sk_buff *skb);
+	struct ip_vs_dest *(*schedule) (struct ip_vs_service * svc,
+					const struct sk_buff * skb);
 };
 
-
 /*
  *	The application module object (a.k.a. app incarnation)
  */
-struct ip_vs_app
-{
-	struct list_head	a_list;		/* member in app list */
-	int			type;		/* IP_VS_APP_TYPE_xxx */
-	char			*name;		/* application module name */
-	__u16			protocol;
-	struct module		*module;	/* THIS_MODULE/NULL */
-	struct list_head	incs_list;	/* list of incarnations */
+struct ip_vs_app {
+	struct list_head a_list;	/* member in app list */
+	int type;		/* IP_VS_APP_TYPE_xxx */
+	char *name;		/* application module name */
+	__u16 protocol;
+	struct module *module;	/* THIS_MODULE/NULL */
+	struct list_head incs_list;	/* list of incarnations */
 
 	/* members for application incarnations */
-	struct list_head	p_list;		/* member in proto app list */
-	struct ip_vs_app	*app;		/* its real application */
-	__be16			port;		/* port number in net order */
-	atomic_t		usecnt;		/* usage counter */
+	struct list_head p_list;	/* member in proto app list */
+	struct ip_vs_app *app;	/* its real application */
+	__be16 port;		/* port number in net order */
+	atomic_t usecnt;	/* usage counter */
 
 	/* output hook: return false if can't linearize. diff set for TCP.  */
-	int (*pkt_out)(struct ip_vs_app *, struct ip_vs_conn *,
-		       struct sk_buff *, int *diff);
+	int (*pkt_out) (struct ip_vs_app *, struct ip_vs_conn *,
+			struct sk_buff *, int *diff);
 
 	/* input hook: return false if can't linearize. diff set for TCP. */
-	int (*pkt_in)(struct ip_vs_app *, struct ip_vs_conn *,
-		      struct sk_buff *, int *diff);
+	int (*pkt_in) (struct ip_vs_app *, struct ip_vs_conn *,
+		       struct sk_buff *, int *diff);
 
 	/* ip_vs_app initializer */
-	int (*init_conn)(struct ip_vs_app *, struct ip_vs_conn *);
+	int (*init_conn) (struct ip_vs_app *, struct ip_vs_conn *);
 
 	/* ip_vs_app finish */
-	int (*done_conn)(struct ip_vs_app *, struct ip_vs_conn *);
-
+	int (*done_conn) (struct ip_vs_app *, struct ip_vs_conn *);
 
 	/* not used now */
-	int (*bind_conn)(struct ip_vs_app *, struct ip_vs_conn *,
-			 struct ip_vs_protocol *);
+	int (*bind_conn) (struct ip_vs_app *, struct ip_vs_conn *,
+			  struct ip_vs_protocol *);
 
-	void (*unbind_conn)(struct ip_vs_app *, struct ip_vs_conn *);
+	void (*unbind_conn) (struct ip_vs_app *, struct ip_vs_conn *);
 
-	int *			timeout_table;
-	int *			timeouts;
-	int			timeouts_size;
+	int *timeout_table;
+	int *timeouts;
+	int timeouts_size;
 
-	int (*conn_schedule)(struct sk_buff *skb, struct ip_vs_app *app,
-			     int *verdict, struct ip_vs_conn **cpp);
+	int (*conn_schedule) (struct sk_buff * skb, struct ip_vs_app * app,
+			      int *verdict, struct ip_vs_conn ** cpp);
 
 	struct ip_vs_conn *
-	(*conn_in_get)(const struct sk_buff *skb, struct ip_vs_app *app,
-		       const struct iphdr *iph, unsigned int proto_off,
-		       int inverse);
+	    (*conn_in_get) (const struct sk_buff * skb, struct ip_vs_app * app,
+			    const struct iphdr * iph, unsigned int proto_off,
+			    int inverse);
 
 	struct ip_vs_conn *
-	(*conn_out_get)(const struct sk_buff *skb, struct ip_vs_app *app,
-			const struct iphdr *iph, unsigned int proto_off,
-			int inverse);
-
-	int (*state_transition)(struct ip_vs_conn *cp, int direction,
-				const struct sk_buff *skb,
-				struct ip_vs_app *app);
+	    (*conn_out_get) (const struct sk_buff * skb, struct ip_vs_app * app,
+			     const struct iphdr * iph, unsigned int proto_off,
+			     int inverse);
+
+	int (*state_transition) (struct ip_vs_conn * cp, int direction,
+				 const struct sk_buff * skb,
+				 struct ip_vs_app * app);
+
+	void (*timeout_change) (struct ip_vs_app * app, int flags);
+};
+
+#define TCPOPT_ADDR  200
+#define TCPOLEN_ADDR 8		/* |opcode|size|ip+port| = 1 + 1 + 6 */
+
+/*
+ * insert client ip in tcp option, now only support IPV4,
+ * must be 4 bytes alignment.
+ */
+struct ip_vs_tcpo_addr {
+	__u8 opcode;
+	__u8 opsize;
+	__u16 port;
+	__u32 addr;
+};
+
+/*
+ * statistics for FULLNAT and SYNPROXY
+ * in /proc/net/ip_vs_ext_stats
+ */
+enum {
+	FULLNAT_ADD_TOA_OK = 1,
+	FULLNAT_ADD_TOA_FAIL_LEN,
+	FULLNAT_ADD_TOA_FAIL_MEM,
+	FULLNAT_ADD_TOA_FAIL_PROTO,
+	FULLNAT_CONN_REUSED,
+	FULLNAT_CONN_REUSED_CLOSE,
+	FULLNAT_CONN_REUSED_TIMEWAIT,
+	FULLNAT_CONN_REUSED_FINWAIT,
+	FULLNAT_CONN_REUSED_CLOSEWAIT,
+	FULLNAT_CONN_REUSED_LASTACK,
+	FULLNAT_CONN_REUSED_ESTAB,
+	SYNPROXY_RS_ERROR,
+	SYNPROXY_NULL_ACK,
+	SYNPROXY_BAD_ACK,
+	SYNPROXY_OK_ACK,
+	SYNPROXY_SYN_CNT,
+	SYNPROXY_ACK_STORM,
+	SYNPROXY_SYNSEND_QLEN,
+	SYNPROXY_CONN_REUSED,
+	SYNPROXY_CONN_REUSED_CLOSE,
+	SYNPROXY_CONN_REUSED_TIMEWAIT,
+	SYNPROXY_CONN_REUSED_FINWAIT,
+	SYNPROXY_CONN_REUSED_CLOSEWAIT,
+	SYNPROXY_CONN_REUSED_LASTACK,
+	DEFENCE_IP_FRAG_DROP,
+	DEFENCE_TCP_DROP,
+	DEFENCE_UDP_DROP,
+	IP_VS_EXT_STAT_LAST
+};
+
+struct ip_vs_estats_entry {
+	char *name;
+	int entry;
+};
+
+#define IP_VS_ESTATS_ITEM(_name, _entry) { \
+        .name = _name,            \
+        .entry = _entry,          \
+}
 
-	void (*timeout_change)(struct ip_vs_app *app, int flags);
+#define IP_VS_ESTATS_LAST {    \
+        NULL,           \
+        0,              \
+}
+
+struct ip_vs_estats_mib {
+	unsigned long mibs[IP_VS_EXT_STAT_LAST];
 };
 
+#define IP_VS_INC_ESTATS(mib, field)         \
+        (per_cpu_ptr(mib, smp_processor_id())->mibs[field]++)
+
+extern struct ip_vs_estats_mib *ip_vs_esmib;
 
 /*
  *      IPVS core functions
@@ -598,7 +765,7 @@ extern void ip_vs_init_hash_table(struct
  *     IPVS connection entry hash table
  */
 #ifndef CONFIG_IP_VS_TAB_BITS
-#define CONFIG_IP_VS_TAB_BITS   12
+#define CONFIG_IP_VS_TAB_BITS   22
 #endif
 
 #define IP_VS_CONN_TAB_BITS	CONFIG_IP_VS_TAB_BITS
@@ -612,17 +779,13 @@ enum {
 	IP_VS_DIR_LAST,
 };
 
-extern struct ip_vs_conn *ip_vs_conn_in_get
-(int af, int protocol, const union nf_inet_addr *s_addr, __be16 s_port,
- const union nf_inet_addr *d_addr, __be16 d_port);
+extern struct ip_vs_conn *ip_vs_conn_get
+    (int af, int protocol, const union nf_inet_addr *s_addr, __be16 s_port,
+     const union nf_inet_addr *d_addr, __be16 d_port, int *res_dir);
 
 extern struct ip_vs_conn *ip_vs_ct_in_get
-(int af, int protocol, const union nf_inet_addr *s_addr, __be16 s_port,
- const union nf_inet_addr *d_addr, __be16 d_port);
-
-extern struct ip_vs_conn *ip_vs_conn_out_get
-(int af, int protocol, const union nf_inet_addr *s_addr, __be16 s_port,
- const union nf_inet_addr *d_addr, __be16 d_port);
+    (int af, int protocol, const union nf_inet_addr *s_addr, __be16 s_port,
+     const union nf_inet_addr *d_addr, __be16 d_port);
 
 /* put back the conn without restarting its timer */
 static inline void __ip_vs_conn_put(struct ip_vs_conn *cp)
@@ -632,14 +795,20 @@ static inline void __ip_vs_conn_put(stru
 extern void ip_vs_conn_put(struct ip_vs_conn *cp);
 extern void ip_vs_conn_fill_cport(struct ip_vs_conn *cp, __be16 cport);
 
-extern struct ip_vs_conn *
-ip_vs_conn_new(int af, int proto, const union nf_inet_addr *caddr, __be16 cport,
-	       const union nf_inet_addr *vaddr, __be16 vport,
-	       const union nf_inet_addr *daddr, __be16 dport, unsigned flags,
-	       struct ip_vs_dest *dest);
+
+extern struct ip_vs_conn *ip_vs_conn_new(int af, int proto,
+					 const union nf_inet_addr *caddr,
+					 __be16 cport,
+					 const union nf_inet_addr *vaddr,
+					 __be16 vport,
+					 const union nf_inet_addr *daddr,
+					 __be16 dport, unsigned flags,
+					 struct ip_vs_dest *dest,
+					 struct sk_buff *skb,
+					 int is_synproxy_on);
 extern void ip_vs_conn_expire_now(struct ip_vs_conn *cp);
 
-extern const char * ip_vs_state_name(__u16 proto, int state);
+extern const char *ip_vs_state_name(__u16 proto, int state);
 
 extern void ip_vs_tcp_conn_listen(struct ip_vs_conn *cp);
 extern int ip_vs_check_template(struct ip_vs_conn *ct);
@@ -707,7 +876,6 @@ ip_vs_control_add(struct ip_vs_conn *cp,
 	atomic_inc(&ctl_cp->n_control);
 }
 
-
 /*
  *      IPVS application functions
  *      (from ip_vs_app.c)
@@ -729,7 +897,6 @@ extern int ip_vs_skb_replace(struct sk_b
 extern int ip_vs_app_init(void);
 extern void ip_vs_app_cleanup(void);
 
-
 /*
  *	IPVS protocol functions (from ip_vs_proto.c)
  */
@@ -750,7 +917,6 @@ extern struct ip_vs_protocol ip_vs_proto
 extern struct ip_vs_protocol ip_vs_protocol_esp;
 extern struct ip_vs_protocol ip_vs_protocol_ah;
 
-
 /*
  *      Registering/unregistering scheduler functions
  *      (from ip_vs_sched.c)
@@ -762,11 +928,11 @@ extern int ip_vs_bind_scheduler(struct i
 extern int ip_vs_unbind_scheduler(struct ip_vs_service *svc);
 extern struct ip_vs_scheduler *ip_vs_scheduler_get(const char *sched_name);
 extern void ip_vs_scheduler_put(struct ip_vs_scheduler *scheduler);
-extern struct ip_vs_conn *
-ip_vs_schedule(struct ip_vs_service *svc, const struct sk_buff *skb);
+extern struct ip_vs_conn *ip_vs_schedule(struct ip_vs_service *svc,
+					 struct sk_buff *skb,
+					 int is_synproxy_on);
 extern int ip_vs_leave(struct ip_vs_service *svc, struct sk_buff *skb,
-			struct ip_vs_protocol *pp);
-
+		       struct ip_vs_protocol *pp);
 
 /*
  *      IPVS control data and functions (from ip_vs_ctl.c)
@@ -778,29 +944,49 @@ extern int sysctl_ip_vs_sync_threshold[2
 extern int sysctl_ip_vs_nat_icmp_send;
 extern struct ip_vs_stats ip_vs_stats;
 extern const struct ctl_path net_vs_ctl_path[];
-
-extern struct ip_vs_service *
-ip_vs_service_get(int af, __u32 fwmark, __u16 protocol,
-		  const union nf_inet_addr *vaddr, __be16 vport);
+extern int sysctl_ip_vs_timestamp_remove_entry;
+extern int sysctl_ip_vs_mss_adjust_entry;
+extern int sysctl_ip_vs_conn_reused_entry;
+extern int sysctl_ip_vs_toa_entry;
+extern int sysctl_ip_vs_lport_max;
+extern int sysctl_ip_vs_lport_min;
+extern int sysctl_ip_vs_lport_tries;
+extern int sysctl_ip_vs_frag_drop_entry;
+extern int sysctl_ip_vs_tcp_drop_entry;
+extern int sysctl_ip_vs_udp_drop_entry;
+extern int sysctl_ip_vs_conn_expire_tcp_rst;
+
+extern struct ip_vs_dsnat  *ip_vs_dsnat_get(void);
+
+extern struct ip_vs_service *ip_vs_service_get(int af, __u32 fwmark,
+					       __u16 protocol,
+					       const union nf_inet_addr *vaddr,
+					       __be16 vport);
+extern struct ip_vs_service *ip_vs_lookup_vip(int af, __u16 protocol,
+					      const union nf_inet_addr *vaddr);
 
 static inline void ip_vs_service_put(struct ip_vs_service *svc)
 {
 	atomic_dec(&svc->usecnt);
 }
 
-extern struct ip_vs_dest *
-ip_vs_lookup_real_service(int af, __u16 protocol,
-			  const union nf_inet_addr *daddr, __be16 dport);
+extern struct ip_vs_dest *ip_vs_lookup_real_service(int af, __u16 protocol,
+						    const union nf_inet_addr
+						    *daddr, __be16 dport);
 
 extern int ip_vs_use_count_inc(void);
 extern void ip_vs_use_count_dec(void);
 extern int ip_vs_control_init(void);
 extern void ip_vs_control_cleanup(void);
-extern struct ip_vs_dest *
-ip_vs_find_dest(int af, const union nf_inet_addr *daddr, __be16 dport,
-		const union nf_inet_addr *vaddr, __be16 vport, __u16 protocol);
+extern struct ip_vs_dest *ip_vs_find_dest(int af,
+					  const union nf_inet_addr *daddr,
+					  __be16 dport,
+					  const union nf_inet_addr *vaddr,
+					  __be16 vport, __u16 protocol);
 extern struct ip_vs_dest *ip_vs_try_bind_dest(struct ip_vs_conn *cp);
 
+extern void ip_vs_laddr_hold(struct ip_vs_laddr *addr);
+extern void ip_vs_laddr_put(struct ip_vs_laddr *addr);
 
 /*
  *      IPVS sync daemon data and function prototypes
@@ -815,7 +1001,6 @@ extern int start_sync_thread(int state, 
 extern int stop_sync_thread(int state);
 extern void ip_vs_sync_conn(struct ip_vs_conn *cp);
 
-
 /*
  *      IPVS rate estimator prototypes (from ip_vs_est.c)
  */
@@ -826,34 +1011,73 @@ extern void ip_vs_kill_estimator(struct 
 extern void ip_vs_zero_estimator(struct ip_vs_stats *stats);
 
 /*
+ *	Lookup route table
+ */
+extern struct rtable *ip_vs_get_rt(union nf_inet_addr *addr, u32 rtos);
+
+#ifdef CONFIG_IP_VS_IPV6
+extern struct rt6_info *ip_vs_get_rt_v6(union nf_inet_addr *addr);
+#endif
+
+/*
  *	Various IPVS packet transmitters (from ip_vs_xmit.c)
  */
 extern int ip_vs_null_xmit
-(struct sk_buff *skb, struct ip_vs_conn *cp, struct ip_vs_protocol *pp);
+    (struct sk_buff *skb, struct ip_vs_conn *cp, struct ip_vs_protocol *pp);
 extern int ip_vs_bypass_xmit
-(struct sk_buff *skb, struct ip_vs_conn *cp, struct ip_vs_protocol *pp);
+    (struct sk_buff *skb, struct ip_vs_conn *cp, struct ip_vs_protocol *pp);
 extern int ip_vs_nat_xmit
-(struct sk_buff *skb, struct ip_vs_conn *cp, struct ip_vs_protocol *pp);
+    (struct sk_buff *skb, struct ip_vs_conn *cp, struct ip_vs_protocol *pp);
+extern int ip_vs_fnat_xmit
+    (struct sk_buff *skb, struct ip_vs_conn *cp, struct ip_vs_protocol *pp);
 extern int ip_vs_tunnel_xmit
-(struct sk_buff *skb, struct ip_vs_conn *cp, struct ip_vs_protocol *pp);
+    (struct sk_buff *skb, struct ip_vs_conn *cp, struct ip_vs_protocol *pp);
 extern int ip_vs_dr_xmit
-(struct sk_buff *skb, struct ip_vs_conn *cp, struct ip_vs_protocol *pp);
+    (struct sk_buff *skb, struct ip_vs_conn *cp, struct ip_vs_protocol *pp);
 extern int ip_vs_icmp_xmit
-(struct sk_buff *skb, struct ip_vs_conn *cp, struct ip_vs_protocol *pp, int offset);
+    (struct sk_buff *skb, struct ip_vs_conn *cp, struct ip_vs_protocol *pp,
+     int offset);
 extern void ip_vs_dst_reset(struct ip_vs_dest *dest);
+extern int ip_vs_normal_response_xmit
+    (struct sk_buff *skb, struct ip_vs_protocol *pp, struct ip_vs_conn *cp,
+     int ihl);
+extern int ip_vs_fnat_response_xmit(struct sk_buff *skb,
+				    struct ip_vs_protocol *pp,
+				    struct ip_vs_conn *cp, int ihl);
+extern int ip_vs_normal_response_icmp_xmit(struct sk_buff *skb,
+					   struct ip_vs_protocol *pp,
+					   struct ip_vs_conn *cp, int offset);
+extern int ip_vs_fnat_response_icmp_xmit(struct sk_buff *skb,
+					 struct ip_vs_protocol *pp,
+					 struct ip_vs_conn *cp, int offset);
 
 #ifdef CONFIG_IP_VS_IPV6
 extern int ip_vs_bypass_xmit_v6
-(struct sk_buff *skb, struct ip_vs_conn *cp, struct ip_vs_protocol *pp);
+    (struct sk_buff *skb, struct ip_vs_conn *cp, struct ip_vs_protocol *pp);
 extern int ip_vs_nat_xmit_v6
-(struct sk_buff *skb, struct ip_vs_conn *cp, struct ip_vs_protocol *pp);
+    (struct sk_buff *skb, struct ip_vs_conn *cp, struct ip_vs_protocol *pp);
+extern int ip_vs_fnat_xmit_v6
+    (struct sk_buff *skb, struct ip_vs_conn *cp, struct ip_vs_protocol *pp);
 extern int ip_vs_tunnel_xmit_v6
-(struct sk_buff *skb, struct ip_vs_conn *cp, struct ip_vs_protocol *pp);
+    (struct sk_buff *skb, struct ip_vs_conn *cp, struct ip_vs_protocol *pp);
 extern int ip_vs_dr_xmit_v6
-(struct sk_buff *skb, struct ip_vs_conn *cp, struct ip_vs_protocol *pp);
+    (struct sk_buff *skb, struct ip_vs_conn *cp, struct ip_vs_protocol *pp);
 extern int ip_vs_icmp_xmit_v6
-(struct sk_buff *skb, struct ip_vs_conn *cp, struct ip_vs_protocol *pp,
- int offset);
+    (struct sk_buff *skb, struct ip_vs_conn *cp, struct ip_vs_protocol *pp,
+     int offset);
+extern int ip_vs_normal_response_xmit_v6
+    (struct sk_buff *skb, struct ip_vs_protocol *pp, struct ip_vs_conn *cp,
+     int ihl);
+extern int ip_vs_fnat_response_xmit_v6(struct sk_buff *skb,
+				       struct ip_vs_protocol *pp,
+				       struct ip_vs_conn *cp, int ihl);
+extern int ip_vs_normal_response_icmp_xmit_v6(struct sk_buff *skb,
+					      struct ip_vs_protocol *pp,
+					      struct ip_vs_conn *cp,
+					      int offset);
+extern int ip_vs_fnat_response_icmp_xmit_v6(struct sk_buff *skb,
+					    struct ip_vs_protocol *pp,
+					    struct ip_vs_conn *cp, int offset);
 #endif
 
 /*
@@ -866,8 +1090,10 @@ extern int ip_vs_drop_counter;
 
 static __inline__ int ip_vs_todrop(void)
 {
-	if (!ip_vs_drop_rate) return 0;
-	if (--ip_vs_drop_counter > 0) return 0;
+	if (!ip_vs_drop_rate)
+		return 0;
+	if (--ip_vs_drop_counter > 0)
+		return 0;
 	ip_vs_drop_counter = ip_vs_drop_rate;
 	return 1;
 }
@@ -883,29 +1109,30 @@ static inline char ip_vs_fwd_tag(struct 
 
 	switch (IP_VS_FWD_METHOD(cp)) {
 	case IP_VS_CONN_F_MASQ:
-		fwd = 'M'; break;
+		fwd = 'M';
+		break;
 	case IP_VS_CONN_F_LOCALNODE:
-		fwd = 'L'; break;
+		fwd = 'L';
+		break;
 	case IP_VS_CONN_F_TUNNEL:
-		fwd = 'T'; break;
+		fwd = 'T';
+		break;
 	case IP_VS_CONN_F_DROUTE:
-		fwd = 'R'; break;
+		fwd = 'R';
+		break;
 	case IP_VS_CONN_F_BYPASS:
-		fwd = 'B'; break;
+		fwd = 'B';
+		break;
+	case IP_VS_CONN_F_FULLNAT:
+		fwd = 'F';
+		break;
 	default:
-		fwd = '?'; break;
+		fwd = '?';
+		break;
 	}
 	return fwd;
 }
 
-extern void ip_vs_nat_icmp(struct sk_buff *skb, struct ip_vs_protocol *pp,
-			   struct ip_vs_conn *cp, int dir);
-
-#ifdef CONFIG_IP_VS_IPV6
-extern void ip_vs_nat_icmp_v6(struct sk_buff *skb, struct ip_vs_protocol *pp,
-			      struct ip_vs_conn *cp, int dir);
-#endif
-
 extern __sum16 ip_vs_checksum_complete(struct sk_buff *skb, int offset);
 
 static inline __wsum ip_vs_check_diff4(__be32 old, __be32 new, __wsum oldsum)
@@ -916,11 +1143,12 @@ static inline __wsum ip_vs_check_diff4(_
 }
 
 #ifdef CONFIG_IP_VS_IPV6
-static inline __wsum ip_vs_check_diff16(const __be32 *old, const __be32 *new,
+static inline __wsum ip_vs_check_diff16(const __be32 * old, const __be32 * new,
 					__wsum oldsum)
 {
 	__be32 diff[8] = { ~old[3], ~old[2], ~old[1], ~old[0],
-			    new[3],  new[2],  new[1],  new[0] };
+		new[3], new[2], new[1], new[0]
+	};
 
 	return csum_partial(diff, sizeof(diff), oldsum);
 }
@@ -933,6 +1161,6 @@ static inline __wsum ip_vs_check_diff2(_
 	return csum_partial(diff, sizeof(diff), oldsum);
 }
 
-#endif /* __KERNEL__ */
+#endif				/* __KERNEL__ */
 
-#endif	/* _NET_IP_VS_H */
+#endif				/* _NET_IP_VS_H */
diff -uprN linux-2.6.32-220.23.1.el6.x86_64.bak/include/net/ip_vs_synproxy.h linux-2.6.32-220.23.1.el6.x86_64/include/net/ip_vs_synproxy.h
--- linux-2.6.32-220.23.1.el6.x86_64.bak/include/net/ip_vs_synproxy.h	1970-01-01 08:00:00.000000000 +0800
+++ linux-2.6.32-220.23.1.el6.x86_64/include/net/ip_vs_synproxy.h	2012-10-11 15:01:03.000000000 +0800
@@ -0,0 +1,135 @@
+/*
+ *     IP Virtual Server Syn-Proxy
+ *     data structure and functionality definitions
+ */
+
+#ifndef _NET_IP_VS_SYNPROXY_H
+#define _NET_IP_VS_SYNPROXY_H
+
+#include <net/ip_vs.h>
+
+/* Add MASKs for TCP OPT in "data" coded in cookie */
+/* |[21][20][19-16][15-0]|
+ * [21]    SACK
+ * [20]    TimeStamp
+ * [19-16] snd_wscale
+ * [15-0]  MSSIND
+ */
+#define IP_VS_SYNPROXY_MSS_BITS 16
+#define IP_VS_SYNPROXY_MSS_MASK (((__u32)1 << IP_VS_SYNPROXY_MSS_BITS) - 1)
+
+#define IP_VS_SYNPROXY_SACKOK_BIT 21
+#define IP_VS_SYNPROXY_SACKOK_MASK ((__u32)1 << IP_VS_SYNPROXY_SACKOK_BIT)
+
+#define IP_VS_SYNPROXY_TSOK_BIT 20
+#define IP_VS_SYNPROXY_TSOK_MASK ((__u32)1 << IP_VS_SYNPROXY_TSOK_BIT)
+
+#define IP_VS_SYNPROXY_SND_WSCALE_BITS 16
+#define IP_VS_SYNPROXY_SND_WSCALE_MASK ((__u32)0xf << IP_VS_SYNPROXY_SND_WSCALE_BITS)
+
+#define IP_VS_SYNPROXY_WSCALE_MAX          14
+
+/* add for supporting tcp options' in syn-proxy */
+struct ip_vs_synproxy_opt {
+	u16 snd_wscale:8,	/* Window scaling received from sender          */
+	 tstamp_ok:1,		/* TIMESTAMP seen on SYN packet                 */
+	 wscale_ok:1,		/* Wscale seen on SYN packet                    */
+	 sack_ok:1;		/* SACK seen on SYN packet                      */
+	u16 mss_clamp;		/* Maximal mss, negotiated at connection setup  */
+};
+
+/* 
+ * For syncookie compute and check 
+ */
+extern __u32 ip_vs_synproxy_cookie_v4_init_sequence(struct sk_buff *skb,
+						    struct ip_vs_synproxy_opt
+						    *opts);
+extern int ip_vs_synproxy_v4_cookie_check(struct sk_buff *skb, __u32 cookie,
+					  struct ip_vs_synproxy_opt *opt);
+
+extern __u32 ip_vs_synproxy_cookie_v6_init_sequence(struct sk_buff *skb,
+						    struct ip_vs_synproxy_opt
+						    *opts);
+extern int ip_vs_synproxy_v6_cookie_check(struct sk_buff *skb, __u32 cookie,
+					  struct ip_vs_synproxy_opt *opt);
+
+/*
+ * Syn-proxy step 1 logic: receive client's Syn.
+ */
+extern int ip_vs_synproxy_syn_rcv(int af, struct sk_buff *skb,
+				  struct ip_vs_iphdr *iph, int *verdict);
+/*
+ * Syn-proxy step 2 logic: receive client's Ack.
+ */
+extern int ip_vs_synproxy_ack_rcv(int af, struct sk_buff *skb,
+				  struct tcphdr *th, struct ip_vs_protocol *pp,
+				  struct ip_vs_conn **cpp,
+				  struct ip_vs_iphdr *iph, int *verdict);
+/*
+ * Syn-proxy step 3 logic: receive rs's Syn/Ack.
+ */
+extern int ip_vs_synproxy_synack_rcv(struct sk_buff *skb, struct ip_vs_conn *cp,
+				     struct ip_vs_protocol *pp,
+				     int ihl, int *verdict);
+/*
+ * Syn-proxy conn reuse logic: receive client's Ack.
+ */
+extern int ip_vs_synproxy_reuse_conn(int af, struct sk_buff *skb,
+				     struct ip_vs_conn *cp,
+				     struct ip_vs_protocol *pp,
+				     struct ip_vs_iphdr *iph, int *verdict);
+/*
+ * Store or drop client's ack packet, when lvs is waiting for 
+ * rs's Syn/Ack packet.
+ */
+extern int ip_vs_synproxy_filter_ack(struct sk_buff *skb, struct ip_vs_conn *cp,
+				     struct ip_vs_protocol *pp,
+				     struct ip_vs_iphdr *iph, int *verdict);
+
+/*
+ * Tranfer ack seq and sack opt for Out-In packet.
+ */
+extern void ip_vs_synproxy_dnat_handler(struct tcphdr *tcph,
+					struct ip_vs_seq *sp_seq);
+/*
+ * Tranfer seq for In-Out packet.
+ */
+extern int ip_vs_synproxy_snat_handler(struct tcphdr *tcph,
+				       struct ip_vs_conn *cp);
+
+/* syn-proxy sysctl variables */
+#define IP_VS_SYNPROXY_INIT_MSS_DEFAULT  	1452
+#define IP_VS_SYNPROXY_TTL_DEFAULT     		63
+#define IP_VS_SYNPROXY_TTL_MIN     		1
+#define IP_VS_SYNPROXY_TTL_MAX     		255
+#define IP_VS_SYNPROXY_SACK_DEFAULT  		1
+#define IP_VS_SYNPROXY_WSCALE_DEFAULT  		0
+#define IP_VS_SYNPROXY_TIMESTAMP_DEFAULT  	0
+#define IP_VS_SYNPROXY_DEFER_DEFAULT   		1
+#define IP_VS_SYNPROXY_DUP_ACK_DEFAULT     	10
+#define IP_VS_SYNPROXY_SKB_STORE_DEFAULT     	3
+#define IP_VS_SYNPROXY_CONN_REUSE_DEFAULT	1
+#define	IP_VS_SYNPROXY_CONN_REUSE_CL_DEFAULT	1
+#define	IP_VS_SYNPROXY_CONN_REUSE_TW_DEFAULT	1
+#define	IP_VS_SYNPROXY_CONN_REUSE_FW_DEFAULT	0
+#define	IP_VS_SYNPROXY_CONN_REUSE_CW_DEFAULT	0
+#define	IP_VS_SYNPROXY_CONN_REUSE_LA_DEFAULT	0
+#define	IP_VS_SYNPROXY_SYN_RETRY_DEFAULT	3
+
+extern int sysctl_ip_vs_synproxy_sack;
+extern int sysctl_ip_vs_synproxy_wscale;
+extern int sysctl_ip_vs_synproxy_timestamp;
+extern int sysctl_ip_vs_synproxy_synack_ttl;
+extern int sysctl_ip_vs_synproxy_init_mss;
+extern int sysctl_ip_vs_synproxy_defer;
+extern int sysctl_ip_vs_synproxy_dup_ack_thresh;
+extern int sysctl_ip_vs_synproxy_skb_store_thresh;
+extern int sysctl_ip_vs_synproxy_syn_retry;
+extern int sysctl_ip_vs_synproxy_conn_reuse;
+extern int sysctl_ip_vs_synproxy_conn_reuse_cl;
+extern int sysctl_ip_vs_synproxy_conn_reuse_tw;
+extern int sysctl_ip_vs_synproxy_conn_reuse_fw;
+extern int sysctl_ip_vs_synproxy_conn_reuse_cw;
+extern int sysctl_ip_vs_synproxy_conn_reuse_la;
+
+#endif
diff -uprN linux-2.6.32-220.23.1.el6.x86_64.bak/net/core/secure_seq.c linux-2.6.32-220.23.1.el6.x86_64/net/core/secure_seq.c
--- linux-2.6.32-220.23.1.el6.x86_64.bak/net/core/secure_seq.c	2012-06-12 21:35:37.000000000 +0800
+++ linux-2.6.32-220.23.1.el6.x86_64/net/core/secure_seq.c	2012-10-11 15:01:03.000000000 +0800
@@ -105,6 +105,7 @@ __u32 secure_tcp_sequence_number(__be32 
 
 	return seq_scale(hash[0]);
 }
+EXPORT_SYMBOL(secure_tcp_sequence_number);
 
 u32 secure_ipv4_port_ephemeral(__be32 saddr, __be32 daddr, __be16 dport)
 {
diff -uprN linux-2.6.32-220.23.1.el6.x86_64.bak/net/ipv4/syncookies.c linux-2.6.32-220.23.1.el6.x86_64/net/ipv4/syncookies.c
--- linux-2.6.32-220.23.1.el6.x86_64.bak/net/ipv4/syncookies.c	2009-12-03 11:51:21.000000000 +0800
+++ linux-2.6.32-220.23.1.el6.x86_64/net/ipv4/syncookies.c	2012-10-11 15:01:03.000000000 +0800
@@ -8,6 +8,12 @@
  *      modify it under the terms of the GNU General Public License
  *      as published by the Free Software Foundation; either version
  *      2 of the License, or (at your option) any later version.
+ *
+ * Changes:
+ *	Jian Chen <jian.chen1225@gmail.com>
+ *	Yan Tian <tianyan.7c00@gmail.com>
+ *
+ *	add synproxy cookies for ipvs module
  */
 
 #include <linux/tcp.h>
@@ -17,6 +23,7 @@
 #include <linux/kernel.h>
 #include <net/tcp.h>
 #include <net/route.h>
+#include <net/ip_vs_synproxy.h>
 
 /* Timestamps: lowest 9 bits store TCP options */
 #define TSBITS 9
@@ -363,3 +370,89 @@ struct sock *cookie_v4_check(struct sock
 	ret = get_cookie_sock(sk, skb, req, &rt->u.dst);
 out:	return ret;
 }
+
+/*
+ * Generate a syncookie for ip_vs module. 
+ * Besides mss, we store additional tcp options in cookie "data".
+ * 
+ * Cookie "data" format: 
+ * |[21][20][19-16][15-0]|
+ * [21] SACKOK
+ * [20] TimeStampOK
+ * [19-16] snd_wscale
+ * [15-0] MSSIND 
+ */
+__u32 ip_vs_synproxy_cookie_v4_init_sequence(struct sk_buff *skb, 
+                                            struct ip_vs_synproxy_opt *opts) 
+{
+       const struct iphdr *iph = ip_hdr(skb);
+       const struct tcphdr *th = tcp_hdr(skb);
+       int mssind;
+       const __u16 mss = opts->mss_clamp;
+       __u32 data = 0;
+
+       /* XXX sort msstab[] by probability?  Binary search? */
+       for (mssind = 0; mss > msstab[mssind + 1]; mssind++)
+               ;
+       opts->mss_clamp = msstab[mssind] + 1;
+
+       data = mssind & IP_VS_SYNPROXY_MSS_MASK;
+       data |= opts->sack_ok << IP_VS_SYNPROXY_SACKOK_BIT;
+       data |= opts->tstamp_ok << IP_VS_SYNPROXY_TSOK_BIT;
+       data |= ((opts->snd_wscale & 0x0f) << IP_VS_SYNPROXY_SND_WSCALE_BITS);
+
+       return secure_tcp_syn_cookie(iph->saddr, iph->daddr,
+                                    th->source, th->dest, ntohl(th->seq),
+                                    jiffies / (HZ * 60), data);
+}
+EXPORT_SYMBOL(ip_vs_synproxy_cookie_v4_init_sequence);
+
+
+/*
+ * when ip_vs_synproxy_cookie_v4_init_sequence is used, we check
+ * cookie as follow:
+ *  1. mssind check.
+ *  2. get sack/timestamp/wscale options.
+ */
+int ip_vs_synproxy_v4_cookie_check(struct sk_buff * skb, __u32 cookie, 
+                             struct ip_vs_synproxy_opt * opt) 
+{
+       const struct iphdr *iph = ip_hdr(skb);
+       const struct tcphdr *th = tcp_hdr(skb);
+       __u32 seq = ntohl(th->seq) - 1;
+       __u32 mssind;
+       int   ret = 0;
+       __u32 res = check_tcp_syn_cookie(cookie, iph->saddr, iph->daddr,
+                                        th->source, th->dest, seq,
+                                        jiffies / (HZ * 60),
+                                        COUNTER_TRIES);
+
+       if(res == (__u32)-1) /* count is invalid, jiffies' >> jiffies */
+               goto out;
+
+       mssind = res & IP_VS_SYNPROXY_MSS_MASK;
+
+       memset(opt, 0, sizeof(struct ip_vs_synproxy_opt));
+
+       if (mssind < NUM_MSS) {
+               opt->mss_clamp = msstab[mssind] + 1;
+               opt->sack_ok = (res & IP_VS_SYNPROXY_SACKOK_MASK) >> 
+                                       IP_VS_SYNPROXY_SACKOK_BIT;
+               opt->tstamp_ok = (res & IP_VS_SYNPROXY_TSOK_MASK) >> 
+                                       IP_VS_SYNPROXY_TSOK_BIT;
+               opt->snd_wscale = (res & IP_VS_SYNPROXY_SND_WSCALE_MASK) >> 
+                                       IP_VS_SYNPROXY_SND_WSCALE_BITS;
+                if (opt->snd_wscale > 0 && 
+                   opt->snd_wscale <= IP_VS_SYNPROXY_WSCALE_MAX)
+                        opt->wscale_ok = 1;
+                else if (opt->snd_wscale == 0)
+                        opt->wscale_ok = 0;
+                else
+                        goto out;
+
+               ret = 1;
+       }
+
+out:   return ret;
+}
+EXPORT_SYMBOL(ip_vs_synproxy_v4_cookie_check);
diff -uprN linux-2.6.32-220.23.1.el6.x86_64.bak/net/ipv6/syncookies.c linux-2.6.32-220.23.1.el6.x86_64/net/ipv6/syncookies.c
--- linux-2.6.32-220.23.1.el6.x86_64.bak/net/ipv6/syncookies.c	2009-12-03 11:51:21.000000000 +0800
+++ linux-2.6.32-220.23.1.el6.x86_64/net/ipv6/syncookies.c	2012-10-11 15:01:03.000000000 +0800
@@ -12,6 +12,12 @@
  *      as published by the Free Software Foundation; either version
  *      2 of the License, or (at your option) any later version.
  *
+ * Changes:
+ *	Jian Chen <jian.chen1225@gmail.com>
+ *	Yan Tian <tianyan.7c00@gmail.com>
+ *
+ *	add synproxy cookies for ipvs module
+ *
  */
 
 #include <linux/tcp.h>
@@ -20,6 +26,7 @@
 #include <linux/kernel.h>
 #include <net/ipv6.h>
 #include <net/tcp.h>
+#include <net/ip_vs_synproxy.h>
 
 extern int sysctl_tcp_syncookies;
 extern __u32 syncookie_secret[2][16-4+SHA_DIGEST_WORDS];
@@ -279,3 +286,87 @@ out_free:
 	return NULL;
 }
 
+/*
+ * Generate a syncookie for ip_vs module. 
+ * Besides mss, we store additional tcp options in cookie "data".
+ * 
+ * Cookie "data" format: 
+ * |[21][20][19-16][15-0]|
+ * [21] SACKOK
+ * [20] TimeStampOK
+ * [19-16] snd_wscale
+ * [15-0] MSSIND 
+ */
+__u32 ip_vs_synproxy_cookie_v6_init_sequence(struct sk_buff *skb, 
+                                            struct ip_vs_synproxy_opt *opts) 
+{
+       struct ipv6hdr *iph = ipv6_hdr(skb);
+       const struct tcphdr *th = tcp_hdr(skb);
+       int mssind;
+       const __u16 mss = opts->mss_clamp;
+       __u32 data = 0;
+
+       /* XXX sort msstab[] by probability?  Binary search? */
+       for (mssind = 0; mss > msstab[mssind + 1]; mssind++)
+               ;
+       opts->mss_clamp = msstab[mssind] + 1;
+
+       data = mssind & IP_VS_SYNPROXY_MSS_MASK;
+       data |= opts->sack_ok << IP_VS_SYNPROXY_SACKOK_BIT;
+       data |= opts->tstamp_ok << IP_VS_SYNPROXY_TSOK_BIT;
+       data |= ((opts->snd_wscale & 0x0f) << IP_VS_SYNPROXY_SND_WSCALE_BITS);
+
+       return secure_tcp_syn_cookie(&iph->saddr, &iph->daddr,
+                                    th->source, th->dest, ntohl(th->seq),
+                                    jiffies / (HZ * 60), data);
+}
+EXPORT_SYMBOL(ip_vs_synproxy_cookie_v6_init_sequence);
+
+/*
+ * when ip_vs_synproxy_cookie_v6_init_sequence is used, we check
+ * cookie as follow:
+ *  1. mssind check.
+ *  2. get sack/timestamp/wscale options.
+ */
+int ip_vs_synproxy_v6_cookie_check(struct sk_buff * skb, __u32 cookie, 
+                             struct ip_vs_synproxy_opt * opt) 
+{
+       struct ipv6hdr *iph = ipv6_hdr(skb);
+       const struct tcphdr *th = tcp_hdr(skb);
+       __u32 seq = ntohl(th->seq) - 1;
+       __u32 mssind;
+       int   ret = 0;
+       __u32 res = check_tcp_syn_cookie(cookie, &iph->saddr, &iph->daddr,
+                                        th->source, th->dest, seq,
+                                        jiffies / (HZ * 60),
+                                        COUNTER_TRIES);
+
+       if(res == (__u32)-1) /* count is invalid, jiffies' >> jiffies */
+               goto out;
+
+       mssind = res & IP_VS_SYNPROXY_MSS_MASK;
+
+       memset(opt, 0, sizeof(struct ip_vs_synproxy_opt));
+
+       if (mssind < NUM_MSS) {
+               opt->mss_clamp = msstab[mssind] + 1;
+               opt->sack_ok = (res & IP_VS_SYNPROXY_SACKOK_MASK) >> 
+                                       IP_VS_SYNPROXY_SACKOK_BIT;
+               opt->tstamp_ok = (res & IP_VS_SYNPROXY_TSOK_MASK) >> 
+                                       IP_VS_SYNPROXY_TSOK_BIT;
+               opt->snd_wscale = (res & IP_VS_SYNPROXY_SND_WSCALE_MASK) >> 
+                                       IP_VS_SYNPROXY_SND_WSCALE_BITS;
+                if (opt->snd_wscale > 0 && 
+                   opt->snd_wscale <= IP_VS_SYNPROXY_WSCALE_MAX)
+                        opt->wscale_ok = 1;
+                else if (opt->snd_wscale == 0)
+                        opt->wscale_ok = 0;
+                else
+                        goto out;
+
+               ret = 1;
+       }
+
+out:   return ret;
+}
+EXPORT_SYMBOL(ip_vs_synproxy_v6_cookie_check);
diff -uprN linux-2.6.32-220.23.1.el6.x86_64.bak/net/netfilter/ipvs/ip_vs_app.c linux-2.6.32-220.23.1.el6.x86_64/net/netfilter/ipvs/ip_vs_app.c
--- linux-2.6.32-220.23.1.el6.x86_64.bak/net/netfilter/ipvs/ip_vs_app.c	2009-12-03 11:51:21.000000000 +0800
+++ linux-2.6.32-220.23.1.el6.x86_64/net/netfilter/ipvs/ip_vs_app.c	2012-10-11 15:01:03.000000000 +0800
@@ -46,7 +46,6 @@ EXPORT_SYMBOL(register_ip_vs_app_inc);
 static LIST_HEAD(ip_vs_app_list);
 static DEFINE_MUTEX(__ip_vs_app_mutex);
 
-
 /*
  *	Get an ip_vs_app object
  */
@@ -55,18 +54,15 @@ static inline int ip_vs_app_get(struct i
 	return try_module_get(app->module);
 }
 
-
 static inline void ip_vs_app_put(struct ip_vs_app *app)
 {
 	module_put(app->module);
 }
 
-
 /*
  *	Allocate/initialize app incarnation and register it in proto apps.
  */
-static int
-ip_vs_app_inc_new(struct ip_vs_app *app, __u16 proto, __u16 port)
+static int ip_vs_app_inc_new(struct ip_vs_app *app, __u16 proto, __u16 port)
 {
 	struct ip_vs_protocol *pp;
 	struct ip_vs_app *inc;
@@ -89,8 +85,8 @@ ip_vs_app_inc_new(struct ip_vs_app *app,
 
 	if (app->timeouts) {
 		inc->timeout_table =
-			ip_vs_create_timeout_table(app->timeouts,
-						   app->timeouts_size);
+		    ip_vs_create_timeout_table(app->timeouts,
+					       app->timeouts_size);
 		if (!inc->timeout_table) {
 			ret = -ENOMEM;
 			goto out;
@@ -107,18 +103,16 @@ ip_vs_app_inc_new(struct ip_vs_app *app,
 
 	return 0;
 
-  out:
+      out:
 	kfree(inc->timeout_table);
 	kfree(inc);
 	return ret;
 }
 
-
 /*
  *	Release app incarnation
  */
-static void
-ip_vs_app_inc_release(struct ip_vs_app *inc)
+static void ip_vs_app_inc_release(struct ip_vs_app *inc)
 {
 	struct ip_vs_protocol *pp;
 
@@ -137,7 +131,6 @@ ip_vs_app_inc_release(struct ip_vs_app *
 	kfree(inc);
 }
 
-
 /*
  *	Get reference to app inc (only called from softirq)
  *
@@ -152,7 +145,6 @@ int ip_vs_app_inc_get(struct ip_vs_app *
 	return result;
 }
 
-
 /*
  *	Put the app inc (only called from timer or net softirq)
  */
@@ -162,12 +154,10 @@ void ip_vs_app_inc_put(struct ip_vs_app 
 	atomic_dec(&inc->usecnt);
 }
 
-
 /*
  *	Register an application incarnation in protocol applications
  */
-int
-register_ip_vs_app_inc(struct ip_vs_app *app, __u16 proto, __u16 port)
+int register_ip_vs_app_inc(struct ip_vs_app *app, __u16 proto, __u16 port)
 {
 	int result;
 
@@ -180,7 +170,6 @@ register_ip_vs_app_inc(struct ip_vs_app 
 	return result;
 }
 
-
 /*
  *	ip_vs_app registration routine
  */
@@ -198,7 +187,6 @@ int register_ip_vs_app(struct ip_vs_app 
 	return 0;
 }
 
-
 /*
  *	ip_vs_app unregistration routine
  *	We are sure there are no app incarnations attached to services
@@ -221,7 +209,6 @@ void unregister_ip_vs_app(struct ip_vs_a
 	ip_vs_use_count_dec();
 }
 
-
 /*
  *	Bind ip_vs_conn to its ip_vs_app (called by cp constructor)
  */
@@ -230,7 +217,6 @@ int ip_vs_bind_app(struct ip_vs_conn *cp
 	return pp->app_conn_bind(cp);
 }
 
-
 /*
  *	Unbind cp from application incarnation (called by cp destructor)
  */
@@ -249,7 +235,6 @@ void ip_vs_unbind_app(struct ip_vs_conn 
 	cp->app = NULL;
 }
 
-
 /*
  *	Fixes th->seq based on ip_vs_seq info.
  */
@@ -258,12 +243,12 @@ static inline void vs_fix_seq(const stru
 	__u32 seq = ntohl(th->seq);
 
 	/*
-	 *	Adjust seq with delta-offset for all packets after
-	 *	the most recent resized pkt seq and with previous_delta offset
-	 *	for all packets	before most recent resized pkt seq.
+	 *      Adjust seq with delta-offset for all packets after
+	 *      the most recent resized pkt seq and with previous_delta offset
+	 *      for all packets before most recent resized pkt seq.
 	 */
 	if (vseq->delta || vseq->previous_delta) {
-		if(after(seq, vseq->init_seq)) {
+		if (after(seq, vseq->init_seq)) {
 			th->seq = htonl(seq + vseq->delta);
 			IP_VS_DBG(9, "%s(): added delta (%d) to seq\n",
 				  __func__, vseq->delta);
@@ -275,7 +260,6 @@ static inline void vs_fix_seq(const stru
 	}
 }
 
-
 /*
  *	Fixes th->ack_seq based on ip_vs_seq info.
  */
@@ -292,7 +276,7 @@ vs_fix_ack_seq(const struct ip_vs_seq *v
 	if (vseq->delta || vseq->previous_delta) {
 		/* since ack_seq is the number of octet that is expected
 		   to receive next, so compare it with init_seq+delta */
-		if(after(ack_seq, vseq->init_seq+vseq->delta)) {
+		if (after(ack_seq, vseq->init_seq + vseq->delta)) {
 			th->ack_seq = htonl(ack_seq - vseq->delta);
 			IP_VS_DBG(9, "%s(): subtracted delta "
 				  "(%d) from ack_seq\n", __func__, vseq->delta);
@@ -306,7 +290,6 @@ vs_fix_ack_seq(const struct ip_vs_seq *v
 	}
 }
 
-
 /*
  *	Updates ip_vs_seq if pkt has been resized
  *	Assumes already checked proto==IPPROTO_TCP and diff!=0.
@@ -339,12 +322,12 @@ static inline int app_tcp_pkt_out(struct
 	th = (struct tcphdr *)(skb_network_header(skb) + tcp_offset);
 
 	/*
-	 *	Remember seq number in case this pkt gets resized
+	 *      Remember seq number in case this pkt gets resized
 	 */
 	seq = ntohl(th->seq);
 
 	/*
-	 *	Fix seq stuff if flagged as so.
+	 *      Fix seq stuff if flagged as so.
 	 */
 	if (cp->flags & IP_VS_CONN_F_OUT_SEQ)
 		vs_fix_seq(&cp->out_seq, th);
@@ -352,7 +335,7 @@ static inline int app_tcp_pkt_out(struct
 		vs_fix_ack_seq(&cp->in_seq, th);
 
 	/*
-	 *	Call private output hook function
+	 *      Call private output hook function
 	 */
 	if (app->pkt_out == NULL)
 		return 1;
@@ -361,7 +344,7 @@ static inline int app_tcp_pkt_out(struct
 		return 0;
 
 	/*
-	 *	Update ip_vs seq stuff if len has changed.
+	 *      Update ip_vs seq stuff if len has changed.
 	 */
 	if (diff != 0)
 		vs_seq_update(cp, &cp->out_seq,
@@ -380,8 +363,8 @@ int ip_vs_app_pkt_out(struct ip_vs_conn 
 	struct ip_vs_app *app;
 
 	/*
-	 *	check if application module is bound to
-	 *	this ip_vs_conn.
+	 *      check if application module is bound to
+	 *      this ip_vs_conn.
 	 */
 	if ((app = cp->app) == NULL)
 		return 1;
@@ -391,7 +374,7 @@ int ip_vs_app_pkt_out(struct ip_vs_conn 
 		return app_tcp_pkt_out(cp, skb, app);
 
 	/*
-	 *	Call private output hook function
+	 *      Call private output hook function
 	 */
 	if (app->pkt_out == NULL)
 		return 1;
@@ -399,7 +382,6 @@ int ip_vs_app_pkt_out(struct ip_vs_conn 
 	return app->pkt_out(app, cp, skb, NULL);
 }
 
-
 static inline int app_tcp_pkt_in(struct ip_vs_conn *cp, struct sk_buff *skb,
 				 struct ip_vs_app *app)
 {
@@ -414,12 +396,12 @@ static inline int app_tcp_pkt_in(struct 
 	th = (struct tcphdr *)(skb_network_header(skb) + tcp_offset);
 
 	/*
-	 *	Remember seq number in case this pkt gets resized
+	 *      Remember seq number in case this pkt gets resized
 	 */
 	seq = ntohl(th->seq);
 
 	/*
-	 *	Fix seq stuff if flagged as so.
+	 *      Fix seq stuff if flagged as so.
 	 */
 	if (cp->flags & IP_VS_CONN_F_IN_SEQ)
 		vs_fix_seq(&cp->in_seq, th);
@@ -427,7 +409,7 @@ static inline int app_tcp_pkt_in(struct 
 		vs_fix_ack_seq(&cp->out_seq, th);
 
 	/*
-	 *	Call private input hook function
+	 *      Call private input hook function
 	 */
 	if (app->pkt_in == NULL)
 		return 1;
@@ -436,11 +418,10 @@ static inline int app_tcp_pkt_in(struct 
 		return 0;
 
 	/*
-	 *	Update ip_vs seq stuff if len has changed.
+	 *      Update ip_vs seq stuff if len has changed.
 	 */
 	if (diff != 0)
-		vs_seq_update(cp, &cp->in_seq,
-			      IP_VS_CONN_F_IN_SEQ, seq, diff);
+		vs_seq_update(cp, &cp->in_seq, IP_VS_CONN_F_IN_SEQ, seq, diff);
 
 	return 1;
 }
@@ -455,8 +436,8 @@ int ip_vs_app_pkt_in(struct ip_vs_conn *
 	struct ip_vs_app *app;
 
 	/*
-	 *	check if application module is bound to
-	 *	this ip_vs_conn.
+	 *      check if application module is bound to
+	 *      this ip_vs_conn.
 	 */
 	if ((app = cp->app) == NULL)
 		return 1;
@@ -466,7 +447,7 @@ int ip_vs_app_pkt_in(struct ip_vs_conn *
 		return app_tcp_pkt_in(cp, skb, app);
 
 	/*
-	 *	Call private input hook function
+	 *      Call private input hook function
 	 */
 	if (app->pkt_in == NULL)
 		return 1;
@@ -474,7 +455,6 @@ int ip_vs_app_pkt_in(struct ip_vs_conn *
 	return app->pkt_in(app, cp, skb, NULL);
 }
 
-
 #ifdef CONFIG_PROC_FS
 /*
  *	/proc/net/ip_vs_app entry function
@@ -494,14 +474,14 @@ static struct ip_vs_app *ip_vs_app_idx(l
 
 }
 
-static void *ip_vs_app_seq_start(struct seq_file *seq, loff_t *pos)
+static void *ip_vs_app_seq_start(struct seq_file *seq, loff_t * pos)
 {
 	mutex_lock(&__ip_vs_app_mutex);
 
 	return *pos ? ip_vs_app_idx(*pos - 1) : SEQ_START_TOKEN;
 }
 
-static void *ip_vs_app_seq_next(struct seq_file *seq, void *v, loff_t *pos)
+static void *ip_vs_app_seq_next(struct seq_file *seq, void *v, loff_t * pos)
 {
 	struct ip_vs_app *inc, *app;
 	struct list_head *e;
@@ -541,17 +521,16 @@ static int ip_vs_app_seq_show(struct seq
 		seq_printf(seq, "%-3s  %-7u %-6d %-17s\n",
 			   ip_vs_proto_name(inc->protocol),
 			   ntohs(inc->port),
-			   atomic_read(&inc->usecnt),
-			   inc->name);
+			   atomic_read(&inc->usecnt), inc->name);
 	}
 	return 0;
 }
 
 static const struct seq_operations ip_vs_app_seq_ops = {
 	.start = ip_vs_app_seq_start,
-	.next  = ip_vs_app_seq_next,
-	.stop  = ip_vs_app_seq_stop,
-	.show  = ip_vs_app_seq_show,
+	.next = ip_vs_app_seq_next,
+	.stop = ip_vs_app_seq_stop,
+	.show = ip_vs_app_seq_show,
 };
 
 static int ip_vs_app_open(struct inode *inode, struct file *file)
@@ -560,15 +539,14 @@ static int ip_vs_app_open(struct inode *
 }
 
 static const struct file_operations ip_vs_app_fops = {
-	.owner	 = THIS_MODULE,
-	.open	 = ip_vs_app_open,
-	.read	 = seq_read,
-	.llseek  = seq_lseek,
+	.owner = THIS_MODULE,
+	.open = ip_vs_app_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
 	.release = seq_release,
 };
 #endif
 
-
 /*
  *	Replace a segment of data with a new segment
  */
@@ -610,7 +588,6 @@ int ip_vs_skb_replace(struct sk_buff *sk
 	return 0;
 }
 
-
 int __init ip_vs_app_init(void)
 {
 	/* we will replace it with proc_net_ipvs_create() soon */
@@ -618,7 +595,6 @@ int __init ip_vs_app_init(void)
 	return 0;
 }
 
-
 void ip_vs_app_cleanup(void)
 {
 	proc_net_remove(&init_net, "ip_vs_app");
diff -uprN linux-2.6.32-220.23.1.el6.x86_64.bak/net/netfilter/ipvs/ip_vs_conn.c linux-2.6.32-220.23.1.el6.x86_64/net/netfilter/ipvs/ip_vs_conn.c
--- linux-2.6.32-220.23.1.el6.x86_64.bak/net/netfilter/ipvs/ip_vs_conn.c	2012-06-12 21:31:32.000000000 +0800
+++ linux-2.6.32-220.23.1.el6.x86_64/net/netfilter/ipvs/ip_vs_conn.c	2012-11-01 06:52:54.000000000 +0800
@@ -19,7 +19,14 @@
  * and others. Many code here is taken from IP MASQ code of kernel 2.2.
  *
  * Changes:
+ *             Yi Yang       <specific@gmail.com>
+ *             Wen Li        <steel.mental@gmail.com>
+ *             Yaoguang Sun  <sunyaoguang@gmail.com>
+ *             Jiaming Wu    <pukong.wjm@taobao.com>
  *
+ *             Modify connection manager to support FULLNAT (a new packet forwarding method)
+ *
+ *	          Yu Bo        <yubo@xiaomi.com>
  */
 
 #define KMSG_COMPONENT "IPVS"
@@ -31,7 +38,7 @@
 #include <linux/kernel.h>
 #include <linux/module.h>
 #include <linux/vmalloc.h>
-#include <linux/proc_fs.h>		/* for proc_net_* */
+#include <linux/proc_fs.h>	/* for proc_net_* */
 #include <linux/seq_file.h>
 #include <linux/jhash.h>
 #include <linux/random.h>
@@ -39,7 +46,6 @@
 #include <net/net_namespace.h>
 #include <net/ip_vs.h>
 
-
 /*
  *  Connection hash table: for input and output packets lookups of IPVS
  */
@@ -60,98 +66,140 @@ static unsigned int ip_vs_conn_rnd;
 /*
  *  Fine locking granularity for big connection hash table
  */
-#define CT_LOCKARRAY_BITS  4
+#define CT_LOCKARRAY_BITS  8
 #define CT_LOCKARRAY_SIZE  (1<<CT_LOCKARRAY_BITS)
 #define CT_LOCKARRAY_MASK  (CT_LOCKARRAY_SIZE-1)
 
-struct ip_vs_aligned_lock
-{
-	rwlock_t	l;
-} __attribute__((__aligned__(SMP_CACHE_BYTES)));
+struct ip_vs_aligned_lock {
+	rwlock_t l;
+} __attribute__ ((__aligned__(SMP_CACHE_BYTES)));
 
 /* lock array for conn table */
 static struct ip_vs_aligned_lock
-__ip_vs_conntbl_lock_array[CT_LOCKARRAY_SIZE] __cacheline_aligned;
+    __ip_vs_conntbl_lock_array[CT_LOCKARRAY_SIZE] __cacheline_aligned;
 
 static inline void ct_read_lock(unsigned key)
 {
-	read_lock(&__ip_vs_conntbl_lock_array[key&CT_LOCKARRAY_MASK].l);
+	read_lock(&__ip_vs_conntbl_lock_array[key & CT_LOCKARRAY_MASK].l);
 }
 
 static inline void ct_read_unlock(unsigned key)
 {
-	read_unlock(&__ip_vs_conntbl_lock_array[key&CT_LOCKARRAY_MASK].l);
+	read_unlock(&__ip_vs_conntbl_lock_array[key & CT_LOCKARRAY_MASK].l);
 }
 
 static inline void ct_write_lock(unsigned key)
 {
-	write_lock(&__ip_vs_conntbl_lock_array[key&CT_LOCKARRAY_MASK].l);
+	write_lock(&__ip_vs_conntbl_lock_array[key & CT_LOCKARRAY_MASK].l);
 }
 
 static inline void ct_write_unlock(unsigned key)
 {
-	write_unlock(&__ip_vs_conntbl_lock_array[key&CT_LOCKARRAY_MASK].l);
+	write_unlock(&__ip_vs_conntbl_lock_array[key & CT_LOCKARRAY_MASK].l);
 }
 
 static inline void ct_read_lock_bh(unsigned key)
 {
-	read_lock_bh(&__ip_vs_conntbl_lock_array[key&CT_LOCKARRAY_MASK].l);
+	read_lock_bh(&__ip_vs_conntbl_lock_array[key & CT_LOCKARRAY_MASK].l);
 }
 
 static inline void ct_read_unlock_bh(unsigned key)
 {
-	read_unlock_bh(&__ip_vs_conntbl_lock_array[key&CT_LOCKARRAY_MASK].l);
+	read_unlock_bh(&__ip_vs_conntbl_lock_array[key & CT_LOCKARRAY_MASK].l);
 }
 
 static inline void ct_write_lock_bh(unsigned key)
 {
-	write_lock_bh(&__ip_vs_conntbl_lock_array[key&CT_LOCKARRAY_MASK].l);
+	write_lock_bh(&__ip_vs_conntbl_lock_array[key & CT_LOCKARRAY_MASK].l);
 }
 
 static inline void ct_write_unlock_bh(unsigned key)
 {
-	write_unlock_bh(&__ip_vs_conntbl_lock_array[key&CT_LOCKARRAY_MASK].l);
+	write_unlock_bh(&__ip_vs_conntbl_lock_array[key & CT_LOCKARRAY_MASK].l);
 }
 
-
 /*
  *	Returns hash value for IPVS connection entry
  */
-static unsigned int ip_vs_conn_hashkey(int af, unsigned proto,
-				       const union nf_inet_addr *addr,
-				       __be16 port)
+static unsigned int ip_vs_conn_hashkey(int af, const union nf_inet_addr *s_addr,
+				       __be16 s_port,
+				       const union nf_inet_addr *d_addr,
+				       __be16 d_port)
 {
 #ifdef CONFIG_IP_VS_IPV6
 	if (af == AF_INET6)
-		return jhash_3words(jhash(addr, 16, ip_vs_conn_rnd),
-				    (__force u32)port, proto, ip_vs_conn_rnd)
-			& IP_VS_CONN_TAB_MASK;
+		return jhash_3words(jhash(s_addr, 16, ip_vs_conn_rnd),
+				    jhash(d_addr, 16, ip_vs_conn_rnd),
+				    ((__force u32) s_port) << 16 | (__force u32)
+				    d_port, ip_vs_conn_rnd)
+		    & IP_VS_CONN_TAB_MASK;
 #endif
-	return jhash_3words((__force u32)addr->ip, (__force u32)port, proto,
+	return jhash_3words((__force u32) s_addr->ip, (__force u32) d_addr->ip,
+			    ((__force u32) s_port) << 16 | (__force u32) d_port,
 			    ip_vs_conn_rnd)
-		& IP_VS_CONN_TAB_MASK;
+	    & IP_VS_CONN_TAB_MASK;
 }
 
+/*
+ * Lock two buckets of ip_vs_conn_tab
+ */
+static inline void ip_vs_conn_lock2(unsigned ihash, unsigned ohash)
+{
+	unsigned ilock, olock;
+
+	ilock = ihash & CT_LOCKARRAY_MASK;
+	olock = ohash & CT_LOCKARRAY_MASK;
+
+	/* lock the conntab bucket */
+	if (ilock < olock) {
+		ct_write_lock(ihash);
+		ct_write_lock(ohash);
+	} else if (ilock > olock) {
+		ct_write_lock(ohash);
+		ct_write_lock(ihash);
+	} else {
+		ct_write_lock(ihash);
+	}
+}
 
 /*
- *	Hashes ip_vs_conn in ip_vs_conn_tab by proto,addr,port.
- *	returns bool success.
+ * Unlock two buckets of ip_vs_conn_tab
  */
-static inline int ip_vs_conn_hash(struct ip_vs_conn *cp)
+static inline void ip_vs_conn_unlock2(unsigned ihash, unsigned ohash)
 {
-	unsigned hash;
-	int ret;
+	unsigned ilock, olock;
 
-	if (cp->flags & IP_VS_CONN_F_ONE_PACKET)
-		return 0;
+	ilock = ihash & CT_LOCKARRAY_MASK;
+	olock = ohash & CT_LOCKARRAY_MASK;
 
-	/* Hash by protocol, client address and port */
-	hash = ip_vs_conn_hashkey(cp->af, cp->protocol, &cp->caddr, cp->cport);
+	/* lock the conntab bucket */
+	if (ilock < olock) {
+		ct_write_unlock(ohash);
+		ct_write_unlock(ihash);
+	} else if (ilock > olock) {
+		ct_write_unlock(ihash);
+		ct_write_unlock(ohash);
+	} else {
+		ct_write_unlock(ihash);
+	}
+}
 
-	ct_write_lock(hash);
+/*
+ *      Hashed ip_vs_conn into ip_vs_conn_tab
+ *	returns bool success.
+ */
+
+static inline int __ip_vs_conn_hash(struct ip_vs_conn *cp, unsigned ihash,
+				    unsigned ohash)
+{
+	struct ip_vs_conn_idx *ci_idx, *co_idx;
+	int ret;
 
 	if (!(cp->flags & IP_VS_CONN_F_HASHED)) {
-		list_add(&cp->c_list, &ip_vs_conn_tab[hash]);
+		ci_idx = cp->in_idx;
+		co_idx = cp->out_idx;
+		list_add(&ci_idx->c_list, &ip_vs_conn_tab[ihash]);
+		list_add(&co_idx->c_list, &ip_vs_conn_tab[ohash]);
 		cp->flags |= IP_VS_CONN_F_HASHED;
 		atomic_inc(&cp->refcnt);
 		ret = 1;
@@ -161,66 +209,114 @@ static inline int ip_vs_conn_hash(struct
 		ret = 0;
 	}
 
-	ct_write_unlock(hash);
-
 	return ret;
 }
 
+/*
+ *	Hashed ip_vs_conn in two buckets of ip_vs_conn_tab
+ *	by caddr/cport/vaddr/vport and raddr/rport/laddr/lport,
+ *	returns bool success.
+ */
+static inline int ip_vs_conn_hash(struct ip_vs_conn *cp)
+{
+	unsigned ihash, ohash;
+	int ret;
+
+	if (cp->flags & IP_VS_CONN_F_ONE_PACKET)
+		return 0;
+
+	/*OUTside2INside: hashed by client address and port, virtual address and port */
+	ihash =
+	    ip_vs_conn_hashkey(cp->af, &cp->caddr, cp->cport, &cp->vaddr,
+			       cp->vport);
+	/*INside2OUTside: hashed by destination address and port, local address and port */
+	ohash =
+	    ip_vs_conn_hashkey(cp->af, &cp->daddr, cp->dport, &cp->laddr,
+			       cp->lport);
+
+	/* locked */
+	ip_vs_conn_lock2(ihash, ohash);
+
+	/* hashed */
+	ret = __ip_vs_conn_hash(cp, ihash, ohash);
+
+	/* unlocked */
+	ip_vs_conn_unlock2(ihash, ohash);
+
+	return ret;
+}
 
 /*
  *	UNhashes ip_vs_conn from ip_vs_conn_tab.
+ *	cp->refcnt must be equal 2,
  *	returns bool success.
  */
 static inline int ip_vs_conn_unhash(struct ip_vs_conn *cp)
 {
-	unsigned hash;
+	unsigned ihash, ohash;
+	struct ip_vs_conn_idx *ci_idx, *co_idx;
 	int ret;
 
-	/* unhash it and decrease its reference counter */
-	hash = ip_vs_conn_hashkey(cp->af, cp->protocol, &cp->caddr, cp->cport);
-
-	ct_write_lock(hash);
-
-	if (cp->flags & IP_VS_CONN_F_HASHED) {
-		list_del(&cp->c_list);
+	/* OUTside2INside: unhash it and decrease its reference counter */
+	ihash =
+	    ip_vs_conn_hashkey(cp->af, &cp->caddr, cp->cport, &cp->vaddr,
+			       cp->vport);
+	/* INside2OUTside: unhash it and decrease its reference counter */
+	ohash =
+	    ip_vs_conn_hashkey(cp->af, &cp->daddr, cp->dport, &cp->laddr,
+			       cp->lport);
+
+	/* locked */
+	ip_vs_conn_lock2(ihash, ohash);
+
+	/* unhashed */
+	if ((cp->flags & IP_VS_CONN_F_HASHED)
+	    && (atomic_read(&cp->refcnt) == 2)) {
+		ci_idx = cp->in_idx;
+		co_idx = cp->out_idx;
+		list_del(&ci_idx->c_list);
+		list_del(&co_idx->c_list);
 		cp->flags &= ~IP_VS_CONN_F_HASHED;
 		atomic_dec(&cp->refcnt);
 		ret = 1;
-	} else
+	} else {
 		ret = 0;
+	}
 
-	ct_write_unlock(hash);
+	/* unlocked */
+	ip_vs_conn_unlock2(ihash, ohash);
 
 	return ret;
 }
 
-
 /*
  *  Gets ip_vs_conn associated with supplied parameters in the ip_vs_conn_tab.
- *  Called for pkts coming from OUTside-to-INside.
- *	s_addr, s_port: pkt source address (foreign host)
- *	d_addr, d_port: pkt dest address (load balancer)
- */
-static inline struct ip_vs_conn *__ip_vs_conn_in_get
-(int af, int protocol, const union nf_inet_addr *s_addr, __be16 s_port,
- const union nf_inet_addr *d_addr, __be16 d_port)
-{
+ *  Return director: OUTside-to-INside or INside-to-OUTside in res_dir.
+ *	s_addr, s_port: pkt source address (foreign host/realserver)
+ *	d_addr, d_port: pkt dest address (virtual address/local address)
+ */
+static inline struct ip_vs_conn *__ip_vs_conn_get
+    (int af, int protocol, const union nf_inet_addr *s_addr, __be16 s_port,
+     const union nf_inet_addr *d_addr, __be16 d_port, int *res_dir) {
 	unsigned hash;
 	struct ip_vs_conn *cp;
+	struct ip_vs_conn_idx *cidx;
 
-	hash = ip_vs_conn_hashkey(af, protocol, s_addr, s_port);
+	hash = ip_vs_conn_hashkey(af, s_addr, s_port, d_addr, d_port);
 
 	ct_read_lock(hash);
 
-	list_for_each_entry(cp, &ip_vs_conn_tab[hash], c_list) {
-		if (cp->af == af &&
-		    ip_vs_addr_equal(af, s_addr, &cp->caddr) &&
-		    ip_vs_addr_equal(af, d_addr, &cp->vaddr) &&
-		    s_port == cp->cport && d_port == cp->vport &&
+	list_for_each_entry(cidx, &ip_vs_conn_tab[hash], c_list) {
+		cp = cidx->cp;
+		if (cidx->af == af &&
+		    ip_vs_addr_equal(af, s_addr, &cidx->s_addr) &&
+		    ip_vs_addr_equal(af, d_addr, &cidx->d_addr) &&
+		    s_port == cidx->s_port && d_port == cidx->d_port &&
 		    ((!s_port) ^ (!(cp->flags & IP_VS_CONN_F_NO_CPORT))) &&
-		    protocol == cp->protocol) {
+		    protocol == cidx->protocol) {
 			/* HIT */
 			atomic_inc(&cp->refcnt);
+			*res_dir = cidx->flags & IP_VS_CIDX_F_DIR_MASK;
 			ct_read_unlock(hash);
 			return cp;
 		}
@@ -231,18 +327,18 @@ static inline struct ip_vs_conn *__ip_vs
 	return NULL;
 }
 
-struct ip_vs_conn *ip_vs_conn_in_get
-(int af, int protocol, const union nf_inet_addr *s_addr, __be16 s_port,
- const union nf_inet_addr *d_addr, __be16 d_port)
-{
+struct ip_vs_conn *ip_vs_conn_get
+    (int af, int protocol, const union nf_inet_addr *s_addr, __be16 s_port,
+     const union nf_inet_addr *d_addr, __be16 d_port, int *res_dir) {
 	struct ip_vs_conn *cp;
 
-	cp = __ip_vs_conn_in_get(af, protocol, s_addr, s_port, d_addr, d_port);
+	cp = __ip_vs_conn_get(af, protocol, s_addr, s_port, d_addr, d_port,
+			      res_dir);
 	if (!cp && atomic_read(&ip_vs_conn_no_cport_cnt))
-		cp = __ip_vs_conn_in_get(af, protocol, s_addr, 0, d_addr,
-					 d_port);
+		cp = __ip_vs_conn_get(af, protocol, s_addr, 0, d_addr, d_port,
+				      res_dir);
 
-	IP_VS_DBG_BUF(9, "lookup/in %s %s:%d->%s:%d %s\n",
+	IP_VS_DBG_BUF(9, "lookup %s %s:%d->%s:%d %s\n",
 		      ip_vs_proto_name(protocol),
 		      IP_VS_DBG_ADDR(af, s_addr), ntohs(s_port),
 		      IP_VS_DBG_ADDR(af, d_addr), ntohs(d_port),
@@ -253,26 +349,27 @@ struct ip_vs_conn *ip_vs_conn_in_get
 
 /* Get reference to connection template */
 struct ip_vs_conn *ip_vs_ct_in_get
-(int af, int protocol, const union nf_inet_addr *s_addr, __be16 s_port,
- const union nf_inet_addr *d_addr, __be16 d_port)
-{
+    (int af, int protocol, const union nf_inet_addr *s_addr, __be16 s_port,
+     const union nf_inet_addr *d_addr, __be16 d_port) {
 	unsigned hash;
+	struct ip_vs_conn_idx *cidx;
 	struct ip_vs_conn *cp;
 
-	hash = ip_vs_conn_hashkey(af, protocol, s_addr, s_port);
+	hash = ip_vs_conn_hashkey(af, s_addr, s_port, d_addr, d_port);
 
 	ct_read_lock(hash);
 
-	list_for_each_entry(cp, &ip_vs_conn_tab[hash], c_list) {
-		if (cp->af == af &&
-		    ip_vs_addr_equal(af, s_addr, &cp->caddr) &&
+	list_for_each_entry(cidx, &ip_vs_conn_tab[hash], c_list) {
+		cp = cidx->cp;
+		if (cidx->af == af &&
+		    ip_vs_addr_equal(af, s_addr, &cidx->s_addr) &&
 		    /* protocol should only be IPPROTO_IP if
 		     * d_addr is a fwmark */
 		    ip_vs_addr_equal(protocol == IPPROTO_IP ? AF_UNSPEC : af,
-		                     d_addr, &cp->vaddr) &&
-		    s_port == cp->cport && d_port == cp->vport &&
+				     d_addr, &cidx->d_addr) &&
+		    s_port == cidx->s_port && d_port == cidx->d_port &&
 		    cp->flags & IP_VS_CONN_F_TEMPLATE &&
-		    protocol == cp->protocol) {
+		    protocol == cidx->protocol) {
 			/* HIT */
 			atomic_inc(&cp->refcnt);
 			goto out;
@@ -280,10 +377,10 @@ struct ip_vs_conn *ip_vs_ct_in_get
 	}
 	cp = NULL;
 
-  out:
+      out:
 	ct_read_unlock(hash);
 
-	IP_VS_DBG_BUF(9, "template lookup/in %s %s:%d->%s:%d %s\n",
+	IP_VS_DBG_BUF(9, "template lookup %s %s:%d->%s:%d %s\n",
 		      ip_vs_proto_name(protocol),
 		      IP_VS_DBG_ADDR(af, s_addr), ntohs(s_port),
 		      IP_VS_DBG_ADDR(af, d_addr), ntohs(d_port),
@@ -293,51 +390,6 @@ struct ip_vs_conn *ip_vs_ct_in_get
 }
 
 /*
- *  Gets ip_vs_conn associated with supplied parameters in the ip_vs_conn_tab.
- *  Called for pkts coming from inside-to-OUTside.
- *	s_addr, s_port: pkt source address (inside host)
- *	d_addr, d_port: pkt dest address (foreign host)
- */
-struct ip_vs_conn *ip_vs_conn_out_get
-(int af, int protocol, const union nf_inet_addr *s_addr, __be16 s_port,
- const union nf_inet_addr *d_addr, __be16 d_port)
-{
-	unsigned hash;
-	struct ip_vs_conn *cp, *ret=NULL;
-
-	/*
-	 *	Check for "full" addressed entries
-	 */
-	hash = ip_vs_conn_hashkey(af, protocol, d_addr, d_port);
-
-	ct_read_lock(hash);
-
-	list_for_each_entry(cp, &ip_vs_conn_tab[hash], c_list) {
-		if (cp->af == af &&
-		    ip_vs_addr_equal(af, d_addr, &cp->caddr) &&
-		    ip_vs_addr_equal(af, s_addr, &cp->daddr) &&
-		    d_port == cp->cport && s_port == cp->dport &&
-		    protocol == cp->protocol) {
-			/* HIT */
-			atomic_inc(&cp->refcnt);
-			ret = cp;
-			break;
-		}
-	}
-
-	ct_read_unlock(hash);
-
-	IP_VS_DBG_BUF(9, "lookup/out %s %s:%d->%s:%d %s\n",
-		      ip_vs_proto_name(protocol),
-		      IP_VS_DBG_ADDR(af, s_addr), ntohs(s_port),
-		      IP_VS_DBG_ADDR(af, d_addr), ntohs(d_port),
-		      ret ? "hit" : "not hit");
-
-	return ret;
-}
-
-
-/*
  *      Put back the conn and restart its timer with its timeout
  */
 void ip_vs_conn_put(struct ip_vs_conn *cp)
@@ -348,12 +400,11 @@ void ip_vs_conn_put(struct ip_vs_conn *c
 		timeout = 0;
 
 	/* reset it expire in its timeout */
-	mod_timer(&cp->timer, jiffies+timeout);
+	mod_timer(&cp->timer, jiffies + timeout);
 
 	__ip_vs_conn_put(cp);
 }
 
-
 /*
  *	Fill a no_client_port connection with a client port number
  */
@@ -373,7 +424,6 @@ void ip_vs_conn_fill_cport(struct ip_vs_
 	}
 }
 
-
 /*
  *	Bind a connection entry with the corresponding packet_xmit.
  *	Called by ip_vs_conn_new.
@@ -385,6 +435,10 @@ static inline void ip_vs_bind_xmit(struc
 		cp->packet_xmit = ip_vs_nat_xmit;
 		break;
 
+	case IP_VS_CONN_F_FULLNAT:
+		cp->packet_xmit = ip_vs_fnat_xmit;
+		break;
+
 	case IP_VS_CONN_F_TUNNEL:
 		cp->packet_xmit = ip_vs_tunnel_xmit;
 		break;
@@ -411,6 +465,10 @@ static inline void ip_vs_bind_xmit_v6(st
 		cp->packet_xmit = ip_vs_nat_xmit_v6;
 		break;
 
+	case IP_VS_CONN_F_FULLNAT:
+		cp->packet_xmit = ip_vs_fnat_xmit_v6;
+		break;
+
 	case IP_VS_CONN_F_TUNNEL:
 		cp->packet_xmit = ip_vs_tunnel_xmit_v6;
 		break;
@@ -430,11 +488,10 @@ static inline void ip_vs_bind_xmit_v6(st
 }
 #endif
 
-
 static inline int ip_vs_dest_totalconns(struct ip_vs_dest *dest)
 {
 	return atomic_read(&dest->activeconns)
-		+ atomic_read(&dest->inactconns);
+	    + atomic_read(&dest->inactconns);
 }
 
 /*
@@ -458,7 +515,7 @@ ip_vs_bind_dest(struct ip_vs_conn *cp, s
 		 * by sync, preserve the activity flag.
 		 */
 		cp->flags |= atomic_read(&dest->conn_flags) &
-			     (~IP_VS_CONN_F_INACTIVE);
+		    (~IP_VS_CONN_F_INACTIVE);
 	else
 		cp->flags |= atomic_read(&dest->conn_flags);
 	cp->dest = dest;
@@ -495,7 +552,6 @@ ip_vs_bind_dest(struct ip_vs_conn *cp, s
 		dest->flags |= IP_VS_DEST_F_OVERLOAD;
 }
 
-
 /*
  * Check if there is a destination for the connection, if so
  * bind the connection to the destination.
@@ -506,15 +562,13 @@ struct ip_vs_dest *ip_vs_try_bind_dest(s
 
 	if ((cp) && (!cp->dest)) {
 		dest = ip_vs_find_dest(cp->af, &cp->daddr, cp->dport,
-				       &cp->vaddr, cp->vport,
-				       cp->protocol);
+				       &cp->vaddr, cp->vport, cp->protocol);
 		ip_vs_bind_dest(cp, dest);
 		return dest;
 	} else
 		return NULL;
 }
 
-
 /*
  *	Unbind a connection entry with its VS destination
  *	Called by the ip_vs_conn_expire function.
@@ -571,6 +625,185 @@ static inline void ip_vs_unbind_dest(str
 	atomic_dec(&dest->refcnt);
 }
 
+/*
+ * get a local address from given virtual service
+ */
+static struct ip_vs_laddr *ip_vs_get_laddr(struct ip_vs_service *svc)
+{
+	struct ip_vs_laddr *local;
+	struct list_head *p, *q;
+
+	write_lock(&svc->laddr_lock);
+	p = svc->curr_laddr;
+	p = p->next;
+	q = p;
+	do {
+		/* skip list head */
+		if (q == &svc->laddr_list) {
+			q = q->next;
+			continue;
+		}
+		local = list_entry(q, struct ip_vs_laddr, n_list);
+		goto out;
+	} while (q != p);
+	write_unlock(&svc->laddr_lock);
+	return NULL;
+
+      out:
+	svc->curr_laddr = q;
+	write_unlock(&svc->laddr_lock);
+	return local;
+}
+
+/*
+ *	Bind a connection entry with a local address
+ *	and hashed it in connection table.
+ *	Called just after a new connection entry is created and destination has binded.
+ *	returns bool success.
+ */
+static inline int ip_vs_hbind_laddr(struct ip_vs_conn *cp)
+{
+	struct ip_vs_dest *dest = cp->dest;
+	struct ip_vs_service *svc = dest->svc;
+	struct ip_vs_laddr *local;
+	int ret = 0;
+	int remaining, i, tport, hit = 0;
+	unsigned ihash, ohash;
+	struct ip_vs_conn_idx *cidx;
+
+
+
+
+	/* fwd methods: not IP_VS_CONN_F_FULLNAT */
+	switch (IP_VS_FWD_METHOD(cp)) {
+	case IP_VS_CONN_F_MASQ:
+	case IP_VS_CONN_F_TUNNEL:
+	case IP_VS_CONN_F_DROUTE:
+	case IP_VS_CONN_F_LOCALNODE:
+	case IP_VS_CONN_F_BYPASS:
+		ip_vs_addr_copy(cp->af, &cp->out_idx->d_addr, &cp->caddr);
+		cp->out_idx->d_port = cp->cport;
+		ip_vs_addr_copy(cp->af, &cp->laddr, &cp->caddr);
+		cp->lport = cp->cport;
+		cp->local = NULL;
+		ip_vs_conn_hash(cp);
+		ret = 1;
+		goto out;
+	}
+
+	if (cp->flags & IP_VS_CONN_F_TEMPLATE) {
+		ip_vs_addr_copy(cp->af, &cp->out_idx->d_addr, &cp->caddr);
+		cp->out_idx->d_port = cp->cport;
+		ip_vs_addr_copy(cp->af, &cp->laddr, &cp->caddr);
+		cp->lport = cp->cport;
+		cp->local = NULL;
+		ip_vs_conn_hash(cp);
+		ret = 1;
+		goto out;
+	}
+	/*
+	 * fwd methods: IP_VS_CONN_F_FULLNAT
+	 */
+	/* choose a local address by round-robin */
+	local = ip_vs_get_laddr(svc);
+	if (local != NULL) {
+		/*OUTside2INside: hashed by client address and port, virtual address and port */
+		ihash =
+		    ip_vs_conn_hashkey(cp->af, &cp->caddr, cp->cport,
+				       &cp->vaddr, cp->vport);
+
+		/* increase the refcnt counter of the local address */
+		ip_vs_laddr_hold(local);
+		ip_vs_addr_copy(cp->af, &cp->out_idx->d_addr, &local->addr);
+		ip_vs_addr_copy(cp->af, &cp->laddr, &local->addr);
+		remaining = sysctl_ip_vs_lport_max - sysctl_ip_vs_lport_min + 1;
+		for (i = 0; i < sysctl_ip_vs_lport_tries; i++) {
+			/* choose a port */
+			tport =
+			    sysctl_ip_vs_lport_min +
+			    atomic64_inc_return(&local->port) % remaining;
+			cp->out_idx->d_port = cp->lport = htons(tport);
+
+			/* init hit everytime before lookup the tuple */
+			hit = 0;
+
+			/*INside2OUTside: hashed by destination address and port, local address and port */
+			ohash =
+			    ip_vs_conn_hashkey(cp->af, &cp->daddr, cp->dport,
+					       &cp->laddr, cp->lport);
+			/* lock the conntab bucket */
+			ip_vs_conn_lock2(ihash, ohash);
+			/*
+			 * check local address and port is valid by lookup connection table
+			 */
+			list_for_each_entry(cidx, &ip_vs_conn_tab[ohash],
+					    c_list) {
+				if (cidx->af == cp->af
+				    && ip_vs_addr_equal(cp->af, &cp->daddr,
+							&cidx->s_addr)
+				    && ip_vs_addr_equal(cp->af, &cp->laddr,
+							&cidx->d_addr)
+				    && cp->dport == cidx->s_port
+				    && cp->lport == cidx->d_port
+				    && cp->protocol == cidx->protocol) {
+					/* HIT */
+					atomic64_inc(&local->port_conflict);
+					hit = 1;
+					break;
+				}
+			}
+			if (hit == 0) {
+				cp->local = local;
+				/* hashed */
+				__ip_vs_conn_hash(cp, ihash, ohash);
+				ip_vs_conn_unlock2(ihash, ohash);
+				atomic_inc(&local->conn_counts);
+				ret = 1;
+				goto out;
+			}
+			ip_vs_conn_unlock2(ihash, ohash);
+		}
+		if (ret == 0) {
+			ip_vs_laddr_put(local);
+		}
+	}
+	ret = 0;
+
+      out:
+	return ret;
+}
+
+/*
+ *	Unbind a connection entry with its local address
+ *	Called by the ip_vs_conn_expire function.
+ */
+static inline void ip_vs_unbind_laddr(struct ip_vs_conn *cp)
+{
+	struct ip_vs_laddr *local = cp->local;
+
+	if (!local)
+		return;
+
+	IP_VS_DBG_BUF(7, "Unbind-laddr %s c:%s:%d v:%s:%d l:%s:%d "
+		      "d:%s:%d fwd:%c s:%u conn->flags:%X conn->refcnt:%d "
+		      "local->refcnt:%d\n",
+		      ip_vs_proto_name(cp->protocol),
+		      IP_VS_DBG_ADDR(cp->af, &cp->caddr), ntohs(cp->cport),
+		      IP_VS_DBG_ADDR(cp->af, &cp->vaddr), ntohs(cp->vport),
+		      IP_VS_DBG_ADDR(cp->af, &cp->laddr), ntohs(cp->lport),
+		      IP_VS_DBG_ADDR(cp->af, &cp->daddr), ntohs(cp->dport),
+		      ip_vs_fwd_tag(cp), cp->state,
+		      cp->flags, atomic_read(&cp->refcnt),
+		      atomic_read(&local->refcnt));
+
+	/* Update the connection counters */
+	atomic_dec(&local->conn_counts);
+
+	/*
+	 * Simply decrease the refcnt of the local address;
+	 */
+	ip_vs_laddr_put(local);
+}
 
 /*
  *	Checking if the destination of a connection template is available.
@@ -580,7 +813,7 @@ static inline void ip_vs_unbind_dest(str
 int ip_vs_check_template(struct ip_vs_conn *ct)
 {
 	struct ip_vs_dest *dest = ct->dest;
-
+	
 	/*
 	 * Checking the dest server status.
 	 */
@@ -590,12 +823,14 @@ int ip_vs_check_template(struct ip_vs_co
 	     (atomic_read(&dest->weight) == 0))) {
 		IP_VS_DBG_BUF(9, "check_template: dest not available for "
 			      "protocol %s s:%s:%d v:%s:%d "
-			      "-> d:%s:%d\n",
+			      "-> l:%s:%d d:%s:%d\n",
 			      ip_vs_proto_name(ct->protocol),
 			      IP_VS_DBG_ADDR(ct->af, &ct->caddr),
 			      ntohs(ct->cport),
 			      IP_VS_DBG_ADDR(ct->af, &ct->vaddr),
 			      ntohs(ct->vport),
+			      IP_VS_DBG_ADDR(ct->af, &ct->laddr),
+			      ntohs(ct->lport),
 			      IP_VS_DBG_ADDR(ct->af, &ct->daddr),
 			      ntohs(ct->dport));
 
@@ -606,6 +841,7 @@ int ip_vs_check_template(struct ip_vs_co
 			if (ip_vs_conn_unhash(ct)) {
 				ct->dport = htons(0xffff);
 				ct->vport = htons(0xffff);
+				ct->lport = 0;
 				ct->cport = 0;
 				ip_vs_conn_hash(ct);
 			}
@@ -621,31 +857,85 @@ int ip_vs_check_template(struct ip_vs_co
 	return 1;
 }
 
+/* Warning: only be allowed call in ip_vs_conn_new */
+static void ip_vs_conn_del(struct ip_vs_conn *cp)
+{
+	if (cp == NULL)
+		return;
+
+	/* delete the timer if it is activated by other users */
+	if (timer_pending(&cp->timer))
+		del_timer(&cp->timer);
+
+	/* does anybody control me? */
+	if (cp->control)
+		ip_vs_control_del(cp);
+
+	if (unlikely(cp->app != NULL))
+		ip_vs_unbind_app(cp);
+	ip_vs_unbind_dest(cp);
+	ip_vs_unbind_laddr(cp);
+	if (cp->flags & IP_VS_CONN_F_NO_CPORT)
+		atomic_dec(&ip_vs_conn_no_cport_cnt);
+	atomic_dec(&ip_vs_conn_count);
+
+	kmem_cache_free(ip_vs_conn_cachep, cp);
+	cp = NULL;
+}
+
 static void ip_vs_conn_expire(unsigned long data)
 {
 	struct ip_vs_conn *cp = (struct ip_vs_conn *)data;
+	struct sk_buff *tmp_skb = NULL;
+	struct ip_vs_protocol *pp = ip_vs_proto_get(cp->protocol);
 
-	cp->timeout = 60*HZ;
+	/*
+	 * Set proper timeout.
+	 */
+	if ((pp != NULL) && (pp->timeout_table != NULL)) {
+		cp->timeout = pp->timeout_table[cp->state];
+	} else {
+		cp->timeout = 60 * HZ;
+	}
 
 	/*
-	 *	hey, I'm using it
+	 *      hey, I'm using it
 	 */
 	atomic_inc(&cp->refcnt);
 
 	/*
-	 *	do I control anybody?
+	 * Retransmit syn packet to rs.
+	 * We just check syn_skb is not NULL, as syn_skb 
+	 * is stored only if syn-proxy is enabled.
+	 */
+	spin_lock(&cp->lock);
+	if (cp->syn_skb != NULL && atomic_read(&cp->syn_retry_max) > 0) {
+		atomic_dec(&cp->syn_retry_max);
+		if (cp->packet_xmit) {
+			tmp_skb = skb_copy(cp->syn_skb, GFP_ATOMIC);
+			cp->packet_xmit(tmp_skb, cp, pp);
+		}
+		/* statistics */
+		IP_VS_INC_ESTATS(ip_vs_esmib, SYNPROXY_RS_ERROR);
+		spin_unlock(&cp->lock);
+		goto expire_later;
+	}
+	spin_unlock(&cp->lock);
+
+	/*
+	 *      do I control anybody?
 	 */
 	if (atomic_read(&cp->n_control))
 		goto expire_later;
 
 	/*
-	 *	unhash it if it is hashed in the conn table
+	 *      unhash it if it is hashed in the conn table
 	 */
 	if (!ip_vs_conn_unhash(cp) && !(cp->flags & IP_VS_CONN_F_ONE_PACKET))
 		goto expire_later;
 
 	/*
-	 *	refcnt==1 implies I'm the only one referrer
+	 *      refcnt==1 implies I'm the only one referrer
 	 */
 	if (likely(atomic_read(&cp->refcnt) == 1)) {
 		/* delete the timer if it is activated by other users */
@@ -656,13 +946,29 @@ static void ip_vs_conn_expire(unsigned l
 		if (cp->control)
 			ip_vs_control_del(cp);
 
+		if (pp->conn_expire_handler)
+			pp->conn_expire_handler(pp, cp);
+
 		if (unlikely(cp->app != NULL))
 			ip_vs_unbind_app(cp);
 		ip_vs_unbind_dest(cp);
+		ip_vs_unbind_laddr(cp);
 		if (cp->flags & IP_VS_CONN_F_NO_CPORT)
 			atomic_dec(&ip_vs_conn_no_cport_cnt);
 		atomic_dec(&ip_vs_conn_count);
 
+		/* free stored ack packet */
+		while ((tmp_skb = skb_dequeue(&cp->ack_skb)) != NULL) {
+			kfree_skb(tmp_skb);
+			tmp_skb = NULL;
+		}
+
+		/* free stored syn skb */
+		if ((tmp_skb = xchg(&cp->syn_skb, NULL)) != NULL) {
+			kfree_skb(tmp_skb);
+			tmp_skb = NULL;
+		}
+
 		kmem_cache_free(ip_vs_conn_cachep, cp);
 		return;
 	}
@@ -670,15 +976,13 @@ static void ip_vs_conn_expire(unsigned l
 	/* hash it back to the table */
 	ip_vs_conn_hash(cp);
 
-  expire_later:
+      expire_later:
 	IP_VS_DBG(7, "delayed: conn->refcnt-1=%d conn->n_control=%d\n",
-		  atomic_read(&cp->refcnt)-1,
-		  atomic_read(&cp->n_control));
+		  atomic_read(&cp->refcnt) - 1, atomic_read(&cp->n_control));
 
 	ip_vs_conn_put(cp);
 }
 
-
 void ip_vs_conn_expire_now(struct ip_vs_conn *cp)
 {
 	if (del_timer(&cp->timer))
@@ -686,17 +990,25 @@ void ip_vs_conn_expire_now(struct ip_vs_
 }
 
 
+
+
+
+
 /*
  *	Create a new connection entry and hash it into the ip_vs_conn_tab
  */
-struct ip_vs_conn *
-ip_vs_conn_new(int af, int proto, const union nf_inet_addr *caddr, __be16 cport,
-	       const union nf_inet_addr *vaddr, __be16 vport,
-	       const union nf_inet_addr *daddr, __be16 dport, unsigned flags,
-	       struct ip_vs_dest *dest)
+struct ip_vs_conn *ip_vs_conn_new(int af, int proto,
+				  const union nf_inet_addr *caddr, __be16 cport,
+				  const union nf_inet_addr *vaddr, __be16 vport,
+				  const union nf_inet_addr *daddr, __be16 dport,
+				  unsigned flags, struct ip_vs_dest *dest,
+				  struct sk_buff *skb, int is_synproxy_on)
 {
 	struct ip_vs_conn *cp;
 	struct ip_vs_protocol *pp = ip_vs_proto_get(proto);
+	struct ip_vs_conn_idx *ci_idx, *co_idx;
+	struct tcphdr _tcph, *th;
+	
 
 	cp = kmem_cache_zalloc(ip_vs_conn_cachep, GFP_ATOMIC);
 	if (cp == NULL) {
@@ -704,20 +1016,50 @@ ip_vs_conn_new(int af, int proto, const 
 		return NULL;
 	}
 
-	INIT_LIST_HEAD(&cp->c_list);
+	/* init connection index of OUTside2INside */
+	ci_idx =
+	    (struct ip_vs_conn_idx *)(((__u8 *) cp) +
+				      sizeof(struct ip_vs_conn));
+	INIT_LIST_HEAD(&ci_idx->c_list);
+	ci_idx->af = af;
+	ci_idx->protocol = proto;
+	ip_vs_addr_copy(af, &ci_idx->s_addr, caddr);
+	ci_idx->s_port = cport;
+	ip_vs_addr_copy(af, &ci_idx->d_addr, vaddr);
+	ci_idx->d_port = vport;
+	ci_idx->flags |= IP_VS_CIDX_F_OUT2IN;
+	ci_idx->cp = cp;
+
+	/* init connection index of INside2OUTside */
+	co_idx =
+	    (struct ip_vs_conn_idx *)(((__u8 *) cp) +
+				      sizeof(struct ip_vs_conn) +
+				      sizeof(struct ip_vs_conn_idx));
+	INIT_LIST_HEAD(&co_idx->c_list);
+	co_idx->af = af;
+	co_idx->protocol = proto;
+	ip_vs_addr_copy(proto == IPPROTO_IP ? AF_UNSPEC : af,
+			&co_idx->s_addr, daddr);
+	co_idx->s_port = dport;
+	co_idx->flags |= IP_VS_CIDX_F_IN2OUT;
+	co_idx->cp = cp;
+
+	/* now init connection */
 	setup_timer(&cp->timer, ip_vs_conn_expire, (unsigned long)cp);
-	cp->af		   = af;
-	cp->protocol	   = proto;
+	cp->af = af;
+	cp->protocol = proto;
 	ip_vs_addr_copy(af, &cp->caddr, caddr);
-	cp->cport	   = cport;
+	cp->cport = cport;
 	ip_vs_addr_copy(af, &cp->vaddr, vaddr);
-	cp->vport	   = vport;
+	cp->vport = vport;
 	/* proto should only be IPPROTO_IP if d_addr is a fwmark */
 	ip_vs_addr_copy(proto == IPPROTO_IP ? AF_UNSPEC : af,
 			&cp->daddr, daddr);
-	cp->dport          = dport;
-	cp->flags	   = flags;
+	cp->dport = dport;
+	cp->flags = flags;
 	spin_lock_init(&cp->lock);
+	cp->in_idx = ci_idx;
+	cp->out_idx = co_idx;
 
 	/*
 	 * Set the entry is referenced by the current thread before hashing
@@ -738,7 +1080,7 @@ ip_vs_conn_new(int af, int proto, const 
 
 	/* Set its state and timeout */
 	cp->state = 0;
-	cp->timeout = 3*HZ;
+	cp->timeout = 3 * HZ;
 
 	/* Bind its packet transmitter */
 #ifdef CONFIG_IP_VS_IPV6
@@ -751,13 +1093,51 @@ ip_vs_conn_new(int af, int proto, const 
 	if (unlikely(pp && atomic_read(&pp->appcnt)))
 		ip_vs_bind_app(cp, pp);
 
-	/* Hash it in the ip_vs_conn_tab finally */
-	ip_vs_conn_hash(cp);
+
+	/* Set syn-proxy members 
+	 * Set cp->flag manually to avoid svn->flags change when 
+	 * ack_skb is on the way
+	 */
+	skb_queue_head_init(&cp->ack_skb);
+	atomic_set(&cp->syn_retry_max, 0);
+	if (is_synproxy_on == 1 && skb != NULL) {
+
+		th = skb_header_pointer(skb, ip_hdr(skb)->ihl * 4,
+					sizeof(_tcph), &_tcph);
+		if (th == NULL) {
+			IP_VS_ERR_RL("%s(): get tcphdr failed\n", __func__);
+			ip_vs_conn_del(cp);
+			return NULL;
+		}
+		/* Set syn-proxy flag */
+		cp->flags |= IP_VS_CONN_F_SYNPROXY;
+
+		/* Save ack packet */
+		skb_queue_tail(&cp->ack_skb, skb);
+		/* Save ack_seq - 1 */
+		cp->syn_proxy_seq.init_seq =
+		    htonl((__u32) ((htonl(th->ack_seq) - 1)));
+		/* Use IP_VS_TCP_S_SYN_SENT for syn */
+		cp->timeout = pp->timeout_table[cp->state =
+						IP_VS_TCP_S_SYN_SENT];
+	} else {
+		/* Unset syn-proxy flag */
+		cp->flags &= ~IP_VS_CONN_F_SYNPROXY;
+	}
+
+	/*
+	 * bind the connection with a local address
+	 * and hash it in the ip_vs_conn_tab finally.
+	 */
+	if (unlikely(ip_vs_hbind_laddr(cp) == 0)) {
+		IP_VS_ERR_RL("bind local address: no port available\n");
+		ip_vs_conn_del(cp);
+		return NULL;
+	}
 
 	return cp;
 }
 
-
 /*
  *	/proc/net/ip_vs_conn entries
  */
@@ -766,14 +1146,14 @@ ip_vs_conn_new(int af, int proto, const 
 static void *ip_vs_conn_array(struct seq_file *seq, loff_t pos)
 {
 	int idx;
-	struct ip_vs_conn *cp;
+	struct ip_vs_conn_idx *cidx;
 
-	for(idx = 0; idx < IP_VS_CONN_TAB_SIZE; idx++) {
+	for (idx = 0; idx < IP_VS_CONN_TAB_SIZE; idx++) {
 		ct_read_lock_bh(idx);
-		list_for_each_entry(cp, &ip_vs_conn_tab[idx], c_list) {
-			if (pos-- == 0) {
+		list_for_each_entry(cidx, &ip_vs_conn_tab[idx], c_list) {
+			if ((cidx->flags & IP_VS_CIDX_F_OUT2IN) && (pos-- == 0)) {
 				seq->private = &ip_vs_conn_tab[idx];
-				return cp;
+				return cidx->cp;
 			}
 		}
 		ct_read_unlock_bh(idx);
@@ -782,34 +1162,42 @@ static void *ip_vs_conn_array(struct seq
 	return NULL;
 }
 
-static void *ip_vs_conn_seq_start(struct seq_file *seq, loff_t *pos)
+static void *ip_vs_conn_seq_start(struct seq_file *seq, loff_t * pos)
 {
 	seq->private = NULL;
-	return *pos ? ip_vs_conn_array(seq, *pos - 1) :SEQ_START_TOKEN;
+	return *pos ? ip_vs_conn_array(seq, *pos - 1) : SEQ_START_TOKEN;
 }
 
-static void *ip_vs_conn_seq_next(struct seq_file *seq, void *v, loff_t *pos)
+static void *ip_vs_conn_seq_next(struct seq_file *seq, void *v, loff_t * pos)
 {
 	struct ip_vs_conn *cp = v;
 	struct list_head *e, *l = seq->private;
+	struct ip_vs_conn_idx *cidx;
 	int idx;
 
 	++*pos;
 	if (v == SEQ_START_TOKEN)
 		return ip_vs_conn_array(seq, 0);
 
+	cidx = cp->in_idx;
 	/* more on same hash chain? */
-	if ((e = cp->c_list.next) != l)
-		return list_entry(e, struct ip_vs_conn, c_list);
+	while ((e = cidx->c_list.next) != l) {
+		cidx = list_entry(e, struct ip_vs_conn_idx, c_list);
+		if (cidx->flags & IP_VS_CIDX_F_OUT2IN) {
+			return cidx->cp;
+		}
+	}
 
 	idx = l - ip_vs_conn_tab;
 	ct_read_unlock_bh(idx);
 
 	while (++idx < IP_VS_CONN_TAB_SIZE) {
 		ct_read_lock_bh(idx);
-		list_for_each_entry(cp, &ip_vs_conn_tab[idx], c_list) {
-			seq->private = &ip_vs_conn_tab[idx];
-			return cp;
+		list_for_each_entry(cidx, &ip_vs_conn_tab[idx], c_list) {
+			if (cidx->flags & IP_VS_CIDX_F_OUT2IN) {
+				seq->private = &ip_vs_conn_tab[idx];
+				return cidx->cp;
+			}
 		}
 		ct_read_unlock_bh(idx);
 	}
@@ -830,39 +1218,42 @@ static int ip_vs_conn_seq_show(struct se
 
 	if (v == SEQ_START_TOKEN)
 		seq_puts(seq,
-   "Pro FromIP   FPrt ToIP     TPrt DestIP   DPrt State       Expires\n");
+			 "Pro FromIP   FPrt ToIP     TPrt LocalIP  LPrt DestIP   DPrt State       Expires\n");
 	else {
 		const struct ip_vs_conn *cp = v;
 
 #ifdef CONFIG_IP_VS_IPV6
 		if (cp->af == AF_INET6)
-			seq_printf(seq, "%-3s %pI6 %04X %pI6 %04X %pI6 %04X %-11s %7lu\n",
-				ip_vs_proto_name(cp->protocol),
-				&cp->caddr.in6, ntohs(cp->cport),
-				&cp->vaddr.in6, ntohs(cp->vport),
-				&cp->daddr.in6, ntohs(cp->dport),
-				ip_vs_state_name(cp->protocol, cp->state),
-				(cp->timer.expires-jiffies)/HZ);
+			seq_printf(seq,
+				   "%-3s %pI6 %04X %pI6 %04X %pI6 %04X %pI6 %04X %-11s %7lu\n",
+				   ip_vs_proto_name(cp->protocol),
+				   &cp->caddr.in6, ntohs(cp->cport),
+				   &cp->vaddr.in6, ntohs(cp->vport),
+				   &cp->laddr.in6, ntohs(cp->lport),
+				   &cp->daddr.in6, ntohs(cp->dport),
+				   ip_vs_state_name(cp->protocol, cp->state),
+				   (cp->timer.expires - jiffies) / HZ);
 		else
 #endif
 			seq_printf(seq,
-				"%-3s %08X %04X %08X %04X"
-				" %08X %04X %-11s %7lu\n",
-				ip_vs_proto_name(cp->protocol),
-				ntohl(cp->caddr.ip), ntohs(cp->cport),
-				ntohl(cp->vaddr.ip), ntohs(cp->vport),
-				ntohl(cp->daddr.ip), ntohs(cp->dport),
-				ip_vs_state_name(cp->protocol, cp->state),
-				(cp->timer.expires-jiffies)/HZ);
+				   "%-3s %08X %04X %08X %04X"
+				   " %08X %04X %08X %04X %-11s %7lu\n",
+				   ip_vs_proto_name(cp->protocol),
+				   ntohl(cp->caddr.ip), ntohs(cp->cport),
+				   ntohl(cp->vaddr.ip), ntohs(cp->vport),
+				   ntohl(cp->laddr.ip), ntohs(cp->lport),
+				   ntohl(cp->daddr.ip), ntohs(cp->dport),
+				   ip_vs_state_name(cp->protocol, cp->state),
+				   (cp->timer.expires - jiffies) / HZ);
 	}
 	return 0;
 }
 
 static const struct seq_operations ip_vs_conn_seq_ops = {
 	.start = ip_vs_conn_seq_start,
-	.next  = ip_vs_conn_seq_next,
-	.stop  = ip_vs_conn_seq_stop,
-	.show  = ip_vs_conn_seq_show,
+	.next = ip_vs_conn_seq_next,
+	.stop = ip_vs_conn_seq_stop,
+	.show = ip_vs_conn_seq_show,
 };
 
 static int ip_vs_conn_open(struct inode *inode, struct file *file)
@@ -871,10 +1262,10 @@ static int ip_vs_conn_open(struct inode 
 }
 
 static const struct file_operations ip_vs_conn_fops = {
-	.owner	 = THIS_MODULE,
-	.open    = ip_vs_conn_open,
-	.read    = seq_read,
-	.llseek  = seq_lseek,
+	.owner = THIS_MODULE,
+	.open = ip_vs_conn_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
 	.release = seq_release,
 };
 
@@ -891,41 +1282,44 @@ static int ip_vs_conn_sync_seq_show(stru
 
 	if (v == SEQ_START_TOKEN)
 		seq_puts(seq,
-   "Pro FromIP   FPrt ToIP     TPrt DestIP   DPrt State       Origin Expires\n");
+			 "Pro FromIP   FPrt ToIP     TPrt LocalIP  LPrt DestIP   DPrt State       Origin Expires\n");
 	else {
 		const struct ip_vs_conn *cp = v;
 
 #ifdef CONFIG_IP_VS_IPV6
 		if (cp->af == AF_INET6)
-			seq_printf(seq, "%-3s %pI6 %04X %pI6 %04X %pI6 %04X %-11s %-6s %7lu\n",
-				ip_vs_proto_name(cp->protocol),
-				&cp->caddr.in6, ntohs(cp->cport),
-				&cp->vaddr.in6, ntohs(cp->vport),
-				&cp->daddr.in6, ntohs(cp->dport),
-				ip_vs_state_name(cp->protocol, cp->state),
-				ip_vs_origin_name(cp->flags),
-				(cp->timer.expires-jiffies)/HZ);
+			seq_printf(seq,
+				   "%-3s %pI6 %04X %pI6 %04X %pI6 %04X %pI6 %04X %-11s %-6s %7lu\n",
+				   ip_vs_proto_name(cp->protocol),
+				   &cp->caddr.in6, ntohs(cp->cport),
+				   &cp->vaddr.in6, ntohs(cp->vport),
+				   &cp->laddr.in6, ntohs(cp->lport),
+				   &cp->daddr.in6, ntohs(cp->dport),
+				   ip_vs_state_name(cp->protocol, cp->state),
+				   ip_vs_origin_name(cp->flags),
+				   (cp->timer.expires - jiffies) / HZ);
 		else
 #endif
 			seq_printf(seq,
-				"%-3s %08X %04X %08X %04X "
-				"%08X %04X %-11s %-6s %7lu\n",
-				ip_vs_proto_name(cp->protocol),
-				ntohl(cp->caddr.ip), ntohs(cp->cport),
-				ntohl(cp->vaddr.ip), ntohs(cp->vport),
-				ntohl(cp->daddr.ip), ntohs(cp->dport),
-				ip_vs_state_name(cp->protocol, cp->state),
-				ip_vs_origin_name(cp->flags),
-				(cp->timer.expires-jiffies)/HZ);
+				   "%-3s %08X %04X %08X %04X "
+				   "%08X %04X %08X %04X %-11s %-6s %7lu\n",
+				   ip_vs_proto_name(cp->protocol),
+				   ntohl(cp->caddr.ip), ntohs(cp->cport),
+				   ntohl(cp->vaddr.ip), ntohs(cp->vport),
+				   ntohl(cp->laddr.ip), ntohs(cp->lport),
+				   ntohl(cp->daddr.ip), ntohs(cp->dport),
+				   ip_vs_state_name(cp->protocol, cp->state),
+				   ip_vs_origin_name(cp->flags),
+				   (cp->timer.expires - jiffies) / HZ);
 	}
 	return 0;
 }
 
 static const struct seq_operations ip_vs_conn_sync_seq_ops = {
 	.start = ip_vs_conn_seq_start,
-	.next  = ip_vs_conn_seq_next,
-	.stop  = ip_vs_conn_seq_stop,
-	.show  = ip_vs_conn_sync_seq_show,
+	.next = ip_vs_conn_seq_next,
+	.stop = ip_vs_conn_seq_stop,
+	.show = ip_vs_conn_sync_seq_show,
 };
 
 static int ip_vs_conn_sync_open(struct inode *inode, struct file *file)
@@ -934,16 +1328,15 @@ static int ip_vs_conn_sync_open(struct i
 }
 
 static const struct file_operations ip_vs_conn_sync_fops = {
-	.owner	 = THIS_MODULE,
-	.open    = ip_vs_conn_sync_open,
-	.read    = seq_read,
-	.llseek  = seq_lseek,
+	.owner = THIS_MODULE,
+	.open = ip_vs_conn_sync_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
 	.release = seq_release,
 };
 
 #endif
 
-
 /*
  *      Randomly drop connection entries before running out of memory
  */
@@ -953,23 +1346,26 @@ static inline int todrop_entry(struct ip
 	 * The drop rate array needs tuning for real environments.
 	 * Called from timer bh only => no locking
 	 */
-	static const char todrop_rate[9] = {0, 1, 2, 3, 4, 5, 6, 7, 8};
-	static char todrop_counter[9] = {0};
+	static const char todrop_rate[9] = { 0, 1, 2, 3, 4, 5, 6, 7, 8 };
+	static char todrop_counter[9] = { 0 };
 	int i;
 
 	/* if the conn entry hasn't lasted for 60 seconds, don't drop it.
 	   This will leave enough time for normal connection to get
 	   through. */
-	if (time_before(cp->timeout + jiffies, cp->timer.expires + 60*HZ))
+	if (time_before(cp->timeout + jiffies, cp->timer.expires + 60 * HZ))
 		return 0;
 
 	/* Don't drop the entry if its number of incoming packets is not
 	   located in [0, 8] */
 	i = atomic_read(&cp->in_pkts);
-	if (i > 8 || i < 0) return 0;
+	if (i > 8 || i < 0)
+		return 0;
 
-	if (!todrop_rate[i]) return 0;
-	if (--todrop_counter[i] > 0) return 0;
+	if (!todrop_rate[i])
+		return 0;
+	if (--todrop_counter[i] > 0)
+		return 0;
 
 	todrop_counter[i] = todrop_rate[i];
 	return 1;
@@ -980,11 +1376,12 @@ void ip_vs_random_dropentry(void)
 {
 	int idx;
 	struct ip_vs_conn *cp;
+	struct ip_vs_conn_idx *cidx;
 
 	/*
 	 * Randomly scan 1/32 of the whole table every second
 	 */
-	for (idx = 0; idx < (IP_VS_CONN_TAB_SIZE>>5); idx++) {
+	for (idx = 0; idx < (IP_VS_CONN_TAB_SIZE >> 5); idx++) {
 		unsigned hash = net_random() & IP_VS_CONN_TAB_MASK;
 
 		/*
@@ -992,13 +1389,14 @@ void ip_vs_random_dropentry(void)
 		 */
 		ct_write_lock_bh(hash);
 
-		list_for_each_entry(cp, &ip_vs_conn_tab[hash], c_list) {
+		list_for_each_entry(cidx, &ip_vs_conn_tab[hash], c_list) {
+			cp = cidx->cp;
 			if (cp->flags & IP_VS_CONN_F_TEMPLATE)
 				/* connection template */
 				continue;
 
 			if (cp->protocol == IPPROTO_TCP) {
-				switch(cp->state) {
+				switch (cp->state) {
 				case IP_VS_TCP_S_SYN_RECV:
 				case IP_VS_TCP_S_SYNACK:
 					break;
@@ -1027,7 +1425,6 @@ void ip_vs_random_dropentry(void)
 	}
 }
 
-
 /*
  *      Flush all the connection entries in the ip_vs_conn_tab
  */
@@ -1035,17 +1432,18 @@ static void ip_vs_conn_flush(void)
 {
 	int idx;
 	struct ip_vs_conn *cp;
+	struct ip_vs_conn_idx *cidx;
 
-  flush_again:
-	for (idx=0; idx<IP_VS_CONN_TAB_SIZE; idx++) {
+      flush_again:
+	for (idx = 0; idx < IP_VS_CONN_TAB_SIZE; idx++) {
 		/*
 		 *  Lock is actually needed in this loop.
 		 */
 		ct_write_lock_bh(idx);
 
-		list_for_each_entry(cp, &ip_vs_conn_tab[idx], c_list) {
-
+		list_for_each_entry(cidx, &ip_vs_conn_tab[idx], c_list) {
 			IP_VS_DBG(4, "del connection\n");
+			cp = cidx->cp;
 			ip_vs_conn_expire_now(cp);
 			if (cp->control) {
 				IP_VS_DBG(4, "del conn template\n");
@@ -1063,7 +1461,6 @@ static void ip_vs_conn_flush(void)
 	}
 }
 
-
 int __init ip_vs_conn_init(void)
 {
 	int idx;
@@ -1071,14 +1468,16 @@ int __init ip_vs_conn_init(void)
 	/*
 	 * Allocate the connection hash table and initialize its list heads
 	 */
-	ip_vs_conn_tab = vmalloc(IP_VS_CONN_TAB_SIZE*sizeof(struct list_head));
+	ip_vs_conn_tab =
+	    vmalloc(IP_VS_CONN_TAB_SIZE * (sizeof(struct list_head)));
 	if (!ip_vs_conn_tab)
 		return -ENOMEM;
 
 	/* Allocate ip_vs_conn slab cache */
 	ip_vs_conn_cachep = kmem_cache_create("ip_vs_conn",
-					      sizeof(struct ip_vs_conn), 0,
-					      SLAB_HWCACHE_ALIGN, NULL);
+					      sizeof(struct ip_vs_conn) +
+					      2 * sizeof(struct ip_vs_conn_idx),
+					      0, SLAB_HWCACHE_ALIGN, NULL);
 	if (!ip_vs_conn_cachep) {
 		vfree(ip_vs_conn_tab);
 		return -ENOMEM;
@@ -1087,20 +1486,22 @@ int __init ip_vs_conn_init(void)
 	pr_info("Connection hash table configured "
 		"(size=%d, memory=%ldKbytes)\n",
 		IP_VS_CONN_TAB_SIZE,
-		(long)(IP_VS_CONN_TAB_SIZE*sizeof(struct list_head))/1024);
+		(long)(IP_VS_CONN_TAB_SIZE * sizeof(struct list_head)) / 1024);
 	IP_VS_DBG(0, "Each connection entry needs %Zd bytes at least\n",
-		  sizeof(struct ip_vs_conn));
+		  sizeof(struct ip_vs_conn) +
+		  2 * sizeof(struct ip_vs_conn_idx));
 
 	for (idx = 0; idx < IP_VS_CONN_TAB_SIZE; idx++) {
 		INIT_LIST_HEAD(&ip_vs_conn_tab[idx]);
 	}
 
-	for (idx = 0; idx < CT_LOCKARRAY_SIZE; idx++)  {
+	for (idx = 0; idx < CT_LOCKARRAY_SIZE; idx++) {
 		rwlock_init(&__ip_vs_conntbl_lock_array[idx].l);
 	}
 
 	proc_net_fops_create(&init_net, "ip_vs_conn", 0, &ip_vs_conn_fops);
-	proc_net_fops_create(&init_net, "ip_vs_conn_sync", 0, &ip_vs_conn_sync_fops);
+	proc_net_fops_create(&init_net, "ip_vs_conn_sync", 0,
+			     &ip_vs_conn_sync_fops);
 
 	/* calculate the random value for connection hash */
 	get_random_bytes(&ip_vs_conn_rnd, sizeof(ip_vs_conn_rnd));
@@ -1108,7 +1509,6 @@ int __init ip_vs_conn_init(void)
 	return 0;
 }
 
-
 void ip_vs_conn_cleanup(void)
 {
 	/* flush all the connection entries first */
diff -uprN linux-2.6.32-220.23.1.el6.x86_64.bak/net/netfilter/ipvs/ip_vs_core.c linux-2.6.32-220.23.1.el6.x86_64/net/netfilter/ipvs/ip_vs_core.c
--- linux-2.6.32-220.23.1.el6.x86_64.bak/net/netfilter/ipvs/ip_vs_core.c	2012-06-12 21:31:32.000000000 +0800
+++ linux-2.6.32-220.23.1.el6.x86_64/net/netfilter/ipvs/ip_vs_core.c	2013-03-18 16:45:37.000000000 +0800
@@ -22,6 +22,11 @@
  *	Paul `Rusty' Russell		properly handle non-linear skbs
  *	Harald Welte			don't use nfcache
  *
+ *	Wen Li       <steel.mental@gmail.com>
+ *	Jiaming Wu   <pukong.wjm@taobao.com>   support FULLNAT and SYNPROXY
+ *
+ *   Yu Bo        <yubo@xiaomi.com>
+ *
  */
 
 #define KMSG_COMPONENT "IPVS"
@@ -36,7 +41,7 @@
 #include <net/ip.h>
 #include <net/tcp.h>
 #include <net/udp.h>
-#include <net/icmp.h>                   /* for icmp_send */
+#include <net/icmp.h>		/* for icmp_send */
 #include <net/route.h>
 
 #include <linux/netfilter.h>
@@ -48,15 +53,14 @@
 #endif
 
 #include <net/ip_vs.h>
-
+#include <net/ip_vs_synproxy.h>
 
 EXPORT_SYMBOL(register_ip_vs_scheduler);
 EXPORT_SYMBOL(unregister_ip_vs_scheduler);
 EXPORT_SYMBOL(ip_vs_skb_replace);
 EXPORT_SYMBOL(ip_vs_proto_name);
 EXPORT_SYMBOL(ip_vs_conn_new);
-EXPORT_SYMBOL(ip_vs_conn_in_get);
-EXPORT_SYMBOL(ip_vs_conn_out_get);
+EXPORT_SYMBOL(ip_vs_conn_get);
 #ifdef CONFIG_IP_VS_PROTO_TCP
 EXPORT_SYMBOL(ip_vs_tcp_conn_listen);
 #endif
@@ -65,7 +69,6 @@ EXPORT_SYMBOL(ip_vs_conn_put);
 EXPORT_SYMBOL(ip_vs_get_debug_level);
 #endif
 
-
 /* ID used in ICMP lookups */
 #define icmp_id(icmph)          (((icmph)->un).echo.id)
 #define icmpv6_id(icmph)        (icmph->icmp6_dataun.u_echo.identifier)
@@ -99,8 +102,7 @@ void ip_vs_init_hash_table(struct list_h
 		INIT_LIST_HEAD(&table[rows]);
 }
 
-static inline void
-ip_vs_in_stats(struct ip_vs_conn *cp, struct sk_buff *skb)
+static inline void ip_vs_in_stats(struct ip_vs_conn *cp, struct sk_buff *skb)
 {
 	struct ip_vs_dest *dest = cp->dest;
 	if (dest && (dest->flags & IP_VS_DEST_F_AVAILABLE)) {
@@ -121,9 +123,7 @@ ip_vs_in_stats(struct ip_vs_conn *cp, st
 	}
 }
 
-
-static inline void
-ip_vs_out_stats(struct ip_vs_conn *cp, struct sk_buff *skb)
+static inline void ip_vs_out_stats(struct ip_vs_conn *cp, struct sk_buff *skb)
 {
 	struct ip_vs_dest *dest = cp->dest;
 	if (dest && (dest->flags & IP_VS_DEST_F_AVAILABLE)) {
@@ -144,7 +144,6 @@ ip_vs_out_stats(struct ip_vs_conn *cp, s
 	}
 }
 
-
 static inline void
 ip_vs_conn_stats(struct ip_vs_conn *cp, struct ip_vs_service *svc)
 {
@@ -161,11 +160,9 @@ ip_vs_conn_stats(struct ip_vs_conn *cp, 
 	spin_unlock(&ip_vs_stats.lock);
 }
 
-
 static inline int
 ip_vs_set_state(struct ip_vs_conn *cp, int direction,
-		const struct sk_buff *skb,
-		struct ip_vs_protocol *pp)
+		const struct sk_buff *skb, struct ip_vs_protocol *pp)
 {
 	if (unlikely(!pp->state_transition))
 		return 0;
@@ -177,9 +174,10 @@ ip_vs_onepacket_enabled(struct ip_vs_ser
 {
 	return (svc->flags & IP_VS_SVC_F_ONEPACKET
 		&& iph->protocol == IPPROTO_UDP)
-		? IP_VS_CONN_F_ONE_PACKET : 0;
+	    ? IP_VS_CONN_F_ONE_PACKET : 0;
 }
 
+
 /*
  *  IPVS persistent scheduling function
  *  It creates a connection entry according to its template if exists,
@@ -187,16 +185,16 @@ ip_vs_onepacket_enabled(struct ip_vs_ser
  *  Locking: we are svc user (svc->refcnt), so we hold all dests too
  *  Protocols supported: TCP, UDP
  */
-static struct ip_vs_conn *
-ip_vs_sched_persist(struct ip_vs_service *svc,
-		    const struct sk_buff *skb,
-		    __be16 ports[2])
+static struct ip_vs_conn *ip_vs_sched_persist(struct ip_vs_service *svc,
+					      struct sk_buff *skb,
+					      __be16 ports[2],
+					      int is_synproxy_on)
 {
 	struct ip_vs_conn *cp = NULL;
 	struct ip_vs_iphdr iph;
 	struct ip_vs_dest *dest;
 	struct ip_vs_conn *ct;
-	__be16  dport;			/* destination port to forward */
+	__be16 dport;		/* destination port to forward */
 	union nf_inet_addr snet;	/* source network of the client,
 					   after masking */
 
@@ -262,14 +260,14 @@ ip_vs_sched_persist(struct ip_vs_service
 						    ports[1],
 						    &dest->addr, dest->port,
 						    IP_VS_CONN_F_TEMPLATE,
-						    dest);
+						    dest, NULL, 0);
 			else
 				ct = ip_vs_conn_new(svc->af, iph.protocol,
 						    &snet, 0,
 						    &iph.daddr, 0,
 						    &dest->addr, 0,
 						    IP_VS_CONN_F_TEMPLATE,
-						    dest);
+						    dest, NULL, 0);
 			if (ct == NULL)
 				return NULL;
 
@@ -324,14 +322,14 @@ ip_vs_sched_persist(struct ip_vs_service
 						    &fwmark, 0,
 						    &dest->addr, 0,
 						    IP_VS_CONN_F_TEMPLATE,
-						    dest);
+						    dest, NULL, 0);
 			} else
 				ct = ip_vs_conn_new(svc->af, iph.protocol,
 						    &snet, 0,
 						    &iph.daddr, 0,
 						    &dest->addr, 0,
 						    IP_VS_CONN_F_TEMPLATE,
-						    dest);
+						    dest, NULL, 0);
 			if (ct == NULL)
 				return NULL;
 
@@ -351,7 +349,7 @@ ip_vs_sched_persist(struct ip_vs_service
 			    &iph.daddr, ports[1],
 			    &dest->addr, dport,
 			    ip_vs_onepacket_enabled(svc, &iph),
-			    dest);
+			    dest, skb, is_synproxy_on);
 	if (cp == NULL) {
 		ip_vs_conn_put(ct);
 		return NULL;
@@ -367,64 +365,108 @@ ip_vs_sched_persist(struct ip_vs_service
 	return cp;
 }
 
-
 /*
  *  IPVS main scheduling function
  *  It selects a server according to the virtual service, and
  *  creates a connection entry.
  *  Protocols supported: TCP, UDP
  */
-struct ip_vs_conn *
-ip_vs_schedule(struct ip_vs_service *svc, const struct sk_buff *skb)
+struct ip_vs_conn *ip_vs_schedule(struct ip_vs_service *svc,
+				  struct sk_buff *skb, int is_synproxy_on)
 {
 	struct ip_vs_conn *cp = NULL;
 	struct ip_vs_iphdr iph;
 	struct ip_vs_dest *dest;
 	__be16 _ports[2], *pptr;
 
+EnterFunction(11);
+
 	ip_vs_fill_iphdr(svc->af, skb_network_header(skb), &iph);
 	pptr = skb_header_pointer(skb, iph.len, sizeof(_ports), _ports);
 	if (pptr == NULL)
 		return NULL;
 
+
+	
 	/*
 	 *    Persistent service
 	 */
 	if (svc->flags & IP_VS_SVC_F_PERSISTENT)
-		return ip_vs_sched_persist(svc, skb, pptr);
+		return ip_vs_sched_persist(svc, skb, pptr, is_synproxy_on);
+
 
 	/*
-	 *    Non-persistent service
+	 *    dsnat yubo@xiaomi.com
 	 */
-	if (!svc->fwmark && pptr[1] != svc->port) {
-		if (!svc->port)
-			pr_err("Schedule: port zero only supported "
-			       "in persistent services, "
-			       "check your ipvs configuration\n");
-		return NULL;
-	}
+	if (svc->port == 0){	
+EnterFunction(11);
 
-	dest = svc->scheduler->schedule(svc, skb);
-	if (dest == NULL) {
-		IP_VS_DBG(1, "Schedule: no dest found.\n");
-		return NULL;
+		dest = (struct ip_vs_dest *)svc->destinations.next;
+		if (dest == NULL) {
+			IP_VS_DBG(1, "Schedule: no dest found.\n");
+			return NULL;
+		}
+
+		/*
+		 *    Create a connection entry.
+		 */
+		cp = ip_vs_conn_new(svc->af, iph.protocol,
+				    &iph.saddr, pptr[0],
+				    &iph.daddr, pptr[1],
+				    &iph.daddr, pptr[1],
+				    ip_vs_onepacket_enabled(svc, &iph),
+				    dest, skb, is_synproxy_on);
+		
+		IP_VS_DBG_BUF(6, "Schedule dsnat fwd:%c c:%s:%u v:%s:%u "
+	      "l:%s:%u d:%s:%u conn->flags:%X conn->refcnt:%d\n",
+	      ip_vs_fwd_tag(cp),
+	      IP_VS_DBG_ADDR(svc->af, &cp->caddr), ntohs(cp->cport),
+	      IP_VS_DBG_ADDR(svc->af, &cp->vaddr), ntohs(cp->vport),
+	      IP_VS_DBG_ADDR(svc->af, &cp->laddr), ntohs(cp->lport),
+	      IP_VS_DBG_ADDR(svc->af, &cp->daddr), ntohs(cp->dport),
+	      cp->flags, atomic_read(&cp->refcnt));
+
+			
+	}else{
+		/*
+		 *    Non-persistent service
+		 */
+		if (!svc->fwmark && pptr[1] != svc->port) {
+			if (!svc->port)
+				pr_err("Schedule: port zero only supported "
+				       "in persistent services, "
+				       "check your ipvs configuration\n");
+			return NULL;
+		}
+
+		dest = svc->scheduler->schedule(svc, skb);
+
+		if (dest == NULL) {
+			IP_VS_DBG(1, "Schedule: no dest found.\n");
+			return NULL;
+		}
+
+		/*
+		 *    Create a connection entry.
+		 */
+		cp = ip_vs_conn_new(svc->af, iph.protocol,
+				    &iph.saddr, pptr[0],
+				    &iph.daddr, pptr[1],
+				    &dest->addr, dest->port ? dest->port : pptr[1],
+				    ip_vs_onepacket_enabled(svc, &iph),
+				    dest, skb, is_synproxy_on);
+
+		
 	}
+	
 
-	/*
-	 *    Create a connection entry.
-	 */
-	cp = ip_vs_conn_new(svc->af, iph.protocol,
-			    &iph.saddr, pptr[0],
-			    &iph.daddr, pptr[1],
-			    &dest->addr, dest->port ? dest->port : pptr[1],
-			    ip_vs_onepacket_enabled(svc, &iph),
-			    dest);
 	if (cp == NULL)
 		return NULL;
 
-	IP_VS_DBG_BUF(6, "Schedule fwd:%c c:%s:%u v:%s:%u "
+	IP_VS_DBG_BUF(6, "Schedule fwd:%c vs:%s:%u c:%s:%u v:%s:%u "
 		      "d:%s:%u conn->flags:%X conn->refcnt:%d\n",
 		      ip_vs_fwd_tag(cp),
+		      IP_VS_DBG_ADDR(svc->af, &svc->addr), ntohs(svc->port),
 		      IP_VS_DBG_ADDR(svc->af, &cp->caddr), ntohs(cp->cport),
 		      IP_VS_DBG_ADDR(svc->af, &cp->vaddr), ntohs(cp->vport),
 		      IP_VS_DBG_ADDR(svc->af, &cp->daddr), ntohs(cp->dport),
@@ -434,7 +476,6 @@ ip_vs_schedule(struct ip_vs_service *svc
 	return cp;
 }
 
-
 /*
  *  Pass or drop the packet.
  *  Called by ip_vs_in, when the virtual service is available but
@@ -453,13 +494,13 @@ int ip_vs_leave(struct ip_vs_service *sv
 		ip_vs_service_put(svc);
 		return NF_DROP;
 	}
-
 #ifdef CONFIG_IP_VS_IPV6
 	if (svc->af == AF_INET6)
 		unicast = ipv6_addr_type(&iph.daddr.in6) & IPV6_ADDR_UNICAST;
 	else
 #endif
-		unicast = (inet_addr_type(&init_net, iph.daddr.ip) == RTN_UNICAST);
+		unicast =
+		    (inet_addr_type(&init_net, iph.daddr.ip) == RTN_UNICAST);
 
 	/* if it is fwmark-based service, the cache_bypass sysctl is up
 	   and the destination is a non-local unicast, then create
@@ -467,7 +508,7 @@ int ip_vs_leave(struct ip_vs_service *sv
 	if (sysctl_ip_vs_cache_bypass && svc->fwmark && unicast) {
 		int ret, cs;
 		struct ip_vs_conn *cp;
-		union nf_inet_addr daddr =  { .all = { 0, 0, 0, 0 } };
+		union nf_inet_addr daddr = {.all = {0, 0, 0, 0} };
 
 		ip_vs_service_put(svc);
 
@@ -479,7 +520,7 @@ int ip_vs_leave(struct ip_vs_service *sv
 				    &daddr, 0,
 				    IP_VS_CONN_F_BYPASS |
 				    ip_vs_onepacket_enabled(svc, &iph),
-				    NULL);
+				    NULL, NULL, 0);
 		if (cp == NULL)
 			return NF_DROP;
 
@@ -529,7 +570,6 @@ int ip_vs_leave(struct ip_vs_service *sv
 	return NF_DROP;
 }
 
-
 /*
  *      It is hooked before NF_IP_PRI_NAT_SRC at the NF_INET_POST_ROUTING
  *      chain, and is used for VS/NAT.
@@ -541,7 +581,7 @@ static unsigned int ip_vs_post_routing(u
 				       struct sk_buff *skb,
 				       const struct net_device *in,
 				       const struct net_device *out,
-				       int (*okfn)(struct sk_buff *))
+				       int (*okfn) (struct sk_buff *))
 {
 	if (!skb->ipvs_property)
 		return NF_ACCEPT;
@@ -549,7 +589,7 @@ static unsigned int ip_vs_post_routing(u
 	return NF_STOP;
 }
 
-__sum16 ip_vs_checksum_complete(struct sk_buff *skb, int offset)
+__sum16 ip_vs_checksum_complete(struct sk_buff * skb, int offset)
 {
 	return csum_fold(skb_checksum(skb, offset, skb->len - offset, 0));
 }
@@ -572,99 +612,8 @@ static inline int ip_vs_gather_frags_v6(
 }
 #endif
 
-/*
- * Packet has been made sufficiently writable in caller
- * - inout: 1=in->out, 0=out->in
- */
-void ip_vs_nat_icmp(struct sk_buff *skb, struct ip_vs_protocol *pp,
-		    struct ip_vs_conn *cp, int inout)
-{
-	struct iphdr *iph	 = ip_hdr(skb);
-	unsigned int icmp_offset = iph->ihl*4;
-	struct icmphdr *icmph	 = (struct icmphdr *)(skb_network_header(skb) +
-						      icmp_offset);
-	struct iphdr *ciph	 = (struct iphdr *)(icmph + 1);
-
-	if (inout) {
-		iph->saddr = cp->vaddr.ip;
-		ip_send_check(iph);
-		ciph->daddr = cp->vaddr.ip;
-		ip_send_check(ciph);
-	} else {
-		iph->daddr = cp->daddr.ip;
-		ip_send_check(iph);
-		ciph->saddr = cp->daddr.ip;
-		ip_send_check(ciph);
-	}
-
-	/* the TCP/UDP port */
-	if (IPPROTO_TCP == ciph->protocol || IPPROTO_UDP == ciph->protocol) {
-		__be16 *ports = (void *)ciph + ciph->ihl*4;
-
-		if (inout)
-			ports[1] = cp->vport;
-		else
-			ports[0] = cp->dport;
-	}
-
-	/* And finally the ICMP checksum */
-	icmph->checksum = 0;
-	icmph->checksum = ip_vs_checksum_complete(skb, icmp_offset);
-	skb->ip_summed = CHECKSUM_UNNECESSARY;
-
-	if (inout)
-		IP_VS_DBG_PKT(11, pp, skb, (void *)ciph - (void *)iph,
-			"Forwarding altered outgoing ICMP");
-	else
-		IP_VS_DBG_PKT(11, pp, skb, (void *)ciph - (void *)iph,
-			"Forwarding altered incoming ICMP");
-}
-
-#ifdef CONFIG_IP_VS_IPV6
-void ip_vs_nat_icmp_v6(struct sk_buff *skb, struct ip_vs_protocol *pp,
-		    struct ip_vs_conn *cp, int inout)
-{
-	struct ipv6hdr *iph	 = ipv6_hdr(skb);
-	unsigned int icmp_offset = sizeof(struct ipv6hdr);
-	struct icmp6hdr *icmph	 = (struct icmp6hdr *)(skb_network_header(skb) +
-						      icmp_offset);
-	struct ipv6hdr *ciph	 = (struct ipv6hdr *)(icmph + 1);
-
-	if (inout) {
-		iph->saddr = cp->vaddr.in6;
-		ciph->daddr = cp->vaddr.in6;
-	} else {
-		iph->daddr = cp->daddr.in6;
-		ciph->saddr = cp->daddr.in6;
-	}
-
-	/* the TCP/UDP port */
-	if (IPPROTO_TCP == ciph->nexthdr || IPPROTO_UDP == ciph->nexthdr) {
-		__be16 *ports = (void *)ciph + sizeof(struct ipv6hdr);
-
-		if (inout)
-			ports[1] = cp->vport;
-		else
-			ports[0] = cp->dport;
-	}
-
-	/* And finally the ICMP checksum */
-	icmph->icmp6_cksum = 0;
-	/* TODO IPv6: is this correct for ICMPv6? */
-	ip_vs_checksum_complete(skb, icmp_offset);
-	skb->ip_summed = CHECKSUM_UNNECESSARY;
-
-	if (inout)
-		IP_VS_DBG_PKT(11, pp, skb, (void *)ciph - (void *)iph,
-			"Forwarding altered outgoing ICMPv6");
-	else
-		IP_VS_DBG_PKT(11, pp, skb, (void *)ciph - (void *)iph,
-			"Forwarding altered incoming ICMPv6");
-}
-#endif
-
 /* Handle relevant response ICMP messages - forward to the right
- * destination host. Used for NAT and local client.
+ * destination host. Used for NAT / local client / FULLNAT.
  */
 static int handle_response_icmp(int af, struct sk_buff *skb,
 				union nf_inet_addr *snet,
@@ -674,7 +623,8 @@ static int handle_response_icmp(int af, 
 {
 	unsigned int verdict = NF_DROP;
 
-	if (IP_VS_FWD_METHOD(cp) != 0) {
+	if ((IP_VS_FWD_METHOD(cp) != IP_VS_CONN_F_MASQ)
+	    && (IP_VS_FWD_METHOD(cp) != IP_VS_CONN_F_FULLNAT)) {
 		pr_err("shouldn't reach here, because the box is on the "
 		       "half connection in the tun/dr module.\n");
 	}
@@ -689,23 +639,35 @@ static int handle_response_icmp(int af, 
 
 	if (IPPROTO_TCP == protocol || IPPROTO_UDP == protocol)
 		offset += 2 * sizeof(__u16);
-	if (!skb_make_writable(skb, offset))
-		goto out;
-
-#ifdef CONFIG_IP_VS_IPV6
-	if (af == AF_INET6)
-		ip_vs_nat_icmp_v6(skb, pp, cp, 1);
-	else
-#endif
-		ip_vs_nat_icmp(skb, pp, cp, 1);
 
 	/* do the statistics and put it back */
 	ip_vs_out_stats(cp, skb);
 
-	skb->ipvs_property = 1;
-	verdict = NF_ACCEPT;
+	if (IP_VS_FWD_METHOD(cp) == IP_VS_CONN_F_FULLNAT) {
+#ifdef CONFIG_IP_VS_IPV6
+		if (af == AF_INET6)
+			verdict =
+			    ip_vs_fnat_response_icmp_xmit_v6(skb, pp, cp,
+							     offset);
+		else
+#endif
+			verdict =
+			    ip_vs_fnat_response_icmp_xmit(skb, pp, cp, offset);
 
-out:
+	} else {
+#ifdef CONFIG_IP_VS_IPV6
+		if (af == AF_INET6)
+			verdict =
+			    ip_vs_normal_response_icmp_xmit_v6(skb, pp, cp,
+							       offset);
+		else
+#endif
+			verdict =
+			    ip_vs_normal_response_icmp_xmit(skb, pp, cp,
+							    offset);
+	}
+
+      out:
 	__ip_vs_conn_put(cp);
 
 	return verdict;
@@ -719,13 +681,14 @@ out:
 static int ip_vs_out_icmp(struct sk_buff *skb, int *related)
 {
 	struct iphdr *iph;
-	struct icmphdr	_icmph, *ic;
-	struct iphdr	_ciph, *cih;	/* The ip header contained within the ICMP */
+	struct icmphdr _icmph, *ic;
+	struct iphdr _ciph, *cih;	/* The ip header contained within the ICMP */
 	struct ip_vs_iphdr ciph;
 	struct ip_vs_conn *cp;
 	struct ip_vs_protocol *pp;
 	unsigned int offset, ihl;
 	union nf_inet_addr snet;
+	int res_dir;
 
 	*related = 1;
 
@@ -742,8 +705,7 @@ static int ip_vs_out_icmp(struct sk_buff
 		return NF_DROP;
 
 	IP_VS_DBG(12, "Outgoing ICMP (%d,%d) %pI4->%pI4\n",
-		  ic->type, ntohs(icmp_id(ic)),
-		  &iph->saddr, &iph->daddr);
+		  ic->type, ntohs(icmp_id(ic)), &iph->saddr, &iph->daddr);
 
 	/*
 	 * Work through seeing if this is for us.
@@ -763,15 +725,14 @@ static int ip_vs_out_icmp(struct sk_buff
 	offset += sizeof(_icmph);
 	cih = skb_header_pointer(skb, offset, sizeof(_ciph), &_ciph);
 	if (cih == NULL)
-		return NF_ACCEPT; /* The packet looks wrong, ignore */
+		return NF_ACCEPT;	/* The packet looks wrong, ignore */
 
 	pp = ip_vs_proto_get(cih->protocol);
 	if (!pp)
 		return NF_ACCEPT;
 
 	/* Is the embedded protocol header present? */
-	if (unlikely(cih->frag_off & htons(IP_OFFSET) &&
-		     pp->dont_defrag))
+	if (unlikely(cih->frag_off & htons(IP_OFFSET) && pp->dont_defrag))
 		return NF_ACCEPT;
 
 	IP_VS_DBG_PKT(11, pp, skb, offset, "Checking outgoing ICMP for");
@@ -780,7 +741,7 @@ static int ip_vs_out_icmp(struct sk_buff
 
 	ip_vs_fill_iphdr(AF_INET, cih, &ciph);
 	/* The embedded headers contain source and dest in reverse order */
-	cp = pp->conn_out_get(AF_INET, skb, pp, &ciph, offset, 1);
+	cp = pp->conn_out_get(AF_INET, skb, pp, &ciph, offset, 1, &res_dir);
 	if (!cp)
 		return NF_ACCEPT;
 
@@ -793,14 +754,15 @@ static int ip_vs_out_icmp(struct sk_buff
 static int ip_vs_out_icmp_v6(struct sk_buff *skb, int *related)
 {
 	struct ipv6hdr *iph;
-	struct icmp6hdr	_icmph, *ic;
-	struct ipv6hdr	_ciph, *cih;	/* The ip header contained
+	struct icmp6hdr _icmph, *ic;
+	struct ipv6hdr _ciph, *cih;	/* The ip header contained
 					   within the ICMP */
 	struct ip_vs_iphdr ciph;
 	struct ip_vs_conn *cp;
 	struct ip_vs_protocol *pp;
 	unsigned int offset;
 	union nf_inet_addr snet;
+	int res_dir;
 
 	*related = 1;
 
@@ -838,7 +800,7 @@ static int ip_vs_out_icmp_v6(struct sk_b
 	offset += sizeof(_icmph);
 	cih = skb_header_pointer(skb, offset, sizeof(_ciph), &_ciph);
 	if (cih == NULL)
-		return NF_ACCEPT; /* The packet looks wrong, ignore */
+		return NF_ACCEPT;	/* The packet looks wrong, ignore */
 
 	pp = ip_vs_proto_get(cih->nexthdr);
 	if (!pp)
@@ -855,7 +817,7 @@ static int ip_vs_out_icmp_v6(struct sk_b
 
 	ip_vs_fill_iphdr(AF_INET6, cih, &ciph);
 	/* The embedded headers contain source and dest in reverse order */
-	cp = pp->conn_out_get(AF_INET6, skb, pp, &ciph, offset, 1);
+	cp = pp->conn_out_get(AF_INET6, skb, pp, &ciph, offset, 1, &res_dir);
 	if (!cp)
 		return NF_ACCEPT;
 
@@ -876,61 +838,52 @@ static inline int is_tcp_reset(const str
 }
 
 /* Handle response packets: rewrite addresses and send away...
- * Used for NAT and local client.
+ * Used for NAT / local client / FULLNAT.
  */
 static unsigned int
 handle_response(int af, struct sk_buff *skb, struct ip_vs_protocol *pp,
 		struct ip_vs_conn *cp, int ihl)
 {
-	IP_VS_DBG_PKT(11, pp, skb, 0, "Outgoing packet");
+	unsigned int ret = NF_DROP;
+
+	/* statistics */
+	ip_vs_out_stats(cp, skb);
 
-	if (!skb_make_writable(skb, ihl))
-		goto drop;
+	/*
+	 * Syn-proxy step 3 logic: receive syn-ack from rs.
+	 */
+	if (ip_vs_synproxy_synack_rcv(skb, cp, pp, ihl, &ret) == 0) {
+		goto out;
+	}
 
-	/* mangle the packet */
-	if (pp->snat_handler && !pp->snat_handler(skb, pp, cp))
-		goto drop;
+	/* state transition */
+	ip_vs_set_state(cp, IP_VS_DIR_OUTPUT, skb, pp);
+	/* transmit */
 
+	if (cp->flags & IP_VS_CONN_F_FULLNAT) {
 #ifdef CONFIG_IP_VS_IPV6
-	if (af == AF_INET6)
-		ipv6_hdr(skb)->saddr = cp->vaddr.in6;
-	else
+		if (af == AF_INET6) {
+			ret = ip_vs_fnat_response_xmit_v6(skb, pp, cp, ihl);
+		} else
 #endif
-	{
-		ip_hdr(skb)->saddr = cp->vaddr.ip;
-		ip_send_check(ip_hdr(skb));
-	}
-
-	/* For policy routing, packets originating from this
-	 * machine itself may be routed differently to packets
-	 * passing through.  We want this packet to be routed as
-	 * if it came from this machine itself.  So re-compute
-	 * the routing information.
-	 */
+		{
+			ret = ip_vs_fnat_response_xmit(skb, pp, cp, ihl);
+		}
+	} else {
 #ifdef CONFIG_IP_VS_IPV6
-	if (af == AF_INET6) {
-		if (ip6_route_me_harder(skb) != 0)
-			goto drop;
-	} else
+		if (af == AF_INET6) {
+			ret = ip_vs_normal_response_xmit_v6(skb, pp, cp, ihl);
+		} else
 #endif
-		if (ip_route_me_harder(skb, RTN_LOCAL) != 0)
-			goto drop;
-
-	IP_VS_DBG_PKT(10, pp, skb, 0, "After SNAT");
-
-	ip_vs_out_stats(cp, skb);
-	ip_vs_set_state(cp, IP_VS_DIR_OUTPUT, skb, pp);
-	ip_vs_conn_put(cp);
-
-	skb->ipvs_property = 1;
+		{
+			ret = ip_vs_normal_response_xmit(skb, pp, cp, ihl);
+		}
 
-	LeaveFunction(11);
-	return NF_ACCEPT;
+	}
 
-drop:
+      out:
 	ip_vs_conn_put(cp);
-	kfree_skb(skb);
-	return NF_STOLEN;
+	return ret;
 }
 
 /*
@@ -940,15 +893,20 @@ drop:
 static unsigned int
 ip_vs_out(unsigned int hooknum, struct sk_buff *skb,
 	  const struct net_device *in, const struct net_device *out,
-	  int (*okfn)(struct sk_buff *))
+	  int (*okfn) (struct sk_buff *))
 {
 	struct ip_vs_iphdr iph;
 	struct ip_vs_protocol *pp;
 	struct ip_vs_conn *cp;
 	int af;
-
+	int res_dir;
+	int ret;
+	int v;
+	int pkts;
+	int forward = 0;
+	
 	EnterFunction(11);
-
+			
 	af = (skb->protocol == htons(ETH_P_IP)) ? AF_INET : AF_INET6;
 
 	if (skb->ipvs_property)
@@ -966,88 +924,165 @@ ip_vs_out(unsigned int hooknum, struct s
 		}
 	} else
 #endif
-		if (unlikely(iph.protocol == IPPROTO_ICMP)) {
-			int related, verdict = ip_vs_out_icmp(skb, &related);
-
-			if (related)
-				return verdict;
-			ip_vs_fill_iphdr(af, skb_network_header(skb), &iph);
-		}
+	if (unlikely(iph.protocol == IPPROTO_ICMP)) {
+		int related, verdict = ip_vs_out_icmp(skb, &related);
 
+		if (related)
+			return verdict;
+		ip_vs_fill_iphdr(af, skb_network_header(skb), &iph);
+	} else {
+		forward = 1;
+EnterFunction(11);		
+	}
+	
+	/* Protocol supported? */
 	pp = ip_vs_proto_get(iph.protocol);
 	if (unlikely(!pp))
 		return NF_ACCEPT;
 
-	/* reassemble IP fragments */
-#ifdef CONFIG_IP_VS_IPV6
-	if (af == AF_INET6) {
-		if (unlikely(iph.protocol == IPPROTO_ICMPV6)) {
-			int related, verdict = ip_vs_out_icmp_v6(skb, &related);
-
-			if (related)
-				return verdict;
 
-			ip_vs_fill_iphdr(af, skb_network_header(skb), &iph);
-		}
-	} else
-#endif
-		if (unlikely(ip_hdr(skb)->frag_off & htons(IP_MF|IP_OFFSET) &&
+	if (unlikely(ip_hdr(skb)->frag_off & htons(IP_MF | IP_OFFSET) &&
 			     !pp->dont_defrag)) {
-			if (ip_vs_gather_frags(skb, IP_DEFRAG_VS_OUT))
-				return NF_STOLEN;
+		if (ip_vs_gather_frags(skb, IP_DEFRAG_VS_OUT))
+			return NF_STOLEN;
 
-			ip_vs_fill_iphdr(af, skb_network_header(skb), &iph);
-		}
+		ip_vs_fill_iphdr(af, skb_network_header(skb), &iph);
+	}
 
 	/*
 	 * Check if the packet belongs to an existing entry
 	 */
-	cp = pp->conn_out_get(af, skb, pp, &iph, iph.len, 0);
+	cp = pp->conn_out_get(af, skb, pp, &iph, iph.len, 0, &res_dir);
 
-	if (unlikely(!cp)) {
-		if (sysctl_ip_vs_nat_icmp_send &&
-		    (pp->protocol == IPPROTO_TCP ||
-		     pp->protocol == IPPROTO_UDP)) {
-			__be16 _ports[2], *pptr;
-
-			pptr = skb_header_pointer(skb, iph.len,
-						  sizeof(_ports), _ports);
-			if (pptr == NULL)
-				return NF_ACCEPT;	/* Not for me */
-			if (ip_vs_lookup_real_service(af, iph.protocol,
-						      &iph.saddr,
-						      pptr[0])) {
-				/*
-				 * Notify the real server: there is no
-				 * existing entry if it is not RST
-				 * packet or not TCP packet.
-				 */
-				if (iph.protocol != IPPROTO_TCP
-				    || !is_tcp_reset(skb, iph.len)) {
+	//dsnat
+	if(forward){
+EnterFunction(11);		
+		if (!cp) {
+			/* create a new connection */
+			int v;
+			IP_VS_DBG(11, "forward out 2 in  conn not found! -> tcp_conn_schedule\n");
+			if (!pp->conn_schedule(af | IP_VS_CONN_F_DSNAT, skb, pp, &v, &cp))
+				return v;
+EnterFunction(11);	
+
+			if (unlikely(!cp)) {
+				/* sorry, all this trouble for a no-hit :) */
+				IP_VS_DBG_PKT(12, pp, skb, 0,
+					      "packet continues traversal as normal");
+				return NF_ACCEPT;
+			}		
+		}
+
+		IP_VS_DBG_PKT(11, pp, skb, 0, "Incoming packet");
+		
+		ip_vs_in_stats(cp, skb);
+
+		/*
+		 * Filter out-in ack packet when cp is at SYN_SENT state.
+		 * DROP it if not a valid packet, STORE it if we have 
+		 * space left. 
+		 */
+		if ((cp->flags & IP_VS_CONN_F_SYNPROXY) &&
+		    (0 == ip_vs_synproxy_filter_ack(skb, cp, pp, &iph, &v))) {
+			ip_vs_conn_put(cp);
+			return v;
+		}
+
+
+		/*
+		 * "Reuse" syn-proxy sessions.
+		 * "Reuse" means update syn_proxy_seq struct and clean ack_skb etc.
+		 */
+		if ((cp->flags & IP_VS_CONN_F_SYNPROXY) &&
+		    (0 != sysctl_ip_vs_synproxy_conn_reuse)) {
+			int v = NF_DROP;
+
+			if (0 == ip_vs_synproxy_reuse_conn(af, skb, cp, pp, &iph, &v)) {
+				ip_vs_conn_put(cp);
+				return v;
+			}
+		}
+
+		ip_vs_set_state(cp, IP_VS_DIR_OUTPUT, skb, pp);
+		
+		if (cp->packet_xmit){
+			EnterFunction(11);		
+			ret = cp->packet_xmit(skb, cp, pp);
+		/* do not touch skb anymore */
+		} else {
+			IP_VS_DBG_RL("warning: packet_xmit is null");
+			ret = NF_ACCEPT;
+		}
+
+		pkts = atomic_add_return(1, &cp->in_pkts);
+		
+		if (af == AF_INET &&
+		    (ip_vs_sync_state & IP_VS_STATE_MASTER) &&
+		    (((cp->protocol != IPPROTO_TCP ||
+		       cp->state == IP_VS_TCP_S_ESTABLISHED) &&
+		      (pkts % sysctl_ip_vs_sync_threshold[1]
+		       == sysctl_ip_vs_sync_threshold[0])) ||
+		     ((cp->protocol == IPPROTO_TCP) && (cp->old_state != cp->state) &&
+		      ((cp->state == IP_VS_TCP_S_FIN_WAIT) ||
+		       (cp->state == IP_VS_TCP_S_CLOSE_WAIT) ||
+		       (cp->state == IP_VS_TCP_S_TIME_WAIT))))){
+			ip_vs_sync_conn(cp);
+
+		}
+		cp->old_state = cp->state;
+
+		ip_vs_conn_put(cp);
+		skb->ipvs_property = 1;
+		
+		return ret;
+	}else {
+EnterFunction(11);
+
+		if (unlikely(!cp)) {
+			if (sysctl_ip_vs_nat_icmp_send &&
+			    (pp->protocol == IPPROTO_TCP ||
+			     pp->protocol == IPPROTO_UDP)) {
+				__be16 _ports[2], *pptr;
+
+				pptr = skb_header_pointer(skb, iph.len,
+							  sizeof(_ports), _ports);
+				if (pptr == NULL)
+					return NF_ACCEPT;	/* Not for me */
+				if (ip_vs_lookup_real_service(af, iph.protocol,
+							      &iph.saddr, pptr[0])) {
+					/*
+					 * Notify the real server: there is no
+					 * existing entry if it is not RST
+					 * packet or not TCP packet.
+					 */
+					if (iph.protocol != IPPROTO_TCP
+					    || !is_tcp_reset(skb, iph.len)) {
 #ifdef CONFIG_IP_VS_IPV6
-					if (af == AF_INET6)
-						icmpv6_send(skb,
-							    ICMPV6_DEST_UNREACH,
-							    ICMPV6_PORT_UNREACH,
-							    0, skb->dev);
-					else
+						if (af == AF_INET6)
+							icmpv6_send(skb,
+								    ICMPV6_DEST_UNREACH,
+								    ICMPV6_PORT_UNREACH,
+								    0, skb->dev);
+						else
 #endif
-						icmp_send(skb,
-							  ICMP_DEST_UNREACH,
-							  ICMP_PORT_UNREACH, 0);
-					return NF_DROP;
+							icmp_send(skb,
+								  ICMP_DEST_UNREACH,
+								  ICMP_PORT_UNREACH, 0);
+						return NF_DROP;
+					}
 				}
 			}
+			IP_VS_DBG_PKT(12, pp, skb, 0,
+				      "packet continues traversal as normal");
+
+			
+			return NF_ACCEPT;
 		}
-		IP_VS_DBG_PKT(12, pp, skb, 0,
-			      "packet continues traversal as normal");
-		return NF_ACCEPT;
-	}
 
-	return handle_response(af, skb, pp, cp, iph.len);
+		return handle_response(af, skb, pp, cp, iph.len);
+	}
 }
 
-
 /*
  *	Handle ICMP messages in the outside-to-inside direction (incoming).
  *	Find any that might be relevant, check against existing connections,
@@ -1058,20 +1093,21 @@ static int
 ip_vs_in_icmp(struct sk_buff *skb, int *related, unsigned int hooknum)
 {
 	struct iphdr *iph;
-	struct icmphdr	_icmph, *ic;
-	struct iphdr	_ciph, *cih;	/* The ip header contained within the ICMP */
+	struct icmphdr _icmph, *ic;
+	struct iphdr _ciph, *cih;	/* The ip header contained within the ICMP */
 	struct ip_vs_iphdr ciph;
 	struct ip_vs_conn *cp;
 	struct ip_vs_protocol *pp;
 	unsigned int offset, ihl, verdict;
 	union nf_inet_addr snet;
+	int res_dir;
 
 	*related = 1;
 
 	/* reassemble IP fragments */
 	if (ip_hdr(skb)->frag_off & htons(IP_MF | IP_OFFSET)) {
 		if (ip_vs_gather_frags(skb, hooknum == NF_INET_LOCAL_IN ?
-					    IP_DEFRAG_VS_IN : IP_DEFRAG_VS_FWD))
+				       IP_DEFRAG_VS_IN : IP_DEFRAG_VS_FWD))
 			return NF_STOLEN;
 	}
 
@@ -1082,8 +1118,7 @@ ip_vs_in_icmp(struct sk_buff *skb, int *
 		return NF_DROP;
 
 	IP_VS_DBG(12, "Incoming ICMP (%d,%d) %pI4->%pI4\n",
-		  ic->type, ntohs(icmp_id(ic)),
-		  &iph->saddr, &iph->daddr);
+		  ic->type, ntohs(icmp_id(ic)), &iph->saddr, &iph->daddr);
 
 	/*
 	 * Work through seeing if this is for us.
@@ -1103,15 +1138,14 @@ ip_vs_in_icmp(struct sk_buff *skb, int *
 	offset += sizeof(_icmph);
 	cih = skb_header_pointer(skb, offset, sizeof(_ciph), &_ciph);
 	if (cih == NULL)
-		return NF_ACCEPT; /* The packet looks wrong, ignore */
+		return NF_ACCEPT;	/* The packet looks wrong, ignore */
 
 	pp = ip_vs_proto_get(cih->protocol);
 	if (!pp)
 		return NF_ACCEPT;
 
 	/* Is the embedded protocol header present? */
-	if (unlikely(cih->frag_off & htons(IP_OFFSET) &&
-		     pp->dont_defrag))
+	if (unlikely(cih->frag_off & htons(IP_OFFSET) && pp->dont_defrag))
 		return NF_ACCEPT;
 
 	IP_VS_DBG_PKT(11, pp, skb, offset, "Checking incoming ICMP for");
@@ -1120,19 +1154,18 @@ ip_vs_in_icmp(struct sk_buff *skb, int *
 
 	ip_vs_fill_iphdr(AF_INET, cih, &ciph);
 	/* The embedded headers contain source and dest in reverse order */
-	cp = pp->conn_in_get(AF_INET, skb, pp, &ciph, offset, 1);
+	cp = pp->conn_in_get(AF_INET, skb, pp, &ciph, offset, 1, &res_dir);
 	if (!cp) {
-		/* The packet could also belong to a local client */
-		cp = pp->conn_out_get(AF_INET, skb, pp, &ciph, offset, 1);
-		if (cp) {
-			snet.ip = iph->saddr;
-			return handle_response_icmp(AF_INET, skb, &snet,
-						    cih->protocol, cp, pp,
-						    offset, ihl);
-		}
 		return NF_ACCEPT;
 	}
 
+	if (res_dir == IP_VS_CIDX_F_IN2OUT) {
+		/* The packet belong to a local client / fullnat */
+		snet.ip = iph->saddr;
+		return handle_response_icmp(AF_INET, skb, &snet,
+					    cih->protocol, cp, pp, offset, ihl);
+	}
+
 	verdict = NF_DROP;
 
 	/* Ensure the checksum is correct */
@@ -1150,7 +1183,7 @@ ip_vs_in_icmp(struct sk_buff *skb, int *
 	verdict = ip_vs_icmp_xmit(skb, cp, pp, offset);
 	/* do not touch skb anymore */
 
-  out:
+      out:
 	__ip_vs_conn_put(cp);
 
 	return verdict;
@@ -1161,22 +1194,22 @@ static int
 ip_vs_in_icmp_v6(struct sk_buff *skb, int *related, unsigned int hooknum)
 {
 	struct ipv6hdr *iph;
-	struct icmp6hdr	_icmph, *ic;
-	struct ipv6hdr	_ciph, *cih;	/* The ip header contained
+	struct icmp6hdr _icmph, *ic;
+	struct ipv6hdr _ciph, *cih;	/* The ip header contained
 					   within the ICMP */
 	struct ip_vs_iphdr ciph;
 	struct ip_vs_conn *cp;
 	struct ip_vs_protocol *pp;
 	unsigned int offset, verdict;
 	union nf_inet_addr snet;
+	int res_dir;
 
 	*related = 1;
 
 	/* reassemble IP fragments */
 	if (ipv6_hdr(skb)->nexthdr == IPPROTO_FRAGMENT) {
 		if (ip_vs_gather_frags_v6(skb, hooknum == NF_INET_LOCAL_IN ?
-					       IP_DEFRAG_VS_IN :
-					       IP_DEFRAG_VS_FWD))
+					  IP_DEFRAG_VS_IN : IP_DEFRAG_VS_FWD))
 			return NF_STOLEN;
 	}
 
@@ -1208,7 +1241,7 @@ ip_vs_in_icmp_v6(struct sk_buff *skb, in
 	offset += sizeof(_icmph);
 	cih = skb_header_pointer(skb, offset, sizeof(_ciph), &_ciph);
 	if (cih == NULL)
-		return NF_ACCEPT; /* The packet looks wrong, ignore */
+		return NF_ACCEPT;	/* The packet looks wrong, ignore */
 
 	pp = ip_vs_proto_get(cih->nexthdr);
 	if (!pp)
@@ -1225,20 +1258,19 @@ ip_vs_in_icmp_v6(struct sk_buff *skb, in
 
 	ip_vs_fill_iphdr(AF_INET6, cih, &ciph);
 	/* The embedded headers contain source and dest in reverse order */
-	cp = pp->conn_in_get(AF_INET6, skb, pp, &ciph, offset, 1);
+	cp = pp->conn_in_get(AF_INET6, skb, pp, &ciph, offset, 1, &res_dir);
 	if (!cp) {
-		/* The packet could also belong to a local client */
-		cp = pp->conn_out_get(AF_INET6, skb, pp, &ciph, offset, 1);
-		if (cp) {
-			ipv6_addr_copy(&snet.in6, &iph->saddr);
-			return handle_response_icmp(AF_INET6, skb, &snet,
-						    cih->nexthdr,
-						    cp, pp, offset,
-						    sizeof(struct ipv6hdr));
-		}
 		return NF_ACCEPT;
 	}
 
+	if (res_dir == IP_VS_CIDX_F_IN2OUT) {
+		ipv6_addr_copy(&snet.in6, &iph->saddr);
+		return handle_response_icmp(AF_INET6, skb, &snet,
+					    cih->nexthdr,
+					    cp, pp, offset,
+					    sizeof(struct ipv6hdr));
+	}
+
 	verdict = NF_DROP;
 
 	/* do the statistics and put it back */
@@ -1254,7 +1286,6 @@ ip_vs_in_icmp_v6(struct sk_buff *skb, in
 }
 #endif
 
-
 /*
  *	Check if it's for virtual services, look it up,
  *	and send it on its way...
@@ -1262,33 +1293,36 @@ ip_vs_in_icmp_v6(struct sk_buff *skb, in
 static unsigned int
 ip_vs_in(unsigned int hooknum, struct sk_buff *skb,
 	 const struct net_device *in, const struct net_device *out,
-	 int (*okfn)(struct sk_buff *))
+	 int (*okfn) (struct sk_buff *))
 {
 	struct ip_vs_iphdr iph;
 	struct ip_vs_protocol *pp;
 	struct ip_vs_conn *cp;
 	int ret, restart, af, pkts;
+	int v = NF_DROP;
+	int res_dir;
+
+	//EnterFunction(11);
 
 	af = (skb->protocol == htons(ETH_P_IP)) ? AF_INET : AF_INET6;
 
 	ip_vs_fill_iphdr(af, skb_network_header(skb), &iph);
 
 	/*
-	 *	Big tappo: only PACKET_HOST, including loopback for local client
-	 *	Don't handle local packets on IPv6 for now
+	 *      Big tappo: only PACKET_HOST, including loopback for local client
+	 *      Don't handle local packets on IPv6 for now
 	 */
 	if (unlikely(skb->pkt_type != PACKET_HOST)) {
 		IP_VS_DBG_BUF(12, "packet type=%d proto=%d daddr=%s ignored\n",
 			      skb->pkt_type,
-			      iph.protocol,
-			      IP_VS_DBG_ADDR(af, &iph.daddr));
+			      iph.protocol, IP_VS_DBG_ADDR(af, &iph.daddr));
 		return NF_ACCEPT;
 	}
-
 #ifdef CONFIG_IP_VS_IPV6
 	if (af == AF_INET6) {
 		if (unlikely(iph.protocol == IPPROTO_ICMPV6)) {
-			int related, verdict = ip_vs_in_icmp_v6(skb, &related, hooknum);
+			int related, verdict =
+			    ip_vs_in_icmp_v6(skb, &related, hooknum);
 
 			if (related)
 				return verdict;
@@ -1296,13 +1330,13 @@ ip_vs_in(unsigned int hooknum, struct sk
 		}
 	} else
 #endif
-		if (unlikely(iph.protocol == IPPROTO_ICMP)) {
-			int related, verdict = ip_vs_in_icmp(skb, &related, hooknum);
+	if (unlikely(iph.protocol == IPPROTO_ICMP)) {
+		int related, verdict = ip_vs_in_icmp(skb, &related, hooknum);
 
-			if (related)
-				return verdict;
-			ip_vs_fill_iphdr(af, skb_network_header(skb), &iph);
-		}
+		if (related)
+			return verdict;
+		ip_vs_fill_iphdr(af, skb_network_header(skb), &iph);
+	}
 
 	/* Protocol supported? */
 	pp = ip_vs_proto_get(iph.protocol);
@@ -1312,15 +1346,16 @@ ip_vs_in(unsigned int hooknum, struct sk
 	/*
 	 * Check if the packet belongs to an existing connection entry
 	 */
-	cp = pp->conn_in_get(af, skb, pp, &iph, iph.len, 0);
-
-	if (unlikely(!cp)) {
-		int v;
+	cp = pp->conn_in_get(af, skb, pp, &iph, iph.len, 0, &res_dir);
 
-		/* For local client packets, it could be a response */
-		cp = pp->conn_out_get(af, skb, pp, &iph, iph.len, 0);
-		if (cp)
+	if (likely(cp)) {
+		/* For full-nat/local-client packets, it could be a response */
+		if (res_dir == IP_VS_CIDX_F_IN2OUT) {
 			return handle_response(af, skb, pp, cp, iph.len);
+		}
+	} else {
+		/* create a new connection */
+		int v;
 
 		if (!pp->conn_schedule(af, skb, pp, &v, &cp))
 			return v;
@@ -1350,10 +1385,41 @@ ip_vs_in(unsigned int hooknum, struct sk
 	}
 
 	ip_vs_in_stats(cp, skb);
+
+	/*
+	 * Filter out-in ack packet when cp is at SYN_SENT state.
+	 * DROP it if not a valid packet, STORE it if we have 
+	 * space left. 
+	 */
+	if ((cp->flags & IP_VS_CONN_F_SYNPROXY) &&
+	    (0 == ip_vs_synproxy_filter_ack(skb, cp, pp, &iph, &v))) {
+		ip_vs_conn_put(cp);
+		return v;
+	}
+
+	/*
+	 * "Reuse" syn-proxy sessions.
+	 * "Reuse" means update syn_proxy_seq struct and clean ack_skb etc.
+	 */
+	if ((cp->flags & IP_VS_CONN_F_SYNPROXY) &&
+	    (0 != sysctl_ip_vs_synproxy_conn_reuse)) {
+		int v = NF_DROP;
+
+		if (0 == ip_vs_synproxy_reuse_conn(af, skb, cp, pp, &iph, &v)) {
+			ip_vs_conn_put(cp);
+			return v;
+		}
+	}
+
+	
+
 	restart = ip_vs_set_state(cp, IP_VS_DIR_INPUT, skb, pp);
+
+
+	
 	if (cp->packet_xmit)
 		ret = cp->packet_xmit(skb, cp, pp);
-		/* do not touch skb anymore */
+	/* do not touch skb anymore */
 	else {
 		IP_VS_DBG_RL("warning: packet_xmit is null");
 		ret = NF_ACCEPT;
@@ -1383,7 +1449,6 @@ ip_vs_in(unsigned int hooknum, struct sk
 	return ret;
 }
 
-
 /*
  *	It is hooked at the NF_INET_FORWARD chain, in order to catch ICMP
  *      related packets destined for 0.0.0.0/0.
@@ -1396,7 +1461,7 @@ ip_vs_in(unsigned int hooknum, struct sk
 static unsigned int
 ip_vs_forward_icmp(unsigned int hooknum, struct sk_buff *skb,
 		   const struct net_device *in, const struct net_device *out,
-		   int (*okfn)(struct sk_buff *))
+		   int (*okfn) (struct sk_buff *))
 {
 	int r;
 
@@ -1410,7 +1475,7 @@ ip_vs_forward_icmp(unsigned int hooknum,
 static unsigned int
 ip_vs_forward_icmp_v6(unsigned int hooknum, struct sk_buff *skb,
 		      const struct net_device *in, const struct net_device *out,
-		      int (*okfn)(struct sk_buff *))
+		      int (*okfn) (struct sk_buff *))
 {
 	int r;
 
@@ -1421,83 +1486,142 @@ ip_vs_forward_icmp_v6(unsigned int hookn
 }
 #endif
 
+#define IPPROTO_OSPF 89
+static unsigned int
+ip_vs_pre_routing(unsigned int hooknum, struct sk_buff *skb,
+		  const struct net_device *in, const struct net_device *out,
+		  int (*okfn) (struct sk_buff *))
+{
+	struct ip_vs_iphdr iph;
+	int af;
+	struct ip_vs_service *svc;
+
+	//EnterFunction(11);
+
+	
+	af = (skb->protocol == htons(ETH_P_IP)) ? AF_INET : AF_INET6;
+
+	ip_vs_fill_iphdr(af, skb_network_header(skb), &iph);
+
+	/* drop all ip fragment except ospf */
+	if ((sysctl_ip_vs_frag_drop_entry == 1)
+	    && (ip_hdr(skb)->frag_off & htons(IP_MF | IP_OFFSET))
+	    && (iph.protocol != IPPROTO_OSPF)) {
+		IP_VS_INC_ESTATS(ip_vs_esmib, DEFENCE_IP_FRAG_DROP);
+		return NF_DROP;
+	}
+
+	/* drop udp packet which send to tcp-vip */
+	if ((sysctl_ip_vs_udp_drop_entry == 1) && (iph.protocol == IPPROTO_UDP)) {
+		if ((svc =
+		     ip_vs_lookup_vip(af, IPPROTO_TCP, &iph.daddr)) != NULL) {
+			IP_VS_INC_ESTATS(ip_vs_esmib, DEFENCE_UDP_DROP);
+			return NF_DROP;
+		}
+	}
+
+	/* synproxy: defence synflood */
+	if (iph.protocol == IPPROTO_TCP) {
+		int v = NF_ACCEPT;
+		if (0 == ip_vs_synproxy_syn_rcv(af, skb, &iph, &v)) {
+			return v;
+		}
+	}
+
+	return NF_ACCEPT;
+}
 
 static struct nf_hook_ops ip_vs_ops[] __read_mostly = {
 	/* After packet filtering, forward packet through VS/DR, VS/TUN,
 	 * or VS/NAT(change destination), so that filtering rules can be
 	 * applied to IPVS. */
 	{
-		.hook		= ip_vs_in,
-		.owner		= THIS_MODULE,
-		.pf		= PF_INET,
-		.hooknum        = NF_INET_LOCAL_IN,
-		.priority       = 100,
-	},
+	 .hook = ip_vs_in,
+	 .owner = THIS_MODULE,
+	 .pf = PF_INET,
+	 .hooknum = NF_INET_LOCAL_IN,
+	 .priority = 100,
+	 },
 	/* After packet filtering, change source only for VS/NAT */
 	{
-		.hook		= ip_vs_out,
-		.owner		= THIS_MODULE,
-		.pf		= PF_INET,
-		.hooknum        = NF_INET_FORWARD,
-		.priority       = 100,
-	},
+	 .hook = ip_vs_out,
+	 .owner = THIS_MODULE,
+	 .pf = PF_INET,
+	 .hooknum = NF_INET_FORWARD,
+	 .priority = 100,
+	 },
 	/* After packet filtering (but before ip_vs_out_icmp), catch icmp
 	 * destined for 0.0.0.0/0, which is for incoming IPVS connections */
 	{
-		.hook		= ip_vs_forward_icmp,
-		.owner		= THIS_MODULE,
-		.pf		= PF_INET,
-		.hooknum        = NF_INET_FORWARD,
-		.priority       = 99,
-	},
+	 .hook = ip_vs_forward_icmp,
+	 .owner = THIS_MODULE,
+	 .pf = PF_INET,
+	 .hooknum = NF_INET_FORWARD,
+	 .priority = 99,
+	 },
 	/* Before the netfilter connection tracking, exit from POST_ROUTING */
 	{
-		.hook		= ip_vs_post_routing,
-		.owner		= THIS_MODULE,
-		.pf		= PF_INET,
-		.hooknum        = NF_INET_POST_ROUTING,
-		.priority       = NF_IP_PRI_NAT_SRC-1,
-	},
+	 .hook = ip_vs_post_routing,
+	 .owner = THIS_MODULE,
+	 .pf = PF_INET,
+	 .hooknum = NF_INET_POST_ROUTING,
+	 .priority = NF_IP_PRI_NAT_SRC - 1,
+	 },
+	/* Before the netfilter connection tracking, only deal with FULLNAT/NAT-SynProxy */
+	{
+	 .hook = ip_vs_pre_routing,
+	 .owner = THIS_MODULE,
+	 .pf = PF_INET,
+	 .hooknum = NF_INET_PRE_ROUTING,
+	 .priority = NF_IP_PRI_CONNTRACK - 1,
+	 },
 #ifdef CONFIG_IP_VS_IPV6
 	/* After packet filtering, forward packet through VS/DR, VS/TUN,
 	 * or VS/NAT(change destination), so that filtering rules can be
 	 * applied to IPVS. */
 	{
-		.hook		= ip_vs_in,
-		.owner		= THIS_MODULE,
-		.pf		= PF_INET6,
-		.hooknum        = NF_INET_LOCAL_IN,
-		.priority       = 100,
-	},
+	 .hook = ip_vs_in,
+	 .owner = THIS_MODULE,
+	 .pf = PF_INET6,
+	 .hooknum = NF_INET_LOCAL_IN,
+	 .priority = 100,
+	 },
 	/* After packet filtering, change source only for VS/NAT */
 	{
-		.hook		= ip_vs_out,
-		.owner		= THIS_MODULE,
-		.pf		= PF_INET6,
-		.hooknum        = NF_INET_FORWARD,
-		.priority       = 100,
-	},
+	 .hook = ip_vs_out,
+	 .owner = THIS_MODULE,
+	 .pf = PF_INET6,
+	 .hooknum = NF_INET_FORWARD,
+	 .priority = 100,
+	 },
 	/* After packet filtering (but before ip_vs_out_icmp), catch icmp
 	 * destined for 0.0.0.0/0, which is for incoming IPVS connections */
 	{
-		.hook		= ip_vs_forward_icmp_v6,
-		.owner		= THIS_MODULE,
-		.pf		= PF_INET6,
-		.hooknum        = NF_INET_FORWARD,
-		.priority       = 99,
-	},
+	 .hook = ip_vs_forward_icmp_v6,
+	 .owner = THIS_MODULE,
+	 .pf = PF_INET6,
+	 .hooknum = NF_INET_FORWARD,
+	 .priority = 99,
+	 },
 	/* Before the netfilter connection tracking, exit from POST_ROUTING */
 	{
-		.hook		= ip_vs_post_routing,
-		.owner		= THIS_MODULE,
-		.pf		= PF_INET6,
-		.hooknum        = NF_INET_POST_ROUTING,
-		.priority       = NF_IP6_PRI_NAT_SRC-1,
-	},
+	 .hook = ip_vs_post_routing,
+	 .owner = THIS_MODULE,
+	 .pf = PF_INET6,
+	 .hooknum = NF_INET_POST_ROUTING,
+	 .priority = NF_IP6_PRI_NAT_SRC - 1,
+	 },
+	/* Before the netfilter connection tracking, only deal with FULLNAT/NAT-SynProxy */
+	{
+	 .hook = ip_vs_pre_routing,
+	 .owner = THIS_MODULE,
+	 .pf = PF_INET6,
+	 .hooknum = NF_INET_PRE_ROUTING,
+	 .priority = NF_IP6_PRI_CONNTRACK - 1,
+	 },
 #endif
 };
 
-
 /*
  *	Initialize IP Virtual Server
  */
@@ -1536,14 +1660,14 @@ static int __init ip_vs_init(void)
 	pr_info("ipvs loaded.\n");
 	return ret;
 
-  cleanup_conn:
+      cleanup_conn:
 	ip_vs_conn_cleanup();
-  cleanup_app:
+      cleanup_app:
 	ip_vs_app_cleanup();
-  cleanup_protocol:
+      cleanup_protocol:
 	ip_vs_protocol_cleanup();
 	ip_vs_control_cleanup();
-  cleanup_estimator:
+      cleanup_estimator:
 	ip_vs_estimator_cleanup();
 	return ret;
 }
diff -uprN linux-2.6.32-220.23.1.el6.x86_64.bak/net/netfilter/ipvs/ip_vs_ctl.c linux-2.6.32-220.23.1.el6.x86_64/net/netfilter/ipvs/ip_vs_ctl.c
--- linux-2.6.32-220.23.1.el6.x86_64.bak/net/netfilter/ipvs/ip_vs_ctl.c	2012-06-12 21:31:32.000000000 +0800
+++ linux-2.6.32-220.23.1.el6.x86_64/net/netfilter/ipvs/ip_vs_ctl.c	2013-03-18 21:07:09.000000000 +0800
@@ -15,7 +15,10 @@
  *              2 of the License, or (at your option) any later version.
  *
  * Changes:
+ *	Shunmin Zhu  <jianghe.zsm@taobao.com>
+ *	Jiaming Wu   <pukong.wjm@taobao.com>	support FULLNAT+SYNPROXY
  *
+ *   Yu Bo        <yubo@xiaomi.com>
  */
 
 #define KMSG_COMPONENT "IPVS"
@@ -49,6 +52,7 @@
 #include <asm/uaccess.h>
 
 #include <net/ip_vs.h>
+#include <net/ip_vs_synproxy.h>
 
 /* semaphore for IPVS sockopts. And, [gs]etsockopt may sleep. */
 static DEFINE_MUTEX(__ip_vs_mutex);
@@ -87,6 +91,67 @@ int sysctl_ip_vs_expire_nodest_conn = 0;
 int sysctl_ip_vs_expire_quiescent_template = 0;
 int sysctl_ip_vs_sync_threshold[2] = { 3, 50 };
 int sysctl_ip_vs_nat_icmp_send = 0;
+/*
+ * sysctl for FULLNAT
+ */
+int sysctl_ip_vs_timestamp_remove_entry = 1;
+int sysctl_ip_vs_mss_adjust_entry = 1;
+int sysctl_ip_vs_conn_reused_entry = 1;
+int sysctl_ip_vs_toa_entry = 1;
+static int ip_vs_entry_min = 0;
+static int ip_vs_entry_max = 1;
+extern int sysctl_ip_vs_tcp_timeouts[IP_VS_TCP_S_LAST + 1];
+/*
+ * sysctl for SYNPROXY
+ */
+/* syn-proxy sysctl variables */
+int sysctl_ip_vs_synproxy_init_mss = IP_VS_SYNPROXY_INIT_MSS_DEFAULT;
+int sysctl_ip_vs_synproxy_sack = IP_VS_SYNPROXY_SACK_DEFAULT;
+int sysctl_ip_vs_synproxy_wscale = IP_VS_SYNPROXY_WSCALE_DEFAULT;
+int sysctl_ip_vs_synproxy_timestamp = IP_VS_SYNPROXY_TIMESTAMP_DEFAULT;
+int sysctl_ip_vs_synproxy_synack_ttl = IP_VS_SYNPROXY_TTL_DEFAULT;
+int sysctl_ip_vs_synproxy_defer = IP_VS_SYNPROXY_DEFER_DEFAULT;
+int sysctl_ip_vs_synproxy_conn_reuse = IP_VS_SYNPROXY_CONN_REUSE_DEFAULT;
+int sysctl_ip_vs_synproxy_conn_reuse_cl = IP_VS_SYNPROXY_CONN_REUSE_CL_DEFAULT;
+int sysctl_ip_vs_synproxy_conn_reuse_tw = IP_VS_SYNPROXY_CONN_REUSE_TW_DEFAULT;
+int sysctl_ip_vs_synproxy_conn_reuse_fw = IP_VS_SYNPROXY_CONN_REUSE_FW_DEFAULT;
+int sysctl_ip_vs_synproxy_conn_reuse_cw = IP_VS_SYNPROXY_CONN_REUSE_CW_DEFAULT;
+int sysctl_ip_vs_synproxy_conn_reuse_la = IP_VS_SYNPROXY_CONN_REUSE_LA_DEFAULT;
+int sysctl_ip_vs_synproxy_dup_ack_thresh = IP_VS_SYNPROXY_DUP_ACK_DEFAULT;
+int sysctl_ip_vs_synproxy_skb_store_thresh = IP_VS_SYNPROXY_SKB_STORE_DEFAULT;
+int sysctl_ip_vs_synproxy_syn_retry = IP_VS_SYNPROXY_SYN_RETRY_DEFAULT;
+
+static int ip_vs_synproxy_switch_min = 0;
+static int ip_vs_synproxy_switch_max = 1;
+static int ip_vs_synproxy_wscale_min = 0;
+static int ip_vs_synproxy_wscale_max = IP_VS_SYNPROXY_WSCALE_MAX;
+static int ip_vs_synproxy_init_mss_min = 0;
+static int ip_vs_synproxy_init_mss_max = 65535;
+static int ip_vs_synproxy_synack_ttl_min = IP_VS_SYNPROXY_TTL_MIN;
+static int ip_vs_synproxy_synack_ttl_max = IP_VS_SYNPROXY_TTL_MAX;
+static int ip_vs_synproxy_dup_ack_cnt_min = 0;
+static int ip_vs_synproxy_dup_ack_cnt_max = 65535;
+static int ip_vs_synproxy_syn_retry_min = 0;
+static int ip_vs_synproxy_syn_retry_max = 6;
+static int ip_vs_synproxy_skb_store_thresh_min = 0;
+static int ip_vs_synproxy_skb_store_thresh_max = 5;
+/* local address port range */
+int sysctl_ip_vs_lport_max = 65535;
+int sysctl_ip_vs_lport_min = 5000;
+int sysctl_ip_vs_lport_tries = 10000;
+static int ip_vs_port_min = 1025;
+static int ip_vs_port_max = 65535;
+static int ip_vs_port_try_min = 10;
+static int ip_vs_port_try_max = 60000;
+/*
+ * sysctl for DEFENCE ATTACK
+ */
+int sysctl_ip_vs_frag_drop_entry = 1;
+int sysctl_ip_vs_tcp_drop_entry = 1;
+int sysctl_ip_vs_udp_drop_entry = 1;
+/* send rst when tcp session expire */
+int sysctl_ip_vs_conn_expire_tcp_rst = 1;
+
 
 
 #ifdef CONFIG_IP_VS_DEBUG
@@ -106,14 +171,14 @@ static int __ip_vs_addr_is_local_v6(cons
 	struct flowi fl = {
 		.oif = 0,
 		.nl_u = {
-			.ip6_u = {
-				.daddr = *addr,
-				.saddr = { .s6_addr32 = {0, 0, 0, 0} }, } },
+			 .ip6_u = {
+				   .daddr = *addr,
+				   .saddr = {.s6_addr32 = {0, 0, 0, 0}},}},
 	};
 
 	rt = (struct rt6_info *)ip6_route_output(&init_net, NULL, &fl);
 	if (rt && rt->rt6i_dev && (rt->rt6i_dev->flags & IFF_LOOPBACK))
-			return 1;
+		return 1;
 
 	return 0;
 }
@@ -179,8 +244,8 @@ static void update_defense_level(void)
 	case 1:
 		if (nomem) {
 			ip_vs_drop_rate = ip_vs_drop_counter
-				= sysctl_ip_vs_amemthresh /
-				(sysctl_ip_vs_amemthresh-availmem);
+			    = sysctl_ip_vs_amemthresh /
+			    (sysctl_ip_vs_amemthresh - availmem);
 			sysctl_ip_vs_drop_packet = 2;
 		} else {
 			ip_vs_drop_rate = 0;
@@ -189,8 +254,8 @@ static void update_defense_level(void)
 	case 2:
 		if (nomem) {
 			ip_vs_drop_rate = ip_vs_drop_counter
-				= sysctl_ip_vs_amemthresh /
-				(sysctl_ip_vs_amemthresh-availmem);
+			    = sysctl_ip_vs_amemthresh /
+			    (sysctl_ip_vs_amemthresh - availmem);
 		} else {
 			ip_vs_drop_rate = 0;
 			sysctl_ip_vs_drop_packet = 1;
@@ -236,13 +301,12 @@ static void update_defense_level(void)
 	}
 	old_secure_tcp = sysctl_ip_vs_secure_tcp;
 	if (to_change >= 0)
-		ip_vs_protocol_timeout_change(sysctl_ip_vs_secure_tcp>1);
+		ip_vs_protocol_timeout_change(sysctl_ip_vs_secure_tcp > 1);
 	write_unlock(&__ip_vs_securetcp_lock);
 
 	local_bh_enable();
 }
 
-
 /*
  *	Timer for checking the defense
  */
@@ -259,19 +323,16 @@ static void defense_work_handler(struct 
 	schedule_delayed_work(&defense_work, DEFENSE_TIMER_PERIOD);
 }
 
-int
-ip_vs_use_count_inc(void)
+int ip_vs_use_count_inc(void)
 {
 	return try_module_get(THIS_MODULE);
 }
 
-void
-ip_vs_use_count_dec(void)
+void ip_vs_use_count_dec(void)
 {
 	module_put(THIS_MODULE);
 }
 
-
 /*
  *	Hash table: for virtual service lookups
  */
@@ -284,6 +345,8 @@ static struct list_head ip_vs_svc_table[
 /* the service table hashed by fwmark */
 static struct list_head ip_vs_svc_fwm_table[IP_VS_SVC_TAB_SIZE];
 
+static struct ip_vs_dsnat ip_vs_dsnat;
+
 /*
  *	Hash table: for real service lookups
  */
@@ -304,27 +367,29 @@ static LIST_HEAD(ip_vs_dest_trash);
 static atomic_t ip_vs_ftpsvc_counter = ATOMIC_INIT(0);
 static atomic_t ip_vs_nullsvc_counter = ATOMIC_INIT(0);
 
-
 /*
  *	Returns hash value for virtual service
  */
 static __inline__ unsigned
-ip_vs_svc_hashkey(int af, unsigned proto, const union nf_inet_addr *addr,
-		  __be16 port)
+ip_vs_svc_hashkey(int af, unsigned proto, const union nf_inet_addr *addr)
 {
-	register unsigned porth = ntohs(port);
 	__be32 addr_fold = addr->ip;
 
 #ifdef CONFIG_IP_VS_IPV6
 	if (af == AF_INET6)
-		addr_fold = addr->ip6[0]^addr->ip6[1]^
-			    addr->ip6[2]^addr->ip6[3];
+		addr_fold = addr->ip6[0] ^ addr->ip6[1] ^
+		addr->ip6[2] ^ addr->ip6[3];
 #endif
 
-	return (proto^ntohl(addr_fold)^(porth>>IP_VS_SVC_TAB_BITS)^porth)
-		& IP_VS_SVC_TAB_MASK;
+	if(af & IP_VS_CONN_F_DSNAT){
+		addr_fold = 0;
+	} 
+
+	return (proto ^ ntohl(addr_fold)) & IP_VS_SVC_TAB_MASK;
 }
 
+
+
 /*
  *	Returns hash value of fwmark for virtual service lookup
  */
@@ -352,8 +417,7 @@ static int ip_vs_svc_hash(struct ip_vs_s
 		/*
 		 *  Hash it by <protocol,addr,port> in ip_vs_svc_table
 		 */
-		hash = ip_vs_svc_hashkey(svc->af, svc->protocol, &svc->addr,
-					 svc->port);
+		hash = ip_vs_svc_hashkey(svc->af, svc->protocol, &svc->addr);
 		list_add(&svc->s_list, &ip_vs_svc_table[hash]);
 	} else {
 		/*
@@ -369,7 +433,6 @@ static int ip_vs_svc_hash(struct ip_vs_s
 	return 1;
 }
 
-
 /*
  *	Unhashes a service from ip_vs_svc_table/ip_vs_svc_fwm_table.
  *	Should be called with locked tables.
@@ -396,42 +459,59 @@ static int ip_vs_svc_unhash(struct ip_vs
 }
 
 
+
+
 /*
  *	Get service by {proto,addr,port} in the service table.
+ *   yubo@xiaomi.com
  */
-static inline struct ip_vs_service *
-__ip_vs_service_get(int af, __u16 protocol, const union nf_inet_addr *vaddr,
-		    __be16 vport)
+static inline struct ip_vs_service *__ip_vs_service_get(int af, __u16 protocol,
+							const union nf_inet_addr
+							*vaddr, __be16 vport)
 {
 	unsigned hash;
 	struct ip_vs_service *svc;
+	int dsnat = af & IP_VS_CONN_F_DSNAT;
+	af &= ~IP_VS_CONN_F_DSNAT; 
 
-	/* Check for "full" addressed entries */
-	hash = ip_vs_svc_hashkey(af, protocol, vaddr, vport);
 
-	list_for_each_entry(svc, &ip_vs_svc_table[hash], s_list){
-		if ((svc->af == af)
-		    && ip_vs_addr_equal(af, &svc->addr, vaddr)
-		    && (svc->port == vport)
-		    && (svc->protocol == protocol)) {
-			/* HIT */
-			atomic_inc(&svc->usecnt);
-			return svc;
+	/* Check for "full" addressed entries */
+	hash = ip_vs_svc_hashkey(af|dsnat, protocol, vaddr);
+	
+	if(dsnat){
+		list_for_each_entry(svc, &ip_vs_svc_table[hash], s_list) {
+			if ((svc->af == af)
+				&& (svc->addr.ip == 0)
+				&& (svc->port == 0) 
+				&& (svc->protocol == protocol)) {  
+				/* HIT */    
+				atomic_inc(&svc->usecnt);
+				return svc;
+			}
+		}
+	}else{
+		list_for_each_entry(svc, &ip_vs_svc_table[hash], s_list) {
+			if ((svc->af == af)
+			    && ip_vs_addr_equal(af, &svc->addr, vaddr)
+			    && (svc->port == vport)
+			    && (svc->protocol == protocol)) {
+				/* HIT */
+				atomic_inc(&svc->usecnt);
+				return svc;
+			}
 		}
 	}
-
 	return NULL;
 }
 
-
 /*
  *	Get service by {fwmark} in the service table.
  */
-static inline struct ip_vs_service *
-__ip_vs_svc_fwm_get(int af, __u32 fwmark)
+static inline struct ip_vs_service *__ip_vs_svc_fwm_get(int af, __u32 fwmark)
 {
 	unsigned hash;
 	struct ip_vs_service *svc;
+	af &= ~IP_VS_CONN_F_DSNAT;
 
 	/* Check for fwmark addressed entries */
 	hash = ip_vs_svc_fwm_hashkey(fwmark);
@@ -447,29 +527,35 @@ __ip_vs_svc_fwm_get(int af, __u32 fwmark
 	return NULL;
 }
 
-struct ip_vs_service *
-ip_vs_service_get(int af, __u32 fwmark, __u16 protocol,
-		  const union nf_inet_addr *vaddr, __be16 vport)
+struct ip_vs_dsnat *ip_vs_dsnat_get(void)
+{
+	return &ip_vs_dsnat;
+}
+
+
+
+struct ip_vs_service *ip_vs_service_get(int af, __u32 fwmark, __u16 protocol,
+					const union nf_inet_addr *vaddr,
+					__be16 vport)
 {
 	struct ip_vs_service *svc;
 
 	read_lock(&__ip_vs_svc_lock);
 
 	/*
-	 *	Check the table hashed by fwmark first
+	 *      Check the table hashed by fwmark first
 	 */
 	if (fwmark && (svc = __ip_vs_svc_fwm_get(af, fwmark)))
 		goto out;
 
 	/*
-	 *	Check the table hashed by <protocol,addr,port>
-	 *	for "full" addressed entries
+	 *      Check the table hashed by <protocol,addr,port>
+	 *      for "full" addressed entries
 	 */
 	svc = __ip_vs_service_get(af, protocol, vaddr, vport);
 
 	if (svc == NULL
-	    && protocol == IPPROTO_TCP
-	    && atomic_read(&ip_vs_ftpsvc_counter)
+	    && protocol == IPPROTO_TCP && atomic_read(&ip_vs_ftpsvc_counter)
 	    && (vport == FTPDATA || ntohs(vport) >= PROT_SOCK)) {
 		/*
 		 * Check if ftp service entry exists, the packet
@@ -478,15 +564,14 @@ ip_vs_service_get(int af, __u32 fwmark, 
 		svc = __ip_vs_service_get(af, protocol, vaddr, FTPPORT);
 	}
 
-	if (svc == NULL
-	    && atomic_read(&ip_vs_nullsvc_counter)) {
+	if (svc == NULL && atomic_read(&ip_vs_nullsvc_counter)) {
 		/*
 		 * Check if the catch-all port (port zero) exists
 		 */
 		svc = __ip_vs_service_get(af, protocol, vaddr, 0);
 	}
 
-  out:
+      out:
 	read_unlock(&__ip_vs_svc_lock);
 
 	IP_VS_DBG_BUF(9, "lookup service: fwm %u %s %s:%u %s\n",
@@ -497,6 +582,43 @@ ip_vs_service_get(int af, __u32 fwmark, 
 	return svc;
 }
 
+struct ip_vs_service *ip_vs_lookup_vip(int af, __u16 protocol,
+				       const union nf_inet_addr *vaddr)
+{
+	struct ip_vs_service *svc;
+	unsigned hash;
+	int dsnat = af & IP_VS_CONN_F_DSNAT;
+	af &= ~IP_VS_CONN_F_DSNAT; 
+
+	read_lock(&__ip_vs_svc_lock);
+
+	hash = ip_vs_svc_hashkey(af|dsnat, protocol, vaddr);
+	if(dsnat){
+		list_for_each_entry(svc, &ip_vs_svc_table[hash], s_list) {
+			if ((svc->af == af)
+				&& (svc->addr.ip == 0)
+				&& (svc->port == 0) 
+				&& (svc->protocol == protocol)) {  
+				/* HIT */    
+				read_unlock(&__ip_vs_svc_lock);
+				return svc;
+			}
+		}
+	}else{
+		list_for_each_entry(svc, &ip_vs_svc_table[hash], s_list) {
+			if ((svc->af == af)
+			    && ip_vs_addr_equal(af, &svc->addr, vaddr)
+			    && (svc->protocol == protocol)) {
+				/* HIT */
+				read_unlock(&__ip_vs_svc_lock);
+				return svc;
+			}
+		}
+	}
+
+	read_unlock(&__ip_vs_svc_lock);
+	return NULL;
+}
 
 static inline void
 __ip_vs_bind_svc(struct ip_vs_dest *dest, struct ip_vs_service *svc)
@@ -505,8 +627,7 @@ __ip_vs_bind_svc(struct ip_vs_dest *dest
 	dest->svc = svc;
 }
 
-static inline void
-__ip_vs_unbind_svc(struct ip_vs_dest *dest)
+static inline void __ip_vs_unbind_svc(struct ip_vs_dest *dest)
 {
 	struct ip_vs_service *svc = dest->svc;
 
@@ -515,25 +636,24 @@ __ip_vs_unbind_svc(struct ip_vs_dest *de
 		kfree(svc);
 }
 
-
 /*
  *	Returns hash value for real service
  */
 static inline unsigned ip_vs_rs_hashkey(int af,
-					    const union nf_inet_addr *addr,
-					    __be16 port)
+					const union nf_inet_addr *addr,
+					__be16 port)
 {
 	register unsigned porth = ntohs(port);
 	__be32 addr_fold = addr->ip;
 
 #ifdef CONFIG_IP_VS_IPV6
 	if (af == AF_INET6)
-		addr_fold = addr->ip6[0]^addr->ip6[1]^
-			    addr->ip6[2]^addr->ip6[3];
+		addr_fold = addr->ip6[0] ^ addr->ip6[1] ^
+		    addr->ip6[2] ^ addr->ip6[3];
 #endif
 
-	return (ntohl(addr_fold)^(porth>>IP_VS_RTAB_BITS)^porth)
-		& IP_VS_RTAB_MASK;
+	return (ntohl(addr_fold) ^ (porth >> IP_VS_RTAB_BITS) ^ porth)
+	    & IP_VS_RTAB_MASK;
 }
 
 /*
@@ -549,8 +669,8 @@ static int ip_vs_rs_hash(struct ip_vs_de
 	}
 
 	/*
-	 *	Hash by proto,addr,port,
-	 *	which are the parameters of the real service.
+	 *      Hash by proto,addr,port,
+	 *      which are the parameters of the real service.
 	 */
 	hash = ip_vs_rs_hashkey(dest->af, &dest->addr, dest->port);
 
@@ -579,17 +699,16 @@ static int ip_vs_rs_unhash(struct ip_vs_
 /*
  *	Lookup real service by <proto,addr,port> in the real service table.
  */
-struct ip_vs_dest *
-ip_vs_lookup_real_service(int af, __u16 protocol,
-			  const union nf_inet_addr *daddr,
-			  __be16 dport)
+struct ip_vs_dest *ip_vs_lookup_real_service(int af, __u16 protocol,
+					     const union nf_inet_addr *daddr,
+					     __be16 dport)
 {
 	unsigned hash;
 	struct ip_vs_dest *dest;
 
 	/*
-	 *	Check for "full" addressed entries
-	 *	Return the first found entry
+	 *      Check for "full" addressed entries
+	 *      Return the first found entry
 	 */
 	hash = ip_vs_rs_hashkey(af, daddr, dport);
 
@@ -598,8 +717,7 @@ ip_vs_lookup_real_service(int af, __u16 
 		if ((dest->af == af)
 		    && ip_vs_addr_equal(af, &dest->addr, daddr)
 		    && (dest->port == dport)
-		    && ((dest->protocol == protocol) ||
-			dest->vfwmark)) {
+		    && ((dest->protocol == protocol) || dest->vfwmark)) {
 			/* HIT */
 			read_unlock(&__ip_vs_rs_lock);
 			return dest;
@@ -613,9 +731,9 @@ ip_vs_lookup_real_service(int af, __u16 
 /*
  *	Lookup destination by {addr,port} in the given service
  */
-static struct ip_vs_dest *
-ip_vs_lookup_dest(struct ip_vs_service *svc, const union nf_inet_addr *daddr,
-		  __be16 dport)
+static struct ip_vs_dest *ip_vs_lookup_dest(struct ip_vs_service *svc,
+					    const union nf_inet_addr *daddr,
+					    __be16 dport)
 {
 	struct ip_vs_dest *dest;
 
@@ -672,9 +790,9 @@ struct ip_vs_dest *ip_vs_find_dest(int a
  *  continue, and the counting information of the dest is also useful for
  *  scheduling.
  */
-static struct ip_vs_dest *
-ip_vs_trash_get_dest(struct ip_vs_service *svc, const union nf_inet_addr *daddr,
-		     __be16 dport)
+static struct ip_vs_dest *ip_vs_trash_get_dest(struct ip_vs_service *svc,
+					       const union nf_inet_addr *daddr,
+					       __be16 dport)
 {
 	struct ip_vs_dest *dest, *nxt;
 
@@ -686,8 +804,7 @@ ip_vs_trash_get_dest(struct ip_vs_servic
 			      "dest->refcnt=%d\n",
 			      dest->vfwmark,
 			      IP_VS_DBG_ADDR(svc->af, &dest->addr),
-			      ntohs(dest->port),
-			      atomic_read(&dest->refcnt));
+			      ntohs(dest->port), atomic_read(&dest->refcnt));
 		if (dest->af == svc->af &&
 		    ip_vs_addr_equal(svc->af, &dest->addr, daddr) &&
 		    dest->port == dport &&
@@ -719,7 +836,6 @@ ip_vs_trash_get_dest(struct ip_vs_servic
 	return NULL;
 }
 
-
 /*
  *  Clean up all the destinations in the trash
  *  Called by the ip_vs_control_cleanup()
@@ -741,9 +857,7 @@ static void ip_vs_trash_cleanup(void)
 	}
 }
 
-
-static void
-ip_vs_zero_stats(struct ip_vs_stats *stats)
+static void ip_vs_zero_stats(struct ip_vs_stats *stats)
 {
 	spin_lock_bh(&stats->lock);
 
@@ -771,14 +885,14 @@ __ip_vs_update_dest(struct ip_vs_service
 	if (svc->af == AF_INET6) {
 		if (__ip_vs_addr_is_local_v6(&udest->addr.in6)) {
 			conn_flags = (conn_flags & ~IP_VS_CONN_F_FWD_MASK)
-				| IP_VS_CONN_F_LOCALNODE;
+			    | IP_VS_CONN_F_LOCALNODE;
 		}
 	} else
 #endif
-		if (inet_addr_type(&init_net, udest->addr.ip) == RTN_LOCAL) {
-			conn_flags = (conn_flags & ~IP_VS_CONN_F_FWD_MASK)
-				| IP_VS_CONN_F_LOCALNODE;
-		}
+	if (inet_addr_type(&init_net, udest->addr.ip) == RTN_LOCAL) {
+		conn_flags = (conn_flags & ~IP_VS_CONN_F_FWD_MASK)
+		    | IP_VS_CONN_F_LOCALNODE;
+	}
 
 	/* set the IP_VS_CONN_F_NOOUTPUT flag if not masquerading/NAT */
 	if ((conn_flags & IP_VS_CONN_F_FWD_MASK) != 0) {
@@ -814,7 +928,6 @@ __ip_vs_update_dest(struct ip_vs_service
 	dest->l_threshold = udest->l_threshold;
 }
 
-
 /*
  *	Create a destination for the given service
  */
@@ -831,8 +944,8 @@ ip_vs_new_dest(struct ip_vs_service *svc
 	if (svc->af == AF_INET6) {
 		atype = ipv6_addr_type(&udest->addr.in6);
 		if ((!(atype & IPV6_ADDR_UNICAST) ||
-			atype & IPV6_ADDR_LINKLOCAL) &&
-			!__ip_vs_addr_is_local_v6(&udest->addr.in6))
+		     atype & IPV6_ADDR_LINKLOCAL) &&
+		    !__ip_vs_addr_is_local_v6(&udest->addr.in6))
 			return -EINVAL;
 	} else
 #endif
@@ -873,7 +986,6 @@ ip_vs_new_dest(struct ip_vs_service *svc
 	return 0;
 }
 
-
 /*
  *	Add a destination into an existing service
  */
@@ -894,7 +1006,7 @@ ip_vs_add_dest(struct ip_vs_service *svc
 
 	if (udest->l_threshold > udest->u_threshold) {
 		pr_err("%s(): lower threshold is higher than upper threshold\n",
-			__func__);
+		       __func__);
 		return -ERANGE;
 	}
 
@@ -986,7 +1098,6 @@ ip_vs_add_dest(struct ip_vs_service *svc
 	return 0;
 }
 
-
 /*
  *	Edit a destination in the given service
  */
@@ -1006,7 +1117,7 @@ ip_vs_edit_dest(struct ip_vs_service *sv
 
 	if (udest->l_threshold > udest->u_threshold) {
 		pr_err("%s(): lower threshold is higher than upper threshold\n",
-			__func__);
+		       __func__);
 		return -ERANGE;
 	}
 
@@ -1040,7 +1151,6 @@ ip_vs_edit_dest(struct ip_vs_service *sv
 	return 0;
 }
 
-
 /*
  *	Delete a destination (must be already unlinked from the service)
  */
@@ -1073,20 +1183,17 @@ static void __ip_vs_del_dest(struct ip_v
 		IP_VS_DBG_BUF(3, "Moving dest %s:%u into trash, "
 			      "dest->refcnt=%d\n",
 			      IP_VS_DBG_ADDR(dest->af, &dest->addr),
-			      ntohs(dest->port),
-			      atomic_read(&dest->refcnt));
+			      ntohs(dest->port), atomic_read(&dest->refcnt));
 		list_add(&dest->n_list, &ip_vs_dest_trash);
 		atomic_inc(&dest->refcnt);
 	}
 }
 
-
 /*
  *	Unlink a destination from the given service
  */
 static void __ip_vs_unlink_dest(struct ip_vs_service *svc,
-				struct ip_vs_dest *dest,
-				int svcupd)
+				struct ip_vs_dest *dest, int svcupd)
 {
 	dest->flags &= ~IP_VS_DEST_F_AVAILABLE;
 
@@ -1100,10 +1207,9 @@ static void __ip_vs_unlink_dest(struct i
 	 *  Call the update_service function of its scheduler
 	 */
 	if (svcupd && svc->scheduler->update_service)
-			svc->scheduler->update_service(svc);
+		svc->scheduler->update_service(svc);
 }
 
-
 /*
  *	Delete a destination server in the given service
  */
@@ -1125,19 +1231,19 @@ ip_vs_del_dest(struct ip_vs_service *svc
 	write_lock_bh(&__ip_vs_svc_lock);
 
 	/*
-	 *	Wait until all other svc users go away.
+	 *      Wait until all other svc users go away.
 	 */
 	IP_VS_WAIT_WHILE(atomic_read(&svc->usecnt) > 1);
 
 	/*
-	 *	Unlink dest from the service
+	 *      Unlink dest from the service
 	 */
 	__ip_vs_unlink_dest(svc, dest, 1);
 
 	write_unlock_bh(&__ip_vs_svc_lock);
 
 	/*
-	 *	Delete the destination
+	 *      Delete the destination
 	 */
 	__ip_vs_del_dest(dest);
 
@@ -1146,6 +1252,167 @@ ip_vs_del_dest(struct ip_vs_service *svc
 	return 0;
 }
 
+void ip_vs_laddr_hold(struct ip_vs_laddr *laddr)
+{
+	atomic_inc(&laddr->refcnt);
+}
+
+void ip_vs_laddr_put(struct ip_vs_laddr *laddr)
+{
+	if (atomic_dec_and_test(&laddr->refcnt)) {
+		kfree(laddr);
+	}
+}
+
+static int
+ip_vs_new_laddr(struct ip_vs_service *svc, struct ip_vs_laddr_user_kern *uladdr,
+		struct ip_vs_laddr **laddr_p)
+{
+	struct ip_vs_laddr *laddr;
+
+	laddr = kzalloc(sizeof(struct ip_vs_laddr), GFP_ATOMIC);
+	if (!laddr) {
+		pr_err("%s(): no memory.\n", __func__);
+		return -ENOMEM;
+	}
+
+	laddr->af = svc->af;
+	ip_vs_addr_copy(svc->af, &laddr->addr, &uladdr->addr);
+	atomic64_set(&laddr->port_conflict, 0);
+	atomic64_set(&laddr->port, 0);
+	atomic_set(&laddr->refcnt, 0);
+	atomic_set(&laddr->conn_counts, 0);
+
+	*laddr_p = laddr;
+
+	return 0;
+}
+
+static struct ip_vs_laddr *ip_vs_lookup_laddr(struct ip_vs_service *svc,
+					      const union nf_inet_addr *addr)
+{
+	struct ip_vs_laddr *laddr;
+
+	/*
+	 * Find the local address for the given service
+	 */
+	list_for_each_entry(laddr, &svc->laddr_list, n_list) {
+		if ((laddr->af == svc->af)
+		    && ip_vs_addr_equal(svc->af, &laddr->addr, addr)) {
+			/* HIT */
+			return laddr;
+		}
+	}
+
+	return NULL;
+}
+
+static int
+ip_vs_add_laddr(struct ip_vs_service *svc, struct ip_vs_laddr_user_kern *uladdr)
+{
+	struct ip_vs_laddr *laddr;
+	int ret;
+
+	IP_VS_DBG_BUF(0, "vip %s:%d add local address %s\n",
+		      IP_VS_DBG_ADDR(svc->af, &svc->addr), ntohs(svc->port),
+		      IP_VS_DBG_ADDR(svc->af, &uladdr->addr));
+
+	/*
+	 * Check if the local address already exists in the list
+	 */
+	laddr = ip_vs_lookup_laddr(svc, &uladdr->addr);
+	if (laddr) {
+		IP_VS_DBG(1, "%s(): local address already exists\n", __func__);
+		return -EEXIST;
+	}
+
+	/*
+	 * Allocate and initialize the dest structure
+	 */
+	ret = ip_vs_new_laddr(svc, uladdr, &laddr);
+	if (ret) {
+		return ret;
+	}
+
+	/*
+	 * Add the local adress entry into the list
+	 */
+	ip_vs_laddr_hold(laddr);
+
+	write_lock_bh(&__ip_vs_svc_lock);
+
+	/*
+	 * Wait until all other svc users go away.
+	 */
+	IP_VS_WAIT_WHILE(atomic_read(&svc->usecnt) > 1);
+
+	list_add_tail(&laddr->n_list, &svc->laddr_list);
+	svc->num_laddrs++;
+
+#ifdef CONFIG_IP_VS_DEBUG
+	/* Dump the destinations */
+	IP_VS_DBG_BUF(0, "		svc %s:%d num %d curr %p \n",
+		      IP_VS_DBG_ADDR(svc->af, &svc->addr),
+		      ntohs(svc->port), svc->num_laddrs, svc->curr_laddr);
+	list_for_each_entry(laddr, &svc->laddr_list, n_list) {
+		IP_VS_DBG_BUF(0, "		laddr %p %s:%d \n",
+			      laddr, IP_VS_DBG_ADDR(svc->af, &laddr->addr), 0);
+	}
+#endif
+
+	write_unlock_bh(&__ip_vs_svc_lock);
+
+	return 0;
+}
+
+static int
+ip_vs_del_laddr(struct ip_vs_service *svc, struct ip_vs_laddr_user_kern *uladdr)
+{
+	struct ip_vs_laddr *laddr;
+
+	IP_VS_DBG_BUF(0, "vip %s:%d del local address %s\n",
+		      IP_VS_DBG_ADDR(svc->af, &svc->addr), ntohs(svc->port),
+		      IP_VS_DBG_ADDR(svc->af, &uladdr->addr));
+
+	laddr = ip_vs_lookup_laddr(svc, &uladdr->addr);
+
+	if (laddr == NULL) {
+		IP_VS_DBG(1, "%s(): local address not found!\n", __func__);
+		return -ENOENT;
+	}
+
+	write_lock_bh(&__ip_vs_svc_lock);
+
+	/*
+	 *      Wait until all other svc users go away.
+	 */
+	IP_VS_WAIT_WHILE(atomic_read(&svc->usecnt) > 1);
+
+	/* update svc->curr_laddr */
+	if (svc->curr_laddr == &laddr->n_list)
+		svc->curr_laddr = laddr->n_list.next;
+	/*
+	 *      Unlink dest from the service
+	 */
+	list_del(&laddr->n_list);
+	svc->num_laddrs--;
+
+#ifdef CONFIG_IP_VS_DEBUG
+	IP_VS_DBG_BUF(0, "	svc %s:%d num %d curr %p \n",
+		      IP_VS_DBG_ADDR(svc->af, &svc->addr),
+		      ntohs(svc->port), svc->num_laddrs, svc->curr_laddr);
+	list_for_each_entry(laddr, &svc->laddr_list, n_list) {
+		IP_VS_DBG_BUF(0, "		laddr %p %s:%d \n",
+			      laddr, IP_VS_DBG_ADDR(svc->af, &laddr->addr), 0);
+	}
+#endif
+
+	ip_vs_laddr_put(laddr);
+
+	write_unlock_bh(&__ip_vs_svc_lock);
+
+	return 0;
+}
 
 /*
  *	Add a service into the service hash table
@@ -1168,7 +1435,6 @@ ip_vs_add_service(struct ip_vs_service_u
 		ret = -ENOENT;
 		goto out_mod_dec;
 	}
-
 #ifdef CONFIG_IP_VS_IPV6
 	if (u->af == AF_INET6 && (u->netmask < 1 || u->netmask > 128)) {
 		ret = -EINVAL;
@@ -1196,6 +1462,12 @@ ip_vs_add_service(struct ip_vs_service_u
 	svc->timeout = u->timeout * HZ;
 	svc->netmask = u->netmask;
 
+	/* Init the local address stuff */
+	rwlock_init(&svc->laddr_lock);
+	INIT_LIST_HEAD(&svc->laddr_list);
+	svc->num_laddrs = 0;
+	svc->curr_laddr = &svc->laddr_list;
+
 	INIT_LIST_HEAD(&svc->destinations);
 	rwlock_init(&svc->sched_lock);
 	spin_lock_init(&svc->stats.lock);
@@ -1226,7 +1498,7 @@ ip_vs_add_service(struct ip_vs_service_u
 	*svc_p = svc;
 	return 0;
 
-  out_err:
+      out_err:
 	if (svc != NULL) {
 		if (svc->scheduler)
 			ip_vs_unbind_scheduler(svc);
@@ -1239,14 +1511,13 @@ ip_vs_add_service(struct ip_vs_service_u
 	}
 	ip_vs_scheduler_put(sched);
 
-  out_mod_dec:
+      out_mod_dec:
 	/* decrease the module use count */
 	ip_vs_use_count_dec();
 
 	return ret;
 }
 
-
 /*
  *	Edit a service and bind it with a new scheduler
  */
@@ -1317,10 +1588,10 @@ ip_vs_edit_service(struct ip_vs_service 
 		}
 	}
 
-  out_unlock:
+      out_unlock:
 	write_unlock_bh(&__ip_vs_svc_lock);
 #ifdef CONFIG_IP_VS_IPV6
-  out:
+      out:
 #endif
 
 	if (old_sched)
@@ -1329,7 +1600,6 @@ ip_vs_edit_service(struct ip_vs_service 
 	return ret;
 }
 
-
 /*
  *	Delete a service from the service list
  *	- The service must be unlinked, unlocked and not referenced!
@@ -1338,6 +1608,7 @@ ip_vs_edit_service(struct ip_vs_service 
 static void __ip_vs_del_service(struct ip_vs_service *svc)
 {
 	struct ip_vs_dest *dest, *nxt;
+	struct ip_vs_laddr *laddr, *laddr_next;
 	struct ip_vs_scheduler *old_sched;
 
 	/* Count only IPv4 services for old get/setsockopt interface */
@@ -1358,6 +1629,12 @@ static void __ip_vs_del_service(struct i
 		svc->inc = NULL;
 	}
 
+	/* Unlink the whole local address list */
+	list_for_each_entry_safe(laddr, laddr_next, &svc->laddr_list, n_list) {
+		list_del(&laddr->n_list);
+		ip_vs_laddr_put(laddr);
+	}
+
 	/*
 	 *    Unlink the whole destination list
 	 */
@@ -1411,7 +1688,6 @@ static int ip_vs_del_service(struct ip_v
 	return 0;
 }
 
-
 /*
  *	Flush all the virtual services
  */
@@ -1423,8 +1699,9 @@ static int ip_vs_flush(void)
 	/*
 	 * Flush the service table hashed by <protocol,addr,port>
 	 */
-	for(idx = 0; idx < IP_VS_SVC_TAB_SIZE; idx++) {
-		list_for_each_entry_safe(svc, nxt, &ip_vs_svc_table[idx], s_list) {
+	for (idx = 0; idx < IP_VS_SVC_TAB_SIZE; idx++) {
+		list_for_each_entry_safe(svc, nxt, &ip_vs_svc_table[idx],
+					 s_list) {
 			write_lock_bh(&__ip_vs_svc_lock);
 			ip_vs_svc_unhash(svc);
 			/*
@@ -1439,7 +1716,7 @@ static int ip_vs_flush(void)
 	/*
 	 * Flush the service table hashed by fwmark
 	 */
-	for(idx = 0; idx < IP_VS_SVC_TAB_SIZE; idx++) {
+	for (idx = 0; idx < IP_VS_SVC_TAB_SIZE; idx++) {
 		list_for_each_entry_safe(svc, nxt,
 					 &ip_vs_svc_fwm_table[idx], f_list) {
 			write_lock_bh(&__ip_vs_svc_lock);
@@ -1456,7 +1733,6 @@ static int ip_vs_flush(void)
 	return 0;
 }
 
-
 /*
  *	Zero counters in a service or all services
  */
@@ -1478,13 +1754,13 @@ static int ip_vs_zero_all(void)
 	int idx;
 	struct ip_vs_service *svc;
 
-	for(idx = 0; idx < IP_VS_SVC_TAB_SIZE; idx++) {
+	for (idx = 0; idx < IP_VS_SVC_TAB_SIZE; idx++) {
 		list_for_each_entry(svc, &ip_vs_svc_table[idx], s_list) {
 			ip_vs_zero_service(svc);
 		}
 	}
 
-	for(idx = 0; idx < IP_VS_SVC_TAB_SIZE; idx++) {
+	for (idx = 0; idx < IP_VS_SVC_TAB_SIZE; idx++) {
 		list_for_each_entry(svc, &ip_vs_svc_fwm_table[idx], f_list) {
 			ip_vs_zero_service(svc);
 		}
@@ -1494,10 +1770,9 @@ static int ip_vs_zero_all(void)
 	return 0;
 }
 
-
 static int
-proc_do_defense_mode(ctl_table *table, int write,
-		     void __user *buffer, size_t *lenp, loff_t *ppos)
+proc_do_defense_mode(ctl_table * table, int write,
+		     void __user * buffer, size_t * lenp, loff_t * ppos)
 {
 	int *valp = table->data;
 	int val = *valp;
@@ -1515,10 +1790,9 @@ proc_do_defense_mode(ctl_table *table, i
 	return rc;
 }
 
-
 static int
-proc_do_sync_threshold(ctl_table *table, int write,
-		       void __user *buffer, size_t *lenp, loff_t *ppos)
+proc_do_sync_threshold(ctl_table * table, int write,
+		       void __user * buffer, size_t * lenp, loff_t * ppos)
 {
 	int *valp = table->data;
 	int val[2];
@@ -1535,189 +1809,437 @@ proc_do_sync_threshold(ctl_table *table,
 	return rc;
 }
 
-
 /*
  *	IPVS sysctl table (under the /proc/sys/net/ipv4/vs/)
  */
 
 static struct ctl_table vs_vars[] = {
 	{
-		.procname	= "amemthresh",
-		.data		= &sysctl_ip_vs_amemthresh,
-		.maxlen		= sizeof(int),
-		.mode		= 0644,
-		.proc_handler	= proc_dointvec,
-	},
+	 .procname = "amemthresh",
+	 .data = &sysctl_ip_vs_amemthresh,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = proc_dointvec,
+	 },
 #ifdef CONFIG_IP_VS_DEBUG
 	{
-		.procname	= "debug_level",
-		.data		= &sysctl_ip_vs_debug_level,
-		.maxlen		= sizeof(int),
-		.mode		= 0644,
-		.proc_handler	= proc_dointvec,
-	},
-#endif
-	{
-		.procname	= "am_droprate",
-		.data		= &sysctl_ip_vs_am_droprate,
-		.maxlen		= sizeof(int),
-		.mode		= 0644,
-		.proc_handler	= proc_dointvec,
-	},
-	{
-		.procname	= "drop_entry",
-		.data		= &sysctl_ip_vs_drop_entry,
-		.maxlen		= sizeof(int),
-		.mode		= 0644,
-		.proc_handler	= proc_do_defense_mode,
-	},
-	{
-		.procname	= "drop_packet",
-		.data		= &sysctl_ip_vs_drop_packet,
-		.maxlen		= sizeof(int),
-		.mode		= 0644,
-		.proc_handler	= proc_do_defense_mode,
-	},
-	{
-		.procname	= "secure_tcp",
-		.data		= &sysctl_ip_vs_secure_tcp,
-		.maxlen		= sizeof(int),
-		.mode		= 0644,
-		.proc_handler	= proc_do_defense_mode,
-	},
-#if 0
-	{
-		.procname	= "timeout_established",
-		.data	= &vs_timeout_table_dos.timeout[IP_VS_S_ESTABLISHED],
-		.maxlen		= sizeof(int),
-		.mode		= 0644,
-		.proc_handler	= proc_dointvec_jiffies,
-	},
-	{
-		.procname	= "timeout_synsent",
-		.data	= &vs_timeout_table_dos.timeout[IP_VS_S_SYN_SENT],
-		.maxlen		= sizeof(int),
-		.mode		= 0644,
-		.proc_handler	= proc_dointvec_jiffies,
-	},
-	{
-		.procname	= "timeout_synrecv",
-		.data	= &vs_timeout_table_dos.timeout[IP_VS_S_SYN_RECV],
-		.maxlen		= sizeof(int),
-		.mode		= 0644,
-		.proc_handler	= proc_dointvec_jiffies,
-	},
-	{
-		.procname	= "timeout_finwait",
-		.data	= &vs_timeout_table_dos.timeout[IP_VS_S_FIN_WAIT],
-		.maxlen		= sizeof(int),
-		.mode		= 0644,
-		.proc_handler	= proc_dointvec_jiffies,
-	},
-	{
-		.procname	= "timeout_timewait",
-		.data	= &vs_timeout_table_dos.timeout[IP_VS_S_TIME_WAIT],
-		.maxlen		= sizeof(int),
-		.mode		= 0644,
-		.proc_handler	= proc_dointvec_jiffies,
-	},
-	{
-		.procname	= "timeout_close",
-		.data	= &vs_timeout_table_dos.timeout[IP_VS_S_CLOSE],
-		.maxlen		= sizeof(int),
-		.mode		= 0644,
-		.proc_handler	= proc_dointvec_jiffies,
-	},
-	{
-		.procname	= "timeout_closewait",
-		.data	= &vs_timeout_table_dos.timeout[IP_VS_S_CLOSE_WAIT],
-		.maxlen		= sizeof(int),
-		.mode		= 0644,
-		.proc_handler	= proc_dointvec_jiffies,
-	},
-	{
-		.procname	= "timeout_lastack",
-		.data	= &vs_timeout_table_dos.timeout[IP_VS_S_LAST_ACK],
-		.maxlen		= sizeof(int),
-		.mode		= 0644,
-		.proc_handler	= proc_dointvec_jiffies,
-	},
-	{
-		.procname	= "timeout_listen",
-		.data	= &vs_timeout_table_dos.timeout[IP_VS_S_LISTEN],
-		.maxlen		= sizeof(int),
-		.mode		= 0644,
-		.proc_handler	= proc_dointvec_jiffies,
-	},
-	{
-		.procname	= "timeout_synack",
-		.data	= &vs_timeout_table_dos.timeout[IP_VS_S_SYNACK],
-		.maxlen		= sizeof(int),
-		.mode		= 0644,
-		.proc_handler	= proc_dointvec_jiffies,
-	},
-	{
-		.procname	= "timeout_udp",
-		.data	= &vs_timeout_table_dos.timeout[IP_VS_S_UDP],
-		.maxlen		= sizeof(int),
-		.mode		= 0644,
-		.proc_handler	= proc_dointvec_jiffies,
-	},
-	{
-		.procname	= "timeout_icmp",
-		.data	= &vs_timeout_table_dos.timeout[IP_VS_S_ICMP],
-		.maxlen		= sizeof(int),
-		.mode		= 0644,
-		.proc_handler	= proc_dointvec_jiffies,
-	},
-#endif
-	{
-		.procname	= "cache_bypass",
-		.data		= &sysctl_ip_vs_cache_bypass,
-		.maxlen		= sizeof(int),
-		.mode		= 0644,
-		.proc_handler	= proc_dointvec,
-	},
-	{
-		.procname	= "expire_nodest_conn",
-		.data		= &sysctl_ip_vs_expire_nodest_conn,
-		.maxlen		= sizeof(int),
-		.mode		= 0644,
-		.proc_handler	= proc_dointvec,
-	},
-	{
-		.procname	= "expire_quiescent_template",
-		.data		= &sysctl_ip_vs_expire_quiescent_template,
-		.maxlen		= sizeof(int),
-		.mode		= 0644,
-		.proc_handler	= proc_dointvec,
-	},
-	{
-		.procname	= "sync_threshold",
-		.data		= &sysctl_ip_vs_sync_threshold,
-		.maxlen		= sizeof(sysctl_ip_vs_sync_threshold),
-		.mode		= 0644,
-		.proc_handler	= proc_do_sync_threshold,
-	},
-	{
-		.procname	= "nat_icmp_send",
-		.data		= &sysctl_ip_vs_nat_icmp_send,
-		.maxlen		= sizeof(int),
-		.mode		= 0644,
-		.proc_handler	= proc_dointvec,
-	},
-	{ .ctl_name = 0 }
+	 .procname = "debug_level",
+	 .data = &sysctl_ip_vs_debug_level,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = proc_dointvec,
+	 },
+#endif
+	{
+	 .procname = "am_droprate",
+	 .data = &sysctl_ip_vs_am_droprate,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = proc_dointvec,
+	 },
+	{
+	 .procname = "drop_entry",
+	 .data = &sysctl_ip_vs_drop_entry,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = proc_do_defense_mode,
+	 },
+	{
+	 .procname = "drop_packet",
+	 .data = &sysctl_ip_vs_drop_packet,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = proc_do_defense_mode,
+	 },
+	{
+	 .procname = "secure_tcp",
+	 .data = &sysctl_ip_vs_secure_tcp,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = proc_do_defense_mode,
+	 },
+	{
+	 .procname = "timeout_established",
+	 .data = &sysctl_ip_vs_tcp_timeouts[IP_VS_TCP_S_ESTABLISHED],
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = proc_dointvec_jiffies,
+	 },
+	{
+	 .procname = "timeout_synsent",
+	 .data = &sysctl_ip_vs_tcp_timeouts[IP_VS_TCP_S_SYN_SENT],
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = proc_dointvec_jiffies,
+	 },
+	{
+	 .procname = "timeout_synrecv",
+	 .data = &sysctl_ip_vs_tcp_timeouts[IP_VS_TCP_S_SYN_RECV],
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = proc_dointvec_jiffies,
+	 },
+	{
+	 .procname = "timeout_finwait",
+	 .data = &sysctl_ip_vs_tcp_timeouts[IP_VS_TCP_S_FIN_WAIT],
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = proc_dointvec_jiffies,
+	 },
+	{
+	 .procname = "timeout_timewait",
+	 .data = &sysctl_ip_vs_tcp_timeouts[IP_VS_TCP_S_TIME_WAIT],
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = proc_dointvec_jiffies,
+	 },
+	{
+	 .procname = "timeout_close",
+	 .data = &sysctl_ip_vs_tcp_timeouts[IP_VS_TCP_S_CLOSE],
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = proc_dointvec_jiffies,
+	 },
+	{
+	 .procname = "timeout_closewait",
+	 .data = &sysctl_ip_vs_tcp_timeouts[IP_VS_TCP_S_CLOSE_WAIT],
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = proc_dointvec_jiffies,
+	 },
+	{
+	 .procname = "timeout_lastack",
+	 .data = &sysctl_ip_vs_tcp_timeouts[IP_VS_TCP_S_LAST_ACK],
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = proc_dointvec_jiffies,
+	 },
+	{
+	 .procname = "timeout_listen",
+	 .data = &sysctl_ip_vs_tcp_timeouts[IP_VS_TCP_S_LISTEN],
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = proc_dointvec_jiffies,
+	 },
+	{
+	 .procname = "timeout_synack",
+	 .data = &sysctl_ip_vs_tcp_timeouts[IP_VS_TCP_S_SYNACK],
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = proc_dointvec_jiffies,
+	 },
+	{
+	 .procname = "cache_bypass",
+	 .data = &sysctl_ip_vs_cache_bypass,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = proc_dointvec,
+	 },
+	{
+	 .procname = "expire_nodest_conn",
+	 .data = &sysctl_ip_vs_expire_nodest_conn,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = proc_dointvec,
+	 },
+	{
+	 .procname = "expire_quiescent_template",
+	 .data = &sysctl_ip_vs_expire_quiescent_template,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = proc_dointvec,
+	 },
+	{
+	 .procname = "sync_threshold",
+	 .data = &sysctl_ip_vs_sync_threshold,
+	 .maxlen = sizeof(sysctl_ip_vs_sync_threshold),
+	 .mode = 0644,
+	 .proc_handler = proc_do_sync_threshold,
+	 }
+	,
+	{
+	 .procname = "nat_icmp_send",
+	 .data = &sysctl_ip_vs_nat_icmp_send,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = proc_dointvec,
+	 },
+	{
+	 .procname = "fullnat_timestamp_remove_entry",
+	 .data = &sysctl_ip_vs_timestamp_remove_entry,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = &proc_dointvec_minmax,
+	 .strategy = &sysctl_intvec,
+	 .extra1 = &ip_vs_entry_min,
+	 .extra2 = &ip_vs_entry_max,
+	 },
+	{
+	 .procname = "fullnat_mss_adjust_entry",
+	 .data = &sysctl_ip_vs_mss_adjust_entry,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = &proc_dointvec_minmax,
+	 .strategy = &sysctl_intvec,
+	 .extra1 = &ip_vs_entry_min,
+	 .extra2 = &ip_vs_entry_max,
+	 },
+	{
+	 .procname = "fullnat_conn_reused_entry",
+	 .data = &sysctl_ip_vs_conn_reused_entry,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = &proc_dointvec_minmax,
+	 .strategy = &sysctl_intvec,
+	 .extra1 = &ip_vs_entry_min,
+	 .extra2 = &ip_vs_entry_max,
+	 },
+	{
+	 .procname = "fullnat_toa_entry",
+	 .data = &sysctl_ip_vs_toa_entry,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = &proc_dointvec_minmax,
+	 .strategy = &sysctl_intvec,
+	 .extra1 = &ip_vs_entry_min,
+	 .extra2 = &ip_vs_entry_max,
+	 },
+	{
+	 .procname = "fullnat_lport_max",
+	 .data = &sysctl_ip_vs_lport_max,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = &proc_dointvec_minmax,
+	 .strategy = &sysctl_intvec,
+	 .extra1 = &ip_vs_port_min,
+	 .extra2 = &ip_vs_port_max,
+	 },
+	{
+	 .procname = "fullnat_lport_min",
+	 .data = &sysctl_ip_vs_lport_min,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = &proc_dointvec_minmax,
+	 .strategy = &sysctl_intvec,
+	 .extra1 = &ip_vs_port_min,
+	 .extra2 = &ip_vs_port_max,
+	 },
+	{
+	 .procname = "fullnat_lport_tries",
+	 .data = &sysctl_ip_vs_lport_tries,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = &proc_dointvec_minmax,
+	 .strategy = &sysctl_intvec,
+	 .extra1 = &ip_vs_port_try_min,
+	 .extra2 = &ip_vs_port_try_max,
+	 },
+	/* syn-proxy sysctl variables */
+	{
+	 .procname = "synproxy_init_mss",
+	 .data = &sysctl_ip_vs_synproxy_init_mss,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = &proc_dointvec_minmax,
+	 .extra1 = &ip_vs_synproxy_init_mss_min,
+	 .extra2 = &ip_vs_synproxy_init_mss_max,
+	 .strategy = &sysctl_intvec,
+	 },
+	{
+	 .procname = "synproxy_sack",
+	 .data = &sysctl_ip_vs_synproxy_sack,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = &proc_dointvec_minmax,
+	 .extra1 = &ip_vs_synproxy_switch_min,
+	 .extra2 = &ip_vs_synproxy_switch_max,
+	 .strategy = &sysctl_intvec,
+	 },
+	{
+	 .procname = "synproxy_wscale",
+	 .data = &sysctl_ip_vs_synproxy_wscale,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = &proc_dointvec_minmax,
+	 .extra1 = &ip_vs_synproxy_wscale_min,
+	 .extra2 = &ip_vs_synproxy_wscale_max,
+	 .strategy = &sysctl_intvec,
+	 },
+	{
+	 .procname = "synproxy_timestamp",
+	 .data = &sysctl_ip_vs_synproxy_timestamp,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = &proc_dointvec_minmax,
+	 .extra1 = &ip_vs_synproxy_switch_min,
+	 .extra2 = &ip_vs_synproxy_switch_max,
+	 .strategy = &sysctl_intvec,
+	 },
+	{
+	 .procname = "synproxy_synack_ttl",
+	 .data = &sysctl_ip_vs_synproxy_synack_ttl,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = &proc_dointvec_minmax,
+	 .extra1 = &ip_vs_synproxy_synack_ttl_min,
+	 .extra2 = &ip_vs_synproxy_synack_ttl_max,
+	 .strategy = &sysctl_intvec,
+	 },
+	{
+	 .procname = "synproxy_defer",
+	 .data = &sysctl_ip_vs_synproxy_defer,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = &proc_dointvec_minmax,
+	 .extra1 = &ip_vs_synproxy_switch_min,
+	 .extra2 = &ip_vs_synproxy_switch_max,
+	 .strategy = &sysctl_intvec,
+	 },
+	{
+	 .procname = "synproxy_conn_reuse",
+	 .data = &sysctl_ip_vs_synproxy_conn_reuse,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = &proc_dointvec_minmax,
+	 .extra1 = &ip_vs_synproxy_switch_min,
+	 .extra2 = &ip_vs_synproxy_switch_max,
+	 .strategy = &sysctl_intvec,
+	 },
+	{
+	 .procname = "synproxy_conn_reuse_close",
+	 .data = &sysctl_ip_vs_synproxy_conn_reuse_cl,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = &proc_dointvec_minmax,
+	 .extra1 = &ip_vs_synproxy_switch_min,
+	 .extra2 = &ip_vs_synproxy_switch_max,
+	 .strategy = &sysctl_intvec,
+	 },
+	{
+	 .procname = "synproxy_conn_reuse_time_wait",
+	 .data = &sysctl_ip_vs_synproxy_conn_reuse_tw,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = &proc_dointvec_minmax,
+	 .extra1 = &ip_vs_synproxy_switch_min,
+	 .extra2 = &ip_vs_synproxy_switch_max,
+	 .strategy = &sysctl_intvec,
+	 },
+	{
+	 .procname = "synproxy_conn_reuse_fin_wait",
+	 .data = &sysctl_ip_vs_synproxy_conn_reuse_fw,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = &proc_dointvec_minmax,
+	 .extra1 = &ip_vs_synproxy_switch_min,
+	 .extra2 = &ip_vs_synproxy_switch_max,
+	 .strategy = &sysctl_intvec,
+	 },
+	{
+	 .procname = "synproxy_conn_reuse_close_wait",
+	 .data = &sysctl_ip_vs_synproxy_conn_reuse_cw,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = &proc_dointvec_minmax,
+	 .extra1 = &ip_vs_synproxy_switch_min,
+	 .extra2 = &ip_vs_synproxy_switch_max,
+	 .strategy = &sysctl_intvec,
+	 },
+	{
+	 .procname = "synproxy_conn_reuse_last_ack",
+	 .data = &sysctl_ip_vs_synproxy_conn_reuse_la,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = &proc_dointvec_minmax,
+	 .extra1 = &ip_vs_synproxy_switch_min,
+	 .extra2 = &ip_vs_synproxy_switch_max,
+	 .strategy = &sysctl_intvec,
+	 },
+	{
+	 .procname = "synproxy_ack_skb_store_thresh",
+	 .data = &sysctl_ip_vs_synproxy_skb_store_thresh,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = &proc_dointvec_minmax,
+	 .extra1 = &ip_vs_synproxy_skb_store_thresh_min,
+	 .extra2 = &ip_vs_synproxy_skb_store_thresh_max,
+	 .strategy = &sysctl_intvec,
+	 },
+	{
+	 .procname = "synproxy_ack_storm_thresh",
+	 .data = &sysctl_ip_vs_synproxy_dup_ack_thresh,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = &proc_dointvec_minmax,
+	 .extra1 = &ip_vs_synproxy_dup_ack_cnt_min,
+	 .extra2 = &ip_vs_synproxy_dup_ack_cnt_max,
+	 .strategy = &sysctl_intvec,
+	 },
+	{
+	 .procname = "synproxy_syn_retry",
+	 .data = &sysctl_ip_vs_synproxy_syn_retry,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = &proc_dointvec_minmax,
+	 .extra1 = &ip_vs_synproxy_syn_retry_min,
+	 .extra2 = &ip_vs_synproxy_syn_retry_max,
+	 .strategy = &sysctl_intvec,
+	 },
+	/* attack-defence sysctl variables */
+	{
+	 .procname = "defence_tcp_drop",
+	 .data = &sysctl_ip_vs_tcp_drop_entry,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = &proc_dointvec_minmax,
+	 .strategy = &sysctl_intvec,
+	 .extra1 = &ip_vs_entry_min,
+	 .extra2 = &ip_vs_entry_max,
+	 },
+	{
+	 .procname = "defence_udp_drop",
+	 .data = &sysctl_ip_vs_udp_drop_entry,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = &proc_dointvec_minmax,
+	 .strategy = &sysctl_intvec,
+	 .extra1 = &ip_vs_entry_min,
+	 .extra2 = &ip_vs_entry_max,
+	 },
+	{
+	 .procname = "defence_frag_drop",
+	 .data = &sysctl_ip_vs_frag_drop_entry,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = &proc_dointvec_minmax,
+	 .strategy = &sysctl_intvec,
+	 .extra1 = &ip_vs_entry_min,
+	 .extra2 = &ip_vs_entry_max,
+	 },
+	/* send rst sysctl variables */
+	{
+	 .procname = "conn_expire_tcp_rst",
+	 .data = &sysctl_ip_vs_conn_expire_tcp_rst,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = &proc_dointvec_minmax,
+	 .strategy = &sysctl_intvec,
+	 .extra1 = &ip_vs_entry_min,	/* zero */
+	 .extra2 = &ip_vs_entry_max,	/* one */
+	 },
+	{.ctl_name = 0}
 };
 
 const struct ctl_path net_vs_ctl_path[] = {
-	{ .procname = "net", .ctl_name = CTL_NET, },
-	{ .procname = "ipv4", .ctl_name = NET_IPV4, },
-	{ .procname = "vs", },
-	{ }
+	{.procname = "net",.ctl_name = CTL_NET,},
+	{.procname = "ipv4",.ctl_name = NET_IPV4,},
+	{.procname = "vs",},
+	{}
 };
+
 EXPORT_SYMBOL_GPL(net_vs_ctl_path);
 
-static struct ctl_table_header * sysctl_header;
+static struct ctl_table_header *sysctl_header;
 
 #ifdef CONFIG_PROC_FS
 
@@ -1739,12 +2261,13 @@ static inline const char *ip_vs_fwd_name
 		return "Tunnel";
 	case IP_VS_CONN_F_DROUTE:
 		return "Route";
+	case IP_VS_CONN_F_FULLNAT:
+		return "FullNat";
 	default:
 		return "Masq";
 	}
 }
 
-
 /* Get the Nth entry in the two lists */
 static struct ip_vs_service *ip_vs_info_array(struct seq_file *seq, loff_t pos)
 {
@@ -1755,7 +2278,7 @@ static struct ip_vs_service *ip_vs_info_
 	/* look in hash by protocol */
 	for (idx = 0; idx < IP_VS_SVC_TAB_SIZE; idx++) {
 		list_for_each_entry(svc, &ip_vs_svc_table[idx], s_list) {
-			if (pos-- == 0){
+			if (pos-- == 0) {
 				iter->table = ip_vs_svc_table;
 				iter->bucket = idx;
 				return svc;
@@ -1777,7 +2300,7 @@ static struct ip_vs_service *ip_vs_info_
 	return NULL;
 }
 
-static void *ip_vs_info_seq_start(struct seq_file *seq, loff_t *pos)
+static void *ip_vs_info_seq_start(struct seq_file *seq, loff_t * pos)
 __acquires(__ip_vs_svc_lock)
 {
 
@@ -1785,8 +2308,7 @@ __acquires(__ip_vs_svc_lock)
 	return *pos ? ip_vs_info_array(seq, *pos - 1) : SEQ_START_TOKEN;
 }
 
-
-static void *ip_vs_info_seq_next(struct seq_file *seq, void *v, loff_t *pos)
+static void *ip_vs_info_seq_next(struct seq_file *seq, void *v, loff_t * pos)
 {
 	struct list_head *e;
 	struct ip_vs_iter *iter;
@@ -1794,7 +2316,7 @@ static void *ip_vs_info_seq_next(struct 
 
 	++*pos;
 	if (v == SEQ_START_TOKEN)
-		return ip_vs_info_array(seq,0);
+		return ip_vs_info_array(seq, 0);
 
 	svc = v;
 	iter = seq->private;
@@ -1804,9 +2326,8 @@ static void *ip_vs_info_seq_next(struct 
 		if ((e = svc->s_list.next) != &ip_vs_svc_table[iter->bucket])
 			return list_entry(e, struct ip_vs_service, s_list);
 
-
 		while (++iter->bucket < IP_VS_SVC_TAB_SIZE) {
-			list_for_each_entry(svc,&ip_vs_svc_table[iter->bucket],
+			list_for_each_entry(svc, &ip_vs_svc_table[iter->bucket],
 					    s_list) {
 				return svc;
 			}
@@ -1821,11 +2342,11 @@ static void *ip_vs_info_seq_next(struct 
 	if ((e = svc->f_list.next) != &ip_vs_svc_fwm_table[iter->bucket])
 		return list_entry(e, struct ip_vs_service, f_list);
 
- scan_fwmark:
+      scan_fwmark:
 	while (++iter->bucket < IP_VS_SVC_TAB_SIZE) {
 		list_for_each_entry(svc, &ip_vs_svc_fwm_table[iter->bucket],
 				    f_list)
-			return svc;
+		    return svc;
 	}
 
 	return NULL;
@@ -1837,15 +2358,13 @@ __releases(__ip_vs_svc_lock)
 	read_unlock_bh(&__ip_vs_svc_lock);
 }
 
-
 static int ip_vs_info_seq_show(struct seq_file *seq, void *v)
 {
 	if (v == SEQ_START_TOKEN) {
 		seq_printf(seq,
-			"IP Virtual Server version %d.%d.%d (size=%d)\n",
-			NVERSION(IP_VS_VERSION_CODE), IP_VS_CONN_TAB_SIZE);
-		seq_puts(seq,
-			 "Prot LocalAddress:Port Scheduler Flags\n");
+			   "IP Virtual Server version %d.%d.%d (size=%d)\n",
+			   NVERSION(IP_VS_VERSION_CODE), IP_VS_CONN_TAB_SIZE);
+		seq_puts(seq, "Prot LocalAddress:Port Scheduler Flags\n");
 		seq_puts(seq,
 			 "  -> RemoteAddress:Port Forward Weight ActiveConn InActConn\n");
 	} else {
@@ -1861,8 +2380,9 @@ static int ip_vs_info_seq_show(struct se
 					   &svc->addr.in6,
 					   ntohs(svc->port),
 					   svc->scheduler->name,
-					   (svc->flags & IP_VS_SVC_F_ONEPACKET)?
-					   " ops":"");
+					   (svc->
+					    flags & IP_VS_SVC_F_ONEPACKET) ?
+					   " ops" : "");
 			else
 #endif
 				seq_printf(seq, "%s  %08X:%04X %s%s ",
@@ -1870,19 +2390,19 @@ static int ip_vs_info_seq_show(struct se
 					   ntohl(svc->addr.ip),
 					   ntohs(svc->port),
 					   svc->scheduler->name,
-					   (svc->flags & IP_VS_SVC_F_ONEPACKET)?
-					   " ops":"");
+					   (svc->
+					    flags & IP_VS_SVC_F_ONEPACKET) ?
+					   " ops" : "");
 		} else {
 			seq_printf(seq, "FWM  %08X %s%s ",
 				   svc->fwmark, svc->scheduler->name,
-				   (svc->flags & IP_VS_SVC_F_ONEPACKET)?
-				   " ops":"");
+				   (svc->flags & IP_VS_SVC_F_ONEPACKET) ?
+				   " ops" : "");
 		}
 
 		if (svc->flags & IP_VS_SVC_F_PERSISTENT)
 			seq_printf(seq, "persistent %d %08X\n",
-				svc->timeout,
-				ntohl(svc->netmask));
+				   svc->timeout, ntohl(svc->netmask));
 		else
 			seq_putc(seq, '\n');
 
@@ -1894,7 +2414,8 @@ static int ip_vs_info_seq_show(struct se
 					   "      %-7s %-6d %-10d %-10d\n",
 					   &dest->addr.in6,
 					   ntohs(dest->port),
-					   ip_vs_fwd_name(atomic_read(&dest->conn_flags)),
+					   ip_vs_fwd_name(atomic_read
+							  (&dest->conn_flags)),
 					   atomic_read(&dest->weight),
 					   atomic_read(&dest->activeconns),
 					   atomic_read(&dest->inactconns));
@@ -1905,7 +2426,8 @@ static int ip_vs_info_seq_show(struct se
 					   "%-7s %-6d %-10d %-10d\n",
 					   ntohl(dest->addr.ip),
 					   ntohs(dest->port),
-					   ip_vs_fwd_name(atomic_read(&dest->conn_flags)),
+					   ip_vs_fwd_name(atomic_read
+							  (&dest->conn_flags)),
 					   atomic_read(&dest->weight),
 					   atomic_read(&dest->activeconns),
 					   atomic_read(&dest->inactconns));
@@ -1917,22 +2439,22 @@ static int ip_vs_info_seq_show(struct se
 
 static const struct seq_operations ip_vs_info_seq_ops = {
 	.start = ip_vs_info_seq_start,
-	.next  = ip_vs_info_seq_next,
-	.stop  = ip_vs_info_seq_stop,
-	.show  = ip_vs_info_seq_show,
+	.next = ip_vs_info_seq_next,
+	.stop = ip_vs_info_seq_stop,
+	.show = ip_vs_info_seq_show,
 };
 
 static int ip_vs_info_open(struct inode *inode, struct file *file)
 {
 	return seq_open_private(file, &ip_vs_info_seq_ops,
-			sizeof(struct ip_vs_iter));
+				sizeof(struct ip_vs_iter));
 }
 
 static const struct file_operations ip_vs_info_fops = {
-	.owner	 = THIS_MODULE,
-	.open    = ip_vs_info_open,
-	.read    = seq_read,
-	.llseek  = seq_lseek,
+	.owner = THIS_MODULE,
+	.open = ip_vs_info_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
 	.release = seq_release_private,
 };
 
@@ -1953,20 +2475,20 @@ static int ip_vs_stats_show(struct seq_f
 		   "   Conns  Packets  Packets            Bytes            Bytes\n");
 
 	spin_lock_bh(&ip_vs_stats.lock);
-	seq_printf(seq, "%8X %8X %8X %16LX %16LX\n\n", ip_vs_stats.ustats.conns,
-		   ip_vs_stats.ustats.inpkts, ip_vs_stats.ustats.outpkts,
-		   (unsigned long long) ip_vs_stats.ustats.inbytes,
-		   (unsigned long long) ip_vs_stats.ustats.outbytes);
+	seq_printf(seq, "%16LX %16LX %16LX %16LX %16LX\n\n",
+		   ip_vs_stats.ustats.conns, ip_vs_stats.ustats.inpkts,
+		   ip_vs_stats.ustats.outpkts,
+		   (unsigned long long)ip_vs_stats.ustats.inbytes,
+		   (unsigned long long)ip_vs_stats.ustats.outbytes);
 
 /*                 01234567 01234567 01234567 0123456701234567 0123456701234567 */
 	seq_puts(seq,
-		   " Conns/s   Pkts/s   Pkts/s          Bytes/s          Bytes/s\n");
-	seq_printf(seq,"%8X %8X %8X %16X %16X\n",
-			ip_vs_stats.ustats.cps,
-			ip_vs_stats.ustats.inpps,
-			ip_vs_stats.ustats.outpps,
-			ip_vs_stats.ustats.inbps,
-			ip_vs_stats.ustats.outbps);
+		 " Conns/s   Pkts/s   Pkts/s          Bytes/s          Bytes/s\n");
+	seq_printf(seq, "%8X %8X %8X %16X %16X\n",
+		   ip_vs_stats.ustats.cps,
+		   ip_vs_stats.ustats.inpps,
+		   ip_vs_stats.ustats.outpps,
+		   ip_vs_stats.ustats.inbps, ip_vs_stats.ustats.outbps);
 	spin_unlock_bh(&ip_vs_stats.lock);
 
 	return 0;
@@ -1987,90 +2509,191 @@ static const struct file_operations ip_v
 
 #endif
 
+#ifdef CONFIG_PROC_FS
+/*
+ * Statistics for FULLNAT and SYNPROXY
+ * in /proc/net/ip_vs_ext_stats
+ */
+
+struct ip_vs_estats_mib *ip_vs_esmib;
+
+static struct ip_vs_estats_entry ext_stats[] = {
+	IP_VS_ESTATS_ITEM("fullnat_add_toa_ok", FULLNAT_ADD_TOA_OK),
+	IP_VS_ESTATS_ITEM("fullnat_add_toa_fail_len", FULLNAT_ADD_TOA_FAIL_LEN),
+	IP_VS_ESTATS_ITEM("fullnat_add_toa_fail_mem", FULLNAT_ADD_TOA_FAIL_MEM),
+	IP_VS_ESTATS_ITEM("fullnat_add_toa_fail_proto",
+			  FULLNAT_ADD_TOA_FAIL_PROTO),
+	IP_VS_ESTATS_ITEM("fullnat_conn_reused", FULLNAT_CONN_REUSED),
+	IP_VS_ESTATS_ITEM("fullnat_conn_reused_close",
+			  FULLNAT_CONN_REUSED_CLOSE),
+	IP_VS_ESTATS_ITEM("fullnat_conn_reused_timewait",
+			  FULLNAT_CONN_REUSED_TIMEWAIT),
+	IP_VS_ESTATS_ITEM("fullnat_conn_reused_finwait",
+			  FULLNAT_CONN_REUSED_FINWAIT),
+	IP_VS_ESTATS_ITEM("fullnat_conn_reused_closewait",
+			  FULLNAT_CONN_REUSED_CLOSEWAIT),
+	IP_VS_ESTATS_ITEM("fullnat_conn_reused_lastack",
+			  FULLNAT_CONN_REUSED_LASTACK),
+	IP_VS_ESTATS_ITEM("fullnat_conn_reused_estab",
+			  FULLNAT_CONN_REUSED_ESTAB),
+	IP_VS_ESTATS_ITEM("synproxy_rs_error", SYNPROXY_RS_ERROR),
+	IP_VS_ESTATS_ITEM("synproxy_null_ack", SYNPROXY_NULL_ACK),
+	IP_VS_ESTATS_ITEM("synproxy_bad_ack", SYNPROXY_BAD_ACK),
+	IP_VS_ESTATS_ITEM("synproxy_ok_ack", SYNPROXY_OK_ACK),
+	IP_VS_ESTATS_ITEM("synproxy_syn_cnt", SYNPROXY_SYN_CNT),
+	IP_VS_ESTATS_ITEM("synproxy_ackstorm", SYNPROXY_ACK_STORM),
+	IP_VS_ESTATS_ITEM("synproxy_synsend_qlen", SYNPROXY_SYNSEND_QLEN),
+	IP_VS_ESTATS_ITEM("synproxy_conn_reused", SYNPROXY_CONN_REUSED),
+	IP_VS_ESTATS_ITEM("synproxy_conn_reused_close",
+			  SYNPROXY_CONN_REUSED_CLOSE),
+	IP_VS_ESTATS_ITEM("synproxy_conn_reused_timewait",
+			  SYNPROXY_CONN_REUSED_TIMEWAIT),
+	IP_VS_ESTATS_ITEM("synproxy_conn_reused_finwait",
+			  SYNPROXY_CONN_REUSED_FINWAIT),
+	IP_VS_ESTATS_ITEM("synproxy_conn_reused_closewait",
+			  SYNPROXY_CONN_REUSED_CLOSEWAIT),
+	IP_VS_ESTATS_ITEM("synproxy_conn_reused_lastack",
+			  SYNPROXY_CONN_REUSED_LASTACK),
+	IP_VS_ESTATS_ITEM("defence_ip_frag_drop", DEFENCE_IP_FRAG_DROP),
+	IP_VS_ESTATS_ITEM("defence_tcp_drop", DEFENCE_TCP_DROP),
+	IP_VS_ESTATS_ITEM("defence_udp_drop", DEFENCE_UDP_DROP),
+	IP_VS_ESTATS_LAST
+};
+
+static int ip_vs_estats_show(struct seq_file *seq, void *v)
+{
+	int i, j;
+
+	/* print CPU first */
+	seq_printf(seq, "                                  ");
+	for (i = 0; i < NR_CPUS; i++)
+		if (cpu_online(i))
+			seq_printf(seq, "CPU%d       ", i);
+	seq_putc(seq, '\n');
+
+	i = 0;
+	while (NULL != ext_stats[i].name) {
+		seq_printf(seq, "%-25s:", ext_stats[i].name);
+		for (j = 0; j < NR_CPUS; j++) {
+			if (cpu_online(j)) {
+				seq_printf(seq, "%10lu ",
+					   *(((unsigned long *)
+					      per_cpu_ptr(ip_vs_esmib,
+							  j)) +
+					     ext_stats[i].entry));
+			}
+		}
+		seq_putc(seq, '\n');
+		i++;
+	}
+	return 0;
+}
+
+static int ip_vs_estats_seq_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, ip_vs_estats_show, NULL);
+}
+
+static const struct file_operations ip_vs_estats_fops = {
+	.owner = THIS_MODULE,
+	.open = ip_vs_estats_seq_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.release = single_release,
+};
+#endif
+
 /*
  *	Set timeout values for tcp tcpfin udp in the timeout_table.
  */
 static int ip_vs_set_timeout(struct ip_vs_timeout_user *u)
 {
 	IP_VS_DBG(2, "Setting timeout tcp:%d tcpfin:%d udp:%d\n",
-		  u->tcp_timeout,
-		  u->tcp_fin_timeout,
-		  u->udp_timeout);
+		  u->tcp_timeout, u->tcp_fin_timeout, u->udp_timeout);
 
 #ifdef CONFIG_IP_VS_PROTO_TCP
 	if (u->tcp_timeout) {
 		ip_vs_protocol_tcp.timeout_table[IP_VS_TCP_S_ESTABLISHED]
-			= u->tcp_timeout * HZ;
+		    = u->tcp_timeout * HZ;
 	}
 
 	if (u->tcp_fin_timeout) {
 		ip_vs_protocol_tcp.timeout_table[IP_VS_TCP_S_FIN_WAIT]
-			= u->tcp_fin_timeout * HZ;
+		    = u->tcp_fin_timeout * HZ;
 	}
 #endif
 
 #ifdef CONFIG_IP_VS_PROTO_UDP
 	if (u->udp_timeout) {
 		ip_vs_protocol_udp.timeout_table[IP_VS_UDP_S_NORMAL]
-			= u->udp_timeout * HZ;
+		    = u->udp_timeout * HZ;
 	}
 #endif
 	return 0;
 }
 
-
 #define SET_CMDID(cmd)		(cmd - IP_VS_BASE_CTL)
 #define SERVICE_ARG_LEN		(sizeof(struct ip_vs_service_user))
 #define SVCDEST_ARG_LEN		(sizeof(struct ip_vs_service_user) +	\
 				 sizeof(struct ip_vs_dest_user))
+#define SVCLADDR_ARG_LEN	(sizeof(struct ip_vs_service_user) +	\
+				 sizeof(struct ip_vs_laddr_user))
 #define TIMEOUT_ARG_LEN		(sizeof(struct ip_vs_timeout_user))
 #define DAEMON_ARG_LEN		(sizeof(struct ip_vs_daemon_user))
 #define MAX_ARG_LEN		SVCDEST_ARG_LEN
 
-static const unsigned char set_arglen[SET_CMDID(IP_VS_SO_SET_MAX)+1] = {
-	[SET_CMDID(IP_VS_SO_SET_ADD)]		= SERVICE_ARG_LEN,
-	[SET_CMDID(IP_VS_SO_SET_EDIT)]		= SERVICE_ARG_LEN,
-	[SET_CMDID(IP_VS_SO_SET_DEL)]		= SERVICE_ARG_LEN,
-	[SET_CMDID(IP_VS_SO_SET_FLUSH)]		= 0,
-	[SET_CMDID(IP_VS_SO_SET_ADDDEST)]	= SVCDEST_ARG_LEN,
-	[SET_CMDID(IP_VS_SO_SET_DELDEST)]	= SVCDEST_ARG_LEN,
-	[SET_CMDID(IP_VS_SO_SET_EDITDEST)]	= SVCDEST_ARG_LEN,
-	[SET_CMDID(IP_VS_SO_SET_TIMEOUT)]	= TIMEOUT_ARG_LEN,
-	[SET_CMDID(IP_VS_SO_SET_STARTDAEMON)]	= DAEMON_ARG_LEN,
-	[SET_CMDID(IP_VS_SO_SET_STOPDAEMON)]	= DAEMON_ARG_LEN,
-	[SET_CMDID(IP_VS_SO_SET_ZERO)]		= SERVICE_ARG_LEN,
+static const unsigned char set_arglen[SET_CMDID(IP_VS_SO_SET_MAX) + 1] = {
+	[SET_CMDID(IP_VS_SO_SET_ADD)] = SERVICE_ARG_LEN,
+	[SET_CMDID(IP_VS_SO_SET_EDIT)] = SERVICE_ARG_LEN,
+	[SET_CMDID(IP_VS_SO_SET_DEL)] = SERVICE_ARG_LEN,
+	[SET_CMDID(IP_VS_SO_SET_FLUSH)] = 0,
+	[SET_CMDID(IP_VS_SO_SET_ADDDEST)] = SVCDEST_ARG_LEN,
+	[SET_CMDID(IP_VS_SO_SET_DELDEST)] = SVCDEST_ARG_LEN,
+	[SET_CMDID(IP_VS_SO_SET_EDITDEST)] = SVCDEST_ARG_LEN,
+	[SET_CMDID(IP_VS_SO_SET_TIMEOUT)] = TIMEOUT_ARG_LEN,
+	[SET_CMDID(IP_VS_SO_SET_STARTDAEMON)] = DAEMON_ARG_LEN,
+	[SET_CMDID(IP_VS_SO_SET_STOPDAEMON)] = DAEMON_ARG_LEN,
+	[SET_CMDID(IP_VS_SO_SET_ZERO)] = SERVICE_ARG_LEN,
+	[SET_CMDID(IP_VS_SO_SET_ADDLADDR)] = SVCLADDR_ARG_LEN,
+	[SET_CMDID(IP_VS_SO_SET_DELLADDR)] = SVCLADDR_ARG_LEN,
 };
 
 static void ip_vs_copy_usvc_compat(struct ip_vs_service_user_kern *usvc,
-				  struct ip_vs_service_user *usvc_compat)
+				   struct ip_vs_service_user *usvc_compat)
 {
-	usvc->af		= AF_INET;
-	usvc->protocol		= usvc_compat->protocol;
-	usvc->addr.ip		= usvc_compat->addr;
-	usvc->port		= usvc_compat->port;
-	usvc->fwmark		= usvc_compat->fwmark;
+	usvc->af = AF_INET;
+	usvc->protocol = usvc_compat->protocol;
+	usvc->addr.ip = usvc_compat->addr;
+	usvc->port = usvc_compat->port;
+	usvc->fwmark = usvc_compat->fwmark;
 
 	/* Deep copy of sched_name is not needed here */
-	usvc->sched_name	= usvc_compat->sched_name;
+	usvc->sched_name = usvc_compat->sched_name;
 
-	usvc->flags		= usvc_compat->flags;
-	usvc->timeout		= usvc_compat->timeout;
-	usvc->netmask		= usvc_compat->netmask;
+	usvc->flags = usvc_compat->flags;
+	usvc->timeout = usvc_compat->timeout;
+	usvc->netmask = usvc_compat->netmask;
 }
 
 static void ip_vs_copy_udest_compat(struct ip_vs_dest_user_kern *udest,
-				   struct ip_vs_dest_user *udest_compat)
+				    struct ip_vs_dest_user *udest_compat)
+{
+	udest->addr.ip = udest_compat->addr;
+	udest->port = udest_compat->port;
+	udest->conn_flags = udest_compat->conn_flags;
+	udest->weight = udest_compat->weight;
+	udest->u_threshold = udest_compat->u_threshold;
+	udest->l_threshold = udest_compat->l_threshold;
+}
+
+static void ip_vs_copy_uladdr_compat(struct ip_vs_laddr_user_kern *uladdr,
+				     struct ip_vs_laddr_user *uladdr_compat)
 {
-	udest->addr.ip		= udest_compat->addr;
-	udest->port		= udest_compat->port;
-	udest->conn_flags	= udest_compat->conn_flags;
-	udest->weight		= udest_compat->weight;
-	udest->u_threshold	= udest_compat->u_threshold;
-	udest->l_threshold	= udest_compat->l_threshold;
+	uladdr->addr.ip = uladdr_compat->addr;
 }
 
 static int
-do_ip_vs_set_ctl(struct sock *sk, int cmd, void __user *user, unsigned int len)
+do_ip_vs_set_ctl(struct sock *sk, int cmd, void __user * user, unsigned int len)
 {
 	int ret;
 	unsigned char arg[MAX_ARG_LEN];
@@ -2079,6 +2702,8 @@ do_ip_vs_set_ctl(struct sock *sk, int cm
 	struct ip_vs_service *svc;
 	struct ip_vs_dest_user *udest_compat;
 	struct ip_vs_dest_user_kern udest;
+	struct ip_vs_laddr_user *uladdr_compat;
+	struct ip_vs_laddr_user_kern uladdr;
 
 	if (!capable(CAP_NET_ADMIN))
 		return -EPERM;
@@ -2120,11 +2745,11 @@ do_ip_vs_set_ctl(struct sock *sk, int cm
 
 	usvc_compat = (struct ip_vs_service_user *)arg;
 	udest_compat = (struct ip_vs_dest_user *)(usvc_compat + 1);
+	uladdr_compat = (struct ip_vs_laddr_user *)(usvc_compat + 1);
 
 	/* We only use the new structs internally, so copy userspace compat
 	 * structs to extended internal versions */
 	ip_vs_copy_usvc_compat(&usvc, usvc_compat);
-	ip_vs_copy_udest_compat(&udest, udest_compat);
 
 	if (cmd == IP_VS_SO_SET_ZERO) {
 		/* if no service address is set, zero counters in all */
@@ -2160,13 +2785,24 @@ do_ip_vs_set_ctl(struct sock *sk, int cm
 	case IP_VS_SO_SET_ADD:
 		if (svc != NULL)
 			ret = -EEXIST;
-		else
+		else{
 			ret = ip_vs_add_service(&usvc, &svc);
+			if(!ret && svc->addr.ip == 0){
+				udest.addr.ip = IP_VS_DSNAT_RS_ADDR;
+				udest.port = IP_VS_DSNAT_RS_PORT;
+				udest.conn_flags = IP_VS_CONN_F_FULLNAT;
+				udest.weight = 0;
+				udest.u_threshold = 0;
+				udest.l_threshold = 0;
+				ret = ip_vs_add_dest(svc, &udest);
+			}
+		}
+			
 		break;
 	case IP_VS_SO_SET_EDIT:
 		ret = ip_vs_edit_service(svc, &usvc);
 		break;
-	case IP_VS_SO_SET_DEL:
+	case IP_VS_SO_SET_DEL:	
 		ret = ip_vs_del_service(svc);
 		if (!ret)
 			goto out_unlock;
@@ -2175,14 +2811,25 @@ do_ip_vs_set_ctl(struct sock *sk, int cm
 		ret = ip_vs_zero_service(svc);
 		break;
 	case IP_VS_SO_SET_ADDDEST:
+		ip_vs_copy_udest_compat(&udest, udest_compat);
 		ret = ip_vs_add_dest(svc, &udest);
 		break;
 	case IP_VS_SO_SET_EDITDEST:
+		ip_vs_copy_udest_compat(&udest, udest_compat);
 		ret = ip_vs_edit_dest(svc, &udest);
 		break;
 	case IP_VS_SO_SET_DELDEST:
+		ip_vs_copy_udest_compat(&udest, udest_compat);
 		ret = ip_vs_del_dest(svc, &udest);
 		break;
+	case IP_VS_SO_SET_ADDLADDR:
+		ip_vs_copy_uladdr_compat(&uladdr, uladdr_compat);
+		ret = ip_vs_add_laddr(svc, &uladdr);
+		break;
+	case IP_VS_SO_SET_DELLADDR:
+		ip_vs_copy_uladdr_compat(&uladdr, uladdr_compat);
+		ret = ip_vs_del_laddr(svc, &uladdr);
+		break;
 	default:
 		ret = -EINVAL;
 	}
@@ -2190,16 +2837,15 @@ do_ip_vs_set_ctl(struct sock *sk, int cm
 	if (svc)
 		ip_vs_service_put(svc);
 
-  out_unlock:
+      out_unlock:
 	mutex_unlock(&__ip_vs_mutex);
-  out_dec:
+      out_dec:
 	/* decrease the module use count */
 	ip_vs_use_count_dec();
 
 	return ret;
 }
 
-
 static void
 ip_vs_copy_stats(struct ip_vs_stats_user *dst, struct ip_vs_stats *src)
 {
@@ -2220,14 +2866,15 @@ ip_vs_copy_service(struct ip_vs_service_
 	dst->timeout = src->timeout / HZ;
 	dst->netmask = src->netmask;
 	dst->num_dests = src->num_dests;
+	dst->num_laddrs = src->num_laddrs;
 	ip_vs_copy_stats(&dst->stats, &src->stats);
 }
 
 static inline int
 __ip_vs_get_service_entries(const struct ip_vs_get_services *get,
-			    struct ip_vs_get_services __user *uptr)
+			    struct ip_vs_get_services __user * uptr)
 {
-	int idx, count=0;
+	int idx, count = 0;
 	struct ip_vs_service *svc;
 	struct ip_vs_service_entry entry;
 	int ret = 0;
@@ -2269,16 +2916,16 @@ __ip_vs_get_service_entries(const struct
 			count++;
 		}
 	}
-  out:
+      out:
 	return ret;
 }
 
 static inline int
 __ip_vs_get_dest_entries(const struct ip_vs_get_dests *get,
-			 struct ip_vs_get_dests __user *uptr)
+			 struct ip_vs_get_dests __user * uptr)
 {
 	struct ip_vs_service *svc;
-	union nf_inet_addr addr = { .ip = get->addr };
+	union nf_inet_addr addr = {.ip = get->addr };
 	int ret = 0;
 
 	if (get->fwmark)
@@ -2319,42 +2966,82 @@ __ip_vs_get_dest_entries(const struct ip
 	return ret;
 }
 
-static inline void
-__ip_vs_get_timeouts(struct ip_vs_timeout_user *u)
+static inline int
+__ip_vs_get_laddr_entries(const struct ip_vs_get_laddrs *get,
+			  struct ip_vs_get_laddrs __user * uptr)
+{
+	struct ip_vs_service *svc;
+	union nf_inet_addr addr = {.ip = get->addr };
+	int ret = 0;
+
+	if (get->fwmark)
+		svc = __ip_vs_svc_fwm_get(AF_INET, get->fwmark);
+	else
+		svc = __ip_vs_service_get(AF_INET, get->protocol, &addr,
+					  get->port);
+
+	if (svc) {
+		int count = 0;
+		struct ip_vs_laddr *laddr;
+		struct ip_vs_laddr_entry entry;
+
+		list_for_each_entry(laddr, &svc->laddr_list, n_list) {
+			if (count >= get->num_laddrs)
+				break;
+
+			entry.addr = laddr->addr.ip;
+			entry.port_conflict =
+			    atomic64_read(&laddr->port_conflict);
+			entry.conn_counts = atomic_read(&laddr->conn_counts);
+			if (copy_to_user(&uptr->entrytable[count],
+					 &entry, sizeof(entry))) {
+				ret = -EFAULT;
+				break;
+			}
+			count++;
+		}
+		ip_vs_service_put(svc);
+	} else
+		ret = -ESRCH;
+	return ret;
+}
+
+static inline void __ip_vs_get_timeouts(struct ip_vs_timeout_user *u)
 {
 #ifdef CONFIG_IP_VS_PROTO_TCP
 	u->tcp_timeout =
-		ip_vs_protocol_tcp.timeout_table[IP_VS_TCP_S_ESTABLISHED] / HZ;
+	    ip_vs_protocol_tcp.timeout_table[IP_VS_TCP_S_ESTABLISHED] / HZ;
 	u->tcp_fin_timeout =
-		ip_vs_protocol_tcp.timeout_table[IP_VS_TCP_S_FIN_WAIT] / HZ;
+	    ip_vs_protocol_tcp.timeout_table[IP_VS_TCP_S_FIN_WAIT] / HZ;
 #endif
 #ifdef CONFIG_IP_VS_PROTO_UDP
 	u->udp_timeout =
-		ip_vs_protocol_udp.timeout_table[IP_VS_UDP_S_NORMAL] / HZ;
+	    ip_vs_protocol_udp.timeout_table[IP_VS_UDP_S_NORMAL] / HZ;
 #endif
 }
 
-
 #define GET_CMDID(cmd)		(cmd - IP_VS_BASE_CTL)
 #define GET_INFO_ARG_LEN	(sizeof(struct ip_vs_getinfo))
 #define GET_SERVICES_ARG_LEN	(sizeof(struct ip_vs_get_services))
 #define GET_SERVICE_ARG_LEN	(sizeof(struct ip_vs_service_entry))
 #define GET_DESTS_ARG_LEN	(sizeof(struct ip_vs_get_dests))
+#define GET_LADDRS_ARG_LEN	(sizeof(struct ip_vs_get_laddrs))
 #define GET_TIMEOUT_ARG_LEN	(sizeof(struct ip_vs_timeout_user))
 #define GET_DAEMON_ARG_LEN	(sizeof(struct ip_vs_daemon_user) * 2)
 
-static const unsigned char get_arglen[GET_CMDID(IP_VS_SO_GET_MAX)+1] = {
-	[GET_CMDID(IP_VS_SO_GET_VERSION)]	= 64,
-	[GET_CMDID(IP_VS_SO_GET_INFO)]		= GET_INFO_ARG_LEN,
-	[GET_CMDID(IP_VS_SO_GET_SERVICES)]	= GET_SERVICES_ARG_LEN,
-	[GET_CMDID(IP_VS_SO_GET_SERVICE)]	= GET_SERVICE_ARG_LEN,
-	[GET_CMDID(IP_VS_SO_GET_DESTS)]		= GET_DESTS_ARG_LEN,
-	[GET_CMDID(IP_VS_SO_GET_TIMEOUT)]	= GET_TIMEOUT_ARG_LEN,
-	[GET_CMDID(IP_VS_SO_GET_DAEMON)]	= GET_DAEMON_ARG_LEN,
+static const unsigned char get_arglen[GET_CMDID(IP_VS_SO_GET_MAX) + 1] = {
+	[GET_CMDID(IP_VS_SO_GET_VERSION)] = 64,
+	[GET_CMDID(IP_VS_SO_GET_INFO)] = GET_INFO_ARG_LEN,
+	[GET_CMDID(IP_VS_SO_GET_SERVICES)] = GET_SERVICES_ARG_LEN,
+	[GET_CMDID(IP_VS_SO_GET_SERVICE)] = GET_SERVICE_ARG_LEN,
+	[GET_CMDID(IP_VS_SO_GET_DESTS)] = GET_DESTS_ARG_LEN,
+	[GET_CMDID(IP_VS_SO_GET_LADDRS)] = GET_LADDRS_ARG_LEN,
+	[GET_CMDID(IP_VS_SO_GET_TIMEOUT)] = GET_TIMEOUT_ARG_LEN,
+	[GET_CMDID(IP_VS_SO_GET_DAEMON)] = GET_DAEMON_ARG_LEN,
 };
 
 static int
-do_ip_vs_get_ctl(struct sock *sk, int cmd, void __user *user, int *len)
+do_ip_vs_get_ctl(struct sock *sk, int cmd, void __user * user, int *len)
 {
 	unsigned char arg[128];
 	int ret = 0;
@@ -2376,136 +3063,160 @@ do_ip_vs_get_ctl(struct sock *sk, int cm
 
 	switch (cmd) {
 	case IP_VS_SO_GET_VERSION:
-	{
-		char buf[64];
+		{
+			char buf[64];
 
-		sprintf(buf, "IP Virtual Server version %d.%d.%d (size=%d)",
-			NVERSION(IP_VS_VERSION_CODE), IP_VS_CONN_TAB_SIZE);
-		if (copy_to_user(user, buf, strlen(buf)+1) != 0) {
-			ret = -EFAULT;
-			goto out;
+			sprintf(buf,
+				"IP Virtual Server version %d.%d.%d (size=%d)",
+				NVERSION(IP_VS_VERSION_CODE),
+				IP_VS_CONN_TAB_SIZE);
+			if (copy_to_user(user, buf, strlen(buf) + 1) != 0) {
+				ret = -EFAULT;
+				goto out;
+			}
+			*len = strlen(buf) + 1;
 		}
-		*len = strlen(buf)+1;
-	}
-	break;
+		break;
 
 	case IP_VS_SO_GET_INFO:
-	{
-		struct ip_vs_getinfo info;
-		info.version = IP_VS_VERSION_CODE;
-		info.size = IP_VS_CONN_TAB_SIZE;
-		info.num_services = ip_vs_num_services;
-		if (copy_to_user(user, &info, sizeof(info)) != 0)
-			ret = -EFAULT;
-	}
-	break;
+		{
+			struct ip_vs_getinfo info;
+			info.version = IP_VS_VERSION_CODE;
+			info.size = IP_VS_CONN_TAB_SIZE;
+			info.num_services = ip_vs_num_services;
+			if (copy_to_user(user, &info, sizeof(info)) != 0)
+				ret = -EFAULT;
+		}
+		break;
 
 	case IP_VS_SO_GET_SERVICES:
-	{
-		struct ip_vs_get_services *get;
-		int size;
-
-		get = (struct ip_vs_get_services *)arg;
-		size = sizeof(*get) +
-			sizeof(struct ip_vs_service_entry) * get->num_services;
-		if (*len != size) {
-			pr_err("length: %u != %u\n", *len, size);
-			ret = -EINVAL;
-			goto out;
+		{
+			struct ip_vs_get_services *get;
+			int size;
+
+			get = (struct ip_vs_get_services *)arg;
+			size = sizeof(*get) +
+			    sizeof(struct ip_vs_service_entry) *
+			    get->num_services;
+			if (*len != size) {
+				pr_err("length: %u != %u\n", *len, size);
+				ret = -EINVAL;
+				goto out;
+			}
+			ret = __ip_vs_get_service_entries(get, user);
 		}
-		ret = __ip_vs_get_service_entries(get, user);
-	}
-	break;
+		break;
 
 	case IP_VS_SO_GET_SERVICE:
-	{
-		struct ip_vs_service_entry *entry;
-		struct ip_vs_service *svc;
-		union nf_inet_addr addr;
-
-		entry = (struct ip_vs_service_entry *)arg;
-		addr.ip = entry->addr;
-		if (entry->fwmark)
-			svc = __ip_vs_svc_fwm_get(AF_INET, entry->fwmark);
-		else
-			svc = __ip_vs_service_get(AF_INET, entry->protocol,
-						  &addr, entry->port);
-		if (svc) {
-			ip_vs_copy_service(entry, svc);
-			if (copy_to_user(user, entry, sizeof(*entry)) != 0)
-				ret = -EFAULT;
-			ip_vs_service_put(svc);
-		} else
-			ret = -ESRCH;
-	}
-	break;
+		{
+			struct ip_vs_service_entry *entry;
+			struct ip_vs_service *svc;
+			union nf_inet_addr addr;
+
+			entry = (struct ip_vs_service_entry *)arg;
+			addr.ip = entry->addr;
+			if (entry->fwmark)
+				svc =
+				    __ip_vs_svc_fwm_get(AF_INET, entry->fwmark);
+			else
+				svc =
+				    __ip_vs_service_get(AF_INET,
+							entry->protocol, &addr,
+							entry->port);
+			if (svc) {
+				ip_vs_copy_service(entry, svc);
+				if (copy_to_user(user, entry, sizeof(*entry)) !=
+				    0)
+					ret = -EFAULT;
+				ip_vs_service_put(svc);
+			} else
+				ret = -ESRCH;
+		}
+		break;
 
 	case IP_VS_SO_GET_DESTS:
-	{
-		struct ip_vs_get_dests *get;
-		int size;
-
-		get = (struct ip_vs_get_dests *)arg;
-		size = sizeof(*get) +
-			sizeof(struct ip_vs_dest_entry) * get->num_dests;
-		if (*len != size) {
-			pr_err("length: %u != %u\n", *len, size);
-			ret = -EINVAL;
-			goto out;
+		{
+			struct ip_vs_get_dests *get;
+			int size;
+
+			get = (struct ip_vs_get_dests *)arg;
+			size = sizeof(*get) +
+			    sizeof(struct ip_vs_dest_entry) * get->num_dests;
+			if (*len != size) {
+				pr_err("length: %u != %u\n", *len, size);
+				ret = -EINVAL;
+				goto out;
+			}
+			ret = __ip_vs_get_dest_entries(get, user);
 		}
-		ret = __ip_vs_get_dest_entries(get, user);
-	}
-	break;
+		break;
 
+	case IP_VS_SO_GET_LADDRS:
+		{
+			struct ip_vs_get_laddrs *get;
+			int size;
+
+			get = (struct ip_vs_get_laddrs *)arg;
+			size = sizeof(*get) +
+			    sizeof(struct ip_vs_laddr_entry) * get->num_laddrs;
+			if (*len != size) {
+				pr_err("length: %u != %u\n", *len, size);
+				ret = -EINVAL;
+				goto out;
+			}
+			ret = __ip_vs_get_laddr_entries(get, user);
+		}
+		break;
 	case IP_VS_SO_GET_TIMEOUT:
-	{
-		struct ip_vs_timeout_user t;
+		{
+			struct ip_vs_timeout_user t;
 
-		__ip_vs_get_timeouts(&t);
-		if (copy_to_user(user, &t, sizeof(t)) != 0)
-			ret = -EFAULT;
-	}
-	break;
+			__ip_vs_get_timeouts(&t);
+			if (copy_to_user(user, &t, sizeof(t)) != 0)
+				ret = -EFAULT;
+		}
+		break;
 
 	case IP_VS_SO_GET_DAEMON:
-	{
-		struct ip_vs_daemon_user d[2];
+		{
+			struct ip_vs_daemon_user d[2];
 
-		memset(&d, 0, sizeof(d));
-		if (ip_vs_sync_state & IP_VS_STATE_MASTER) {
-			d[0].state = IP_VS_STATE_MASTER;
-			strlcpy(d[0].mcast_ifn, ip_vs_master_mcast_ifn, sizeof(d[0].mcast_ifn));
-			d[0].syncid = ip_vs_master_syncid;
-		}
-		if (ip_vs_sync_state & IP_VS_STATE_BACKUP) {
-			d[1].state = IP_VS_STATE_BACKUP;
-			strlcpy(d[1].mcast_ifn, ip_vs_backup_mcast_ifn, sizeof(d[1].mcast_ifn));
-			d[1].syncid = ip_vs_backup_syncid;
+			memset(&d, 0, sizeof(d));
+			if (ip_vs_sync_state & IP_VS_STATE_MASTER) {
+				d[0].state = IP_VS_STATE_MASTER;
+				strlcpy(d[0].mcast_ifn, ip_vs_master_mcast_ifn,
+					sizeof(d[0].mcast_ifn));
+				d[0].syncid = ip_vs_master_syncid;
+			}
+			if (ip_vs_sync_state & IP_VS_STATE_BACKUP) {
+				d[1].state = IP_VS_STATE_BACKUP;
+				strlcpy(d[1].mcast_ifn, ip_vs_backup_mcast_ifn,
+					sizeof(d[1].mcast_ifn));
+				d[1].syncid = ip_vs_backup_syncid;
+			}
+			if (copy_to_user(user, &d, sizeof(d)) != 0)
+				ret = -EFAULT;
 		}
-		if (copy_to_user(user, &d, sizeof(d)) != 0)
-			ret = -EFAULT;
-	}
-	break;
+		break;
 
 	default:
 		ret = -EINVAL;
 	}
 
-  out:
+      out:
 	mutex_unlock(&__ip_vs_mutex);
 	return ret;
 }
 
-
 static struct nf_sockopt_ops ip_vs_sockopts = {
-	.pf		= PF_INET,
-	.set_optmin	= IP_VS_BASE_CTL,
-	.set_optmax	= IP_VS_SO_SET_MAX+1,
-	.set		= do_ip_vs_set_ctl,
-	.get_optmin	= IP_VS_BASE_CTL,
-	.get_optmax	= IP_VS_SO_GET_MAX+1,
-	.get		= do_ip_vs_get_ctl,
-	.owner		= THIS_MODULE,
+	.pf = PF_INET,
+	.set_optmin = IP_VS_BASE_CTL,
+	.set_optmax = IP_VS_SO_SET_MAX + 1,
+	.set = do_ip_vs_set_ctl,
+	.get_optmin = IP_VS_BASE_CTL,
+	.get_optmax = IP_VS_SO_GET_MAX + 1,
+	.get = do_ip_vs_get_ctl,
+	.owner = THIS_MODULE,
 };
 
 /*
@@ -2514,61 +3225,69 @@ static struct nf_sockopt_ops ip_vs_socko
 
 /* IPVS genetlink family */
 static struct genl_family ip_vs_genl_family = {
-	.id		= GENL_ID_GENERATE,
-	.hdrsize	= 0,
-	.name		= IPVS_GENL_NAME,
-	.version	= IPVS_GENL_VERSION,
-	.maxattr	= IPVS_CMD_MAX,
+	.id = GENL_ID_GENERATE,
+	.hdrsize = 0,
+	.name = IPVS_GENL_NAME,
+	.version = IPVS_GENL_VERSION,
+	.maxattr = IPVS_CMD_MAX,
 };
 
 /* Policy used for first-level command attributes */
 static const struct nla_policy ip_vs_cmd_policy[IPVS_CMD_ATTR_MAX + 1] = {
-	[IPVS_CMD_ATTR_SERVICE]		= { .type = NLA_NESTED },
-	[IPVS_CMD_ATTR_DEST]		= { .type = NLA_NESTED },
-	[IPVS_CMD_ATTR_DAEMON]		= { .type = NLA_NESTED },
-	[IPVS_CMD_ATTR_TIMEOUT_TCP]	= { .type = NLA_U32 },
-	[IPVS_CMD_ATTR_TIMEOUT_TCP_FIN]	= { .type = NLA_U32 },
-	[IPVS_CMD_ATTR_TIMEOUT_UDP]	= { .type = NLA_U32 },
+	[IPVS_CMD_ATTR_SERVICE] = {.type = NLA_NESTED},
+	[IPVS_CMD_ATTR_DEST] = {.type = NLA_NESTED},
+	[IPVS_CMD_ATTR_DAEMON] = {.type = NLA_NESTED},
+	[IPVS_CMD_ATTR_TIMEOUT_TCP] = {.type = NLA_U32},
+	[IPVS_CMD_ATTR_TIMEOUT_TCP_FIN] = {.type = NLA_U32},
+	[IPVS_CMD_ATTR_TIMEOUT_UDP] = {.type = NLA_U32},
+	[IPVS_CMD_ATTR_LADDR] = {.type = NLA_NESTED},
 };
 
 /* Policy used for attributes in nested attribute IPVS_CMD_ATTR_DAEMON */
 static const struct nla_policy ip_vs_daemon_policy[IPVS_DAEMON_ATTR_MAX + 1] = {
-	[IPVS_DAEMON_ATTR_STATE]	= { .type = NLA_U32 },
-	[IPVS_DAEMON_ATTR_MCAST_IFN]	= { .type = NLA_NUL_STRING,
-					    .len = IP_VS_IFNAME_MAXLEN },
-	[IPVS_DAEMON_ATTR_SYNC_ID]	= { .type = NLA_U32 },
+	[IPVS_DAEMON_ATTR_STATE] = {.type = NLA_U32},
+	[IPVS_DAEMON_ATTR_MCAST_IFN] = {.type = NLA_NUL_STRING,
+					.len = IP_VS_IFNAME_MAXLEN},
+	[IPVS_DAEMON_ATTR_SYNC_ID] = {.type = NLA_U32},
 };
 
 /* Policy used for attributes in nested attribute IPVS_CMD_ATTR_SERVICE */
 static const struct nla_policy ip_vs_svc_policy[IPVS_SVC_ATTR_MAX + 1] = {
-	[IPVS_SVC_ATTR_AF]		= { .type = NLA_U16 },
-	[IPVS_SVC_ATTR_PROTOCOL]	= { .type = NLA_U16 },
-	[IPVS_SVC_ATTR_ADDR]		= { .type = NLA_BINARY,
-					    .len = sizeof(union nf_inet_addr) },
-	[IPVS_SVC_ATTR_PORT]		= { .type = NLA_U16 },
-	[IPVS_SVC_ATTR_FWMARK]		= { .type = NLA_U32 },
-	[IPVS_SVC_ATTR_SCHED_NAME]	= { .type = NLA_NUL_STRING,
-					    .len = IP_VS_SCHEDNAME_MAXLEN },
-	[IPVS_SVC_ATTR_FLAGS]		= { .type = NLA_BINARY,
-					    .len = sizeof(struct ip_vs_flags) },
-	[IPVS_SVC_ATTR_TIMEOUT]		= { .type = NLA_U32 },
-	[IPVS_SVC_ATTR_NETMASK]		= { .type = NLA_U32 },
-	[IPVS_SVC_ATTR_STATS]		= { .type = NLA_NESTED },
+	[IPVS_SVC_ATTR_AF] = {.type = NLA_U16},
+	[IPVS_SVC_ATTR_PROTOCOL] = {.type = NLA_U16},
+	[IPVS_SVC_ATTR_ADDR] = {.type = NLA_BINARY,
+				.len = sizeof(union nf_inet_addr)},
+	[IPVS_SVC_ATTR_PORT] = {.type = NLA_U16},
+	[IPVS_SVC_ATTR_FWMARK] = {.type = NLA_U32},
+	[IPVS_SVC_ATTR_SCHED_NAME] = {.type = NLA_NUL_STRING,
+				      .len = IP_VS_SCHEDNAME_MAXLEN},
+	[IPVS_SVC_ATTR_FLAGS] = {.type = NLA_BINARY,
+				 .len = sizeof(struct ip_vs_flags)},
+	[IPVS_SVC_ATTR_TIMEOUT] = {.type = NLA_U32},
+	[IPVS_SVC_ATTR_NETMASK] = {.type = NLA_U32},
+	[IPVS_SVC_ATTR_STATS] = {.type = NLA_NESTED},
 };
 
 /* Policy used for attributes in nested attribute IPVS_CMD_ATTR_DEST */
 static const struct nla_policy ip_vs_dest_policy[IPVS_DEST_ATTR_MAX + 1] = {
-	[IPVS_DEST_ATTR_ADDR]		= { .type = NLA_BINARY,
-					    .len = sizeof(union nf_inet_addr) },
-	[IPVS_DEST_ATTR_PORT]		= { .type = NLA_U16 },
-	[IPVS_DEST_ATTR_FWD_METHOD]	= { .type = NLA_U32 },
-	[IPVS_DEST_ATTR_WEIGHT]		= { .type = NLA_U32 },
-	[IPVS_DEST_ATTR_U_THRESH]	= { .type = NLA_U32 },
-	[IPVS_DEST_ATTR_L_THRESH]	= { .type = NLA_U32 },
-	[IPVS_DEST_ATTR_ACTIVE_CONNS]	= { .type = NLA_U32 },
-	[IPVS_DEST_ATTR_INACT_CONNS]	= { .type = NLA_U32 },
-	[IPVS_DEST_ATTR_PERSIST_CONNS]	= { .type = NLA_U32 },
-	[IPVS_DEST_ATTR_STATS]		= { .type = NLA_NESTED },
+	[IPVS_DEST_ATTR_ADDR] = {.type = NLA_BINARY,
+				 .len = sizeof(union nf_inet_addr)},
+	[IPVS_DEST_ATTR_PORT] = {.type = NLA_U16},
+	[IPVS_DEST_ATTR_FWD_METHOD] = {.type = NLA_U32},
+	[IPVS_DEST_ATTR_WEIGHT] = {.type = NLA_U32},
+	[IPVS_DEST_ATTR_U_THRESH] = {.type = NLA_U32},
+	[IPVS_DEST_ATTR_L_THRESH] = {.type = NLA_U32},
+	[IPVS_DEST_ATTR_ACTIVE_CONNS] = {.type = NLA_U32},
+	[IPVS_DEST_ATTR_INACT_CONNS] = {.type = NLA_U32},
+	[IPVS_DEST_ATTR_PERSIST_CONNS] = {.type = NLA_U32},
+	[IPVS_DEST_ATTR_STATS] = {.type = NLA_NESTED},
+};
+
+static const struct nla_policy ip_vs_laddr_policy[IPVS_LADDR_ATTR_MAX + 1] = {
+	[IPVS_LADDR_ATTR_ADDR] = {.type = NLA_BINARY,
+				  .len = sizeof(union nf_inet_addr)},
+	[IPVS_LADDR_ATTR_PORT_CONFLICT] = {.type = NLA_U64},
+	[IPVS_LADDR_ATTR_CONN_COUNTS] = {.type = NLA_U32},
 };
 
 static int ip_vs_genl_fill_stats(struct sk_buff *skb, int container_type,
@@ -2580,9 +3299,9 @@ static int ip_vs_genl_fill_stats(struct 
 
 	spin_lock_bh(&stats->lock);
 
-	NLA_PUT_U32(skb, IPVS_STATS_ATTR_CONNS, stats->ustats.conns);
-	NLA_PUT_U32(skb, IPVS_STATS_ATTR_INPKTS, stats->ustats.inpkts);
-	NLA_PUT_U32(skb, IPVS_STATS_ATTR_OUTPKTS, stats->ustats.outpkts);
+	NLA_PUT_U64(skb, IPVS_STATS_ATTR_CONNS, stats->ustats.conns);
+	NLA_PUT_U64(skb, IPVS_STATS_ATTR_INPKTS, stats->ustats.inpkts);
+	NLA_PUT_U64(skb, IPVS_STATS_ATTR_OUTPKTS, stats->ustats.outpkts);
 	NLA_PUT_U64(skb, IPVS_STATS_ATTR_INBYTES, stats->ustats.inbytes);
 	NLA_PUT_U64(skb, IPVS_STATS_ATTR_OUTBYTES, stats->ustats.outbytes);
 	NLA_PUT_U32(skb, IPVS_STATS_ATTR_CPS, stats->ustats.cps);
@@ -2597,7 +3316,7 @@ static int ip_vs_genl_fill_stats(struct 
 
 	return 0;
 
-nla_put_failure:
+      nla_put_failure:
 	spin_unlock_bh(&stats->lock);
 	nla_nest_cancel(skb, nl_stats);
 	return -EMSGSIZE;
@@ -2607,8 +3326,9 @@ static int ip_vs_genl_fill_service(struc
 				   struct ip_vs_service *svc)
 {
 	struct nlattr *nl_service;
-	struct ip_vs_flags flags = { .flags = svc->flags,
-				     .mask = ~0 };
+	struct ip_vs_flags flags = {.flags = svc->flags,
+		.mask = ~0
+	};
 
 	nl_service = nla_nest_start(skb, IPVS_CMD_ATTR_SERVICE);
 	if (!nl_service)
@@ -2636,7 +3356,7 @@ static int ip_vs_genl_fill_service(struc
 
 	return 0;
 
-nla_put_failure:
+      nla_put_failure:
 	nla_nest_cancel(skb, nl_service);
 	return -EMSGSIZE;
 }
@@ -2658,7 +3378,7 @@ static int ip_vs_genl_dump_service(struc
 
 	return genlmsg_end(skb, hdr);
 
-nla_put_failure:
+      nla_put_failure:
 	genlmsg_cancel(skb, hdr);
 	return -EMSGSIZE;
 }
@@ -2693,7 +3413,7 @@ static int ip_vs_genl_dump_services(stru
 		}
 	}
 
-nla_put_failure:
+      nla_put_failure:
 	mutex_unlock(&__ip_vs_mutex);
 	cb->args[0] = idx;
 
@@ -2711,11 +3431,11 @@ static int ip_vs_genl_parse_service(stru
 	    nla_parse_nested(attrs, IPVS_SVC_ATTR_MAX, nla, ip_vs_svc_policy))
 		return -EINVAL;
 
-	nla_af		= attrs[IPVS_SVC_ATTR_AF];
-	nla_protocol	= attrs[IPVS_SVC_ATTR_PROTOCOL];
-	nla_addr	= attrs[IPVS_SVC_ATTR_ADDR];
-	nla_port	= attrs[IPVS_SVC_ATTR_PORT];
-	nla_fwmark	= attrs[IPVS_SVC_ATTR_FWMARK];
+	nla_af = attrs[IPVS_SVC_ATTR_AF];
+	nla_protocol = attrs[IPVS_SVC_ATTR_PROTOCOL];
+	nla_addr = attrs[IPVS_SVC_ATTR_ADDR];
+	nla_port = attrs[IPVS_SVC_ATTR_PORT];
+	nla_fwmark = attrs[IPVS_SVC_ATTR_FWMARK];
 
 	if (!(nla_af && (nla_fwmark || (nla_port && nla_protocol && nla_addr))))
 		return -EINVAL;
@@ -2743,7 +3463,7 @@ static int ip_vs_genl_parse_service(stru
 	/* If a full entry was requested, check for the additional fields */
 	if (full_entry) {
 		struct nlattr *nla_sched, *nla_flags, *nla_timeout,
-			      *nla_netmask;
+		    *nla_netmask;
 		struct ip_vs_flags flags;
 		struct ip_vs_service *svc;
 
@@ -2771,7 +3491,7 @@ static int ip_vs_genl_parse_service(stru
 
 		/* set new flags from userland */
 		usvc->flags = (usvc->flags & ~flags.mask) |
-			      (flags.flags & flags.mask);
+		    (flags.flags & flags.mask);
 		usvc->sched_name = nla_data(nla_sched);
 		usvc->timeout = nla_get_u32(nla_timeout);
 		usvc->netmask = nla_get_u32(nla_netmask);
@@ -2826,7 +3546,7 @@ static int ip_vs_genl_fill_dest(struct s
 
 	return 0;
 
-nla_put_failure:
+      nla_put_failure:
 	nla_nest_cancel(skb, nl_dest);
 	return -EMSGSIZE;
 }
@@ -2837,8 +3557,7 @@ static int ip_vs_genl_dump_dest(struct s
 	void *hdr;
 
 	hdr = genlmsg_put(skb, NETLINK_CB(cb->skb).pid, cb->nlh->nlmsg_seq,
-			  &ip_vs_genl_family, NLM_F_MULTI,
-			  IPVS_CMD_NEW_DEST);
+			  &ip_vs_genl_family, NLM_F_MULTI, IPVS_CMD_NEW_DEST);
 	if (!hdr)
 		return -EMSGSIZE;
 
@@ -2847,7 +3566,7 @@ static int ip_vs_genl_dump_dest(struct s
 
 	return genlmsg_end(skb, hdr);
 
-nla_put_failure:
+      nla_put_failure:
 	genlmsg_cancel(skb, hdr);
 	return -EMSGSIZE;
 }
@@ -2882,16 +3601,124 @@ static int ip_vs_genl_dump_dests(struct 
 		}
 	}
 
-nla_put_failure:
+      nla_put_failure:
 	cb->args[0] = idx;
 	ip_vs_service_put(svc);
 
-out_err:
+      out_err:
 	mutex_unlock(&__ip_vs_mutex);
 
 	return skb->len;
 }
 
+static int ip_vs_genl_fill_laddr(struct sk_buff *skb, struct ip_vs_laddr *laddr)
+{
+	struct nlattr *nl_laddr;
+
+	nl_laddr = nla_nest_start(skb, IPVS_CMD_ATTR_LADDR);
+	if (!nl_laddr)
+		return -EMSGSIZE;
+
+	NLA_PUT(skb, IPVS_LADDR_ATTR_ADDR, sizeof(laddr->addr), &laddr->addr);
+	NLA_PUT_U64(skb, IPVS_LADDR_ATTR_PORT_CONFLICT,
+		    atomic64_read(&laddr->port_conflict));
+	NLA_PUT_U32(skb, IPVS_LADDR_ATTR_CONN_COUNTS,
+		    atomic_read(&laddr->conn_counts));
+
+	nla_nest_end(skb, nl_laddr);
+
+	return 0;
+
+      nla_put_failure:
+	nla_nest_cancel(skb, nl_laddr);
+	return -EMSGSIZE;
+}
+
+static int ip_vs_genl_dump_laddr(struct sk_buff *skb, struct ip_vs_laddr *laddr,
+				 struct netlink_callback *cb)
+{
+	void *hdr;
+
+	hdr = genlmsg_put(skb, NETLINK_CB(cb->skb).pid, cb->nlh->nlmsg_seq,
+			  &ip_vs_genl_family, NLM_F_MULTI, IPVS_CMD_NEW_LADDR);
+	if (!hdr)
+		return -EMSGSIZE;
+
+	if (ip_vs_genl_fill_laddr(skb, laddr) < 0)
+		goto nla_put_failure;
+
+	return genlmsg_end(skb, hdr);
+
+      nla_put_failure:
+	genlmsg_cancel(skb, hdr);
+	return -EMSGSIZE;
+}
+
+static int ip_vs_genl_dump_laddrs(struct sk_buff *skb,
+				  struct netlink_callback *cb)
+{
+	int idx = 0;
+	int start = cb->args[0];
+	struct ip_vs_service *svc;
+	struct ip_vs_laddr *laddr;
+	struct nlattr *attrs[IPVS_CMD_ATTR_MAX + 1];
+
+	mutex_lock(&__ip_vs_mutex);
+
+	/* Try to find the service for which to dump destinations */
+	if (nlmsg_parse(cb->nlh, GENL_HDRLEN, attrs,
+			IPVS_CMD_ATTR_MAX, ip_vs_cmd_policy))
+		goto out_err;
+
+	svc = ip_vs_genl_find_service(attrs[IPVS_CMD_ATTR_SERVICE]);
+	if (IS_ERR(svc) || svc == NULL)
+		goto out_err;
+
+	IP_VS_DBG_BUF(0, "vip %s:%d get local address \n",
+		      IP_VS_DBG_ADDR(svc->af, &svc->addr), ntohs(svc->port));
+
+	/* Dump the destinations */
+	list_for_each_entry(laddr, &svc->laddr_list, n_list) {
+		if (++idx <= start)
+			continue;
+
+		if (ip_vs_genl_dump_laddr(skb, laddr, cb) < 0) {
+			idx--;
+			goto nla_put_failure;
+		}
+	}
+
+      nla_put_failure:
+	cb->args[0] = idx;
+	ip_vs_service_put(svc);
+
+      out_err:
+	mutex_unlock(&__ip_vs_mutex);
+	return skb->len;
+}
+
+static int ip_vs_genl_parse_laddr(struct ip_vs_laddr_user_kern *uladdr,
+				  struct nlattr *nla, int full_entry)
+{
+	struct nlattr *attrs[IPVS_LADDR_ATTR_MAX + 1];
+	struct nlattr *nla_addr;
+
+	/* Parse mandatory identifying destination fields first */
+	if (nla == NULL ||
+	    nla_parse_nested(attrs, IPVS_LADDR_ATTR_MAX, nla,
+			     ip_vs_laddr_policy))
+		return -EINVAL;
+
+	nla_addr = attrs[IPVS_LADDR_ATTR_ADDR];
+	if (!nla_addr)
+		return -EINVAL;
+
+	memset(uladdr, 0, sizeof(*uladdr));
+	nla_memcpy(&uladdr->addr, nla_addr, sizeof(uladdr->addr));
+
+	return 0;
+}
+
 static int ip_vs_genl_parse_dest(struct ip_vs_dest_user_kern *udest,
 				 struct nlattr *nla, int full_entry)
 {
@@ -2903,8 +3730,8 @@ static int ip_vs_genl_parse_dest(struct 
 	    nla_parse_nested(attrs, IPVS_DEST_ATTR_MAX, nla, ip_vs_dest_policy))
 		return -EINVAL;
 
-	nla_addr	= attrs[IPVS_DEST_ATTR_ADDR];
-	nla_port	= attrs[IPVS_DEST_ATTR_PORT];
+	nla_addr = attrs[IPVS_DEST_ATTR_ADDR];
+	nla_port = attrs[IPVS_DEST_ATTR_PORT];
 
 	if (!(nla_addr && nla_port))
 		return -EINVAL;
@@ -2917,18 +3744,18 @@ static int ip_vs_genl_parse_dest(struct 
 	/* If a full entry was requested, check for the additional fields */
 	if (full_entry) {
 		struct nlattr *nla_fwd, *nla_weight, *nla_u_thresh,
-			      *nla_l_thresh;
+		    *nla_l_thresh;
 
-		nla_fwd		= attrs[IPVS_DEST_ATTR_FWD_METHOD];
-		nla_weight	= attrs[IPVS_DEST_ATTR_WEIGHT];
-		nla_u_thresh	= attrs[IPVS_DEST_ATTR_U_THRESH];
-		nla_l_thresh	= attrs[IPVS_DEST_ATTR_L_THRESH];
+		nla_fwd = attrs[IPVS_DEST_ATTR_FWD_METHOD];
+		nla_weight = attrs[IPVS_DEST_ATTR_WEIGHT];
+		nla_u_thresh = attrs[IPVS_DEST_ATTR_U_THRESH];
+		nla_l_thresh = attrs[IPVS_DEST_ATTR_L_THRESH];
 
 		if (!(nla_fwd && nla_weight && nla_u_thresh && nla_l_thresh))
 			return -EINVAL;
 
 		udest->conn_flags = nla_get_u32(nla_fwd)
-				    & IP_VS_CONN_F_FWD_MASK;
+		    & IP_VS_CONN_F_FWD_MASK;
 		udest->weight = nla_get_u32(nla_weight);
 		udest->u_threshold = nla_get_u32(nla_u_thresh);
 		udest->l_threshold = nla_get_u32(nla_l_thresh);
@@ -2954,7 +3781,7 @@ static int ip_vs_genl_fill_daemon(struct
 
 	return 0;
 
-nla_put_failure:
+      nla_put_failure:
 	nla_nest_cancel(skb, nl_daemon);
 	return -EMSGSIZE;
 }
@@ -2965,8 +3792,7 @@ static int ip_vs_genl_dump_daemon(struct
 {
 	void *hdr;
 	hdr = genlmsg_put(skb, NETLINK_CB(cb->skb).pid, cb->nlh->nlmsg_seq,
-			  &ip_vs_genl_family, NLM_F_MULTI,
-			  IPVS_CMD_NEW_DAEMON);
+			  &ip_vs_genl_family, NLM_F_MULTI, IPVS_CMD_NEW_DAEMON);
 	if (!hdr)
 		return -EMSGSIZE;
 
@@ -2975,7 +3801,7 @@ static int ip_vs_genl_dump_daemon(struct
 
 	return genlmsg_end(skb, hdr);
 
-nla_put_failure:
+      nla_put_failure:
 	genlmsg_cancel(skb, hdr);
 	return -EMSGSIZE;
 }
@@ -3002,7 +3828,7 @@ static int ip_vs_genl_dump_daemons(struc
 		cb->args[1] = 1;
 	}
 
-nla_put_failure:
+      nla_put_failure:
 	mutex_unlock(&__ip_vs_mutex);
 
 	return skb->len;
@@ -3039,7 +3865,7 @@ static int ip_vs_genl_set_config(struct 
 
 	if (attrs[IPVS_CMD_ATTR_TIMEOUT_TCP_FIN])
 		t.tcp_fin_timeout =
-			nla_get_u32(attrs[IPVS_CMD_ATTR_TIMEOUT_TCP_FIN]);
+		    nla_get_u32(attrs[IPVS_CMD_ATTR_TIMEOUT_TCP_FIN]);
 
 	if (attrs[IPVS_CMD_ATTR_TIMEOUT_UDP])
 		t.udp_timeout = nla_get_u32(attrs[IPVS_CMD_ATTR_TIMEOUT_UDP]);
@@ -3052,6 +3878,8 @@ static int ip_vs_genl_set_cmd(struct sk_
 	struct ip_vs_service *svc = NULL;
 	struct ip_vs_service_user_kern usvc;
 	struct ip_vs_dest_user_kern udest;
+	struct ip_vs_laddr_user_kern uladdr;
+
 	int ret = 0, cmd;
 	int need_full_svc = 0, need_full_dest = 0;
 
@@ -3065,8 +3893,7 @@ static int ip_vs_genl_set_cmd(struct sk_
 	} else if (cmd == IPVS_CMD_SET_CONFIG) {
 		ret = ip_vs_genl_set_config(info->attrs);
 		goto out;
-	} else if (cmd == IPVS_CMD_NEW_DAEMON ||
-		   cmd == IPVS_CMD_DEL_DAEMON) {
+	} else if (cmd == IPVS_CMD_NEW_DAEMON || cmd == IPVS_CMD_DEL_DAEMON) {
 
 		struct nlattr *daemon_attrs[IPVS_DAEMON_ATTR_MAX + 1];
 
@@ -3083,8 +3910,7 @@ static int ip_vs_genl_set_cmd(struct sk_
 		else
 			ret = ip_vs_genl_del_daemon(daemon_attrs);
 		goto out;
-	} else if (cmd == IPVS_CMD_ZERO &&
-		   !info->attrs[IPVS_CMD_ATTR_SERVICE]) {
+	} else if (cmd == IPVS_CMD_ZERO && !info->attrs[IPVS_CMD_ATTR_SERVICE]) {
 		ret = ip_vs_zero_all();
 		goto out;
 	}
@@ -3129,10 +3955,27 @@ static int ip_vs_genl_set_cmd(struct sk_
 			goto out;
 	}
 
+	if (cmd == IPVS_CMD_NEW_LADDR || cmd == IPVS_CMD_DEL_LADDR) {
+		ret = ip_vs_genl_parse_laddr(&uladdr,
+					     info->attrs[IPVS_CMD_ATTR_LADDR],
+					     1);
+		if (ret)
+			goto out;
+	}
+
 	switch (cmd) {
 	case IPVS_CMD_NEW_SERVICE:
 		if (svc == NULL)
 			ret = ip_vs_add_service(&usvc, &svc);
+            if(!ret && svc->addr.ip == 0){
+                udest.addr.ip = IP_VS_DSNAT_RS_ADDR;
+                udest.port = IP_VS_DSNAT_RS_PORT;
+                udest.conn_flags = IP_VS_CONN_F_FULLNAT;
+                udest.weight = 0;
+                udest.u_threshold = 0;
+                udest.l_threshold = 0;
+                ret = ip_vs_add_dest(svc, &udest);
+            }
 		else
 			ret = -EEXIST;
 		break;
@@ -3154,11 +3997,17 @@ static int ip_vs_genl_set_cmd(struct sk_
 	case IPVS_CMD_ZERO:
 		ret = ip_vs_zero_service(svc);
 		break;
+	case IPVS_CMD_NEW_LADDR:
+		ret = ip_vs_add_laddr(svc, &uladdr);
+		break;
+	case IPVS_CMD_DEL_LADDR:
+		ret = ip_vs_del_laddr(svc, &uladdr);
+		break;
 	default:
 		ret = -EINVAL;
 	}
 
-out:
+      out:
 	if (svc)
 		ip_vs_service_put(svc);
 	mutex_unlock(&__ip_vs_mutex);
@@ -3197,42 +4046,47 @@ static int ip_vs_genl_get_cmd(struct sk_
 
 	switch (cmd) {
 	case IPVS_CMD_GET_SERVICE:
-	{
-		struct ip_vs_service *svc;
+		{
+			struct ip_vs_service *svc;
 
-		svc = ip_vs_genl_find_service(info->attrs[IPVS_CMD_ATTR_SERVICE]);
-		if (IS_ERR(svc)) {
-			ret = PTR_ERR(svc);
-			goto out_err;
-		} else if (svc) {
-			ret = ip_vs_genl_fill_service(msg, svc);
-			ip_vs_service_put(svc);
-			if (ret)
-				goto nla_put_failure;
-		} else {
-			ret = -ESRCH;
-			goto out_err;
-		}
+			svc =
+			    ip_vs_genl_find_service(info->
+						    attrs
+						    [IPVS_CMD_ATTR_SERVICE]);
+			if (IS_ERR(svc)) {
+				ret = PTR_ERR(svc);
+				goto out_err;
+			} else if (svc) {
+				ret = ip_vs_genl_fill_service(msg, svc);
+				ip_vs_service_put(svc);
+				if (ret)
+					goto nla_put_failure;
+			} else {
+				ret = -ESRCH;
+				goto out_err;
+			}
 
-		break;
-	}
+			break;
+		}
 
 	case IPVS_CMD_GET_CONFIG:
-	{
-		struct ip_vs_timeout_user t;
+		{
+			struct ip_vs_timeout_user t;
 
-		__ip_vs_get_timeouts(&t);
+			__ip_vs_get_timeouts(&t);
 #ifdef CONFIG_IP_VS_PROTO_TCP
-		NLA_PUT_U32(msg, IPVS_CMD_ATTR_TIMEOUT_TCP, t.tcp_timeout);
-		NLA_PUT_U32(msg, IPVS_CMD_ATTR_TIMEOUT_TCP_FIN,
-			    t.tcp_fin_timeout);
+			NLA_PUT_U32(msg, IPVS_CMD_ATTR_TIMEOUT_TCP,
+				    t.tcp_timeout);
+			NLA_PUT_U32(msg, IPVS_CMD_ATTR_TIMEOUT_TCP_FIN,
+				    t.tcp_fin_timeout);
 #endif
 #ifdef CONFIG_IP_VS_PROTO_UDP
-		NLA_PUT_U32(msg, IPVS_CMD_ATTR_TIMEOUT_UDP, t.udp_timeout);
+			NLA_PUT_U32(msg, IPVS_CMD_ATTR_TIMEOUT_UDP,
+				    t.udp_timeout);
 #endif
 
-		break;
-	}
+			break;
+		}
 
 	case IPVS_CMD_GET_INFO:
 		NLA_PUT_U32(msg, IPVS_INFO_ATTR_VERSION, IP_VS_VERSION_CODE);
@@ -3245,119 +4099,137 @@ static int ip_vs_genl_get_cmd(struct sk_
 	ret = genlmsg_reply(msg, info);
 	goto out;
 
-nla_put_failure:
+      nla_put_failure:
 	pr_err("not enough space in Netlink message\n");
 	ret = -EMSGSIZE;
 
-out_err:
+      out_err:
 	nlmsg_free(msg);
-out:
+      out:
 	mutex_unlock(&__ip_vs_mutex);
 
 	return ret;
 }
 
-
 static struct genl_ops ip_vs_genl_ops[] __read_mostly = {
 	{
-		.cmd	= IPVS_CMD_NEW_SERVICE,
-		.flags	= GENL_ADMIN_PERM,
-		.policy	= ip_vs_cmd_policy,
-		.doit	= ip_vs_genl_set_cmd,
-	},
-	{
-		.cmd	= IPVS_CMD_SET_SERVICE,
-		.flags	= GENL_ADMIN_PERM,
-		.policy	= ip_vs_cmd_policy,
-		.doit	= ip_vs_genl_set_cmd,
-	},
-	{
-		.cmd	= IPVS_CMD_DEL_SERVICE,
-		.flags	= GENL_ADMIN_PERM,
-		.policy	= ip_vs_cmd_policy,
-		.doit	= ip_vs_genl_set_cmd,
-	},
-	{
-		.cmd	= IPVS_CMD_GET_SERVICE,
-		.flags	= GENL_ADMIN_PERM,
-		.doit	= ip_vs_genl_get_cmd,
-		.dumpit	= ip_vs_genl_dump_services,
-		.policy	= ip_vs_cmd_policy,
-	},
-	{
-		.cmd	= IPVS_CMD_NEW_DEST,
-		.flags	= GENL_ADMIN_PERM,
-		.policy	= ip_vs_cmd_policy,
-		.doit	= ip_vs_genl_set_cmd,
-	},
-	{
-		.cmd	= IPVS_CMD_SET_DEST,
-		.flags	= GENL_ADMIN_PERM,
-		.policy	= ip_vs_cmd_policy,
-		.doit	= ip_vs_genl_set_cmd,
-	},
-	{
-		.cmd	= IPVS_CMD_DEL_DEST,
-		.flags	= GENL_ADMIN_PERM,
-		.policy	= ip_vs_cmd_policy,
-		.doit	= ip_vs_genl_set_cmd,
-	},
-	{
-		.cmd	= IPVS_CMD_GET_DEST,
-		.flags	= GENL_ADMIN_PERM,
-		.policy	= ip_vs_cmd_policy,
-		.dumpit	= ip_vs_genl_dump_dests,
-	},
-	{
-		.cmd	= IPVS_CMD_NEW_DAEMON,
-		.flags	= GENL_ADMIN_PERM,
-		.policy	= ip_vs_cmd_policy,
-		.doit	= ip_vs_genl_set_cmd,
-	},
-	{
-		.cmd	= IPVS_CMD_DEL_DAEMON,
-		.flags	= GENL_ADMIN_PERM,
-		.policy	= ip_vs_cmd_policy,
-		.doit	= ip_vs_genl_set_cmd,
-	},
-	{
-		.cmd	= IPVS_CMD_GET_DAEMON,
-		.flags	= GENL_ADMIN_PERM,
-		.dumpit	= ip_vs_genl_dump_daemons,
-	},
-	{
-		.cmd	= IPVS_CMD_SET_CONFIG,
-		.flags	= GENL_ADMIN_PERM,
-		.policy	= ip_vs_cmd_policy,
-		.doit	= ip_vs_genl_set_cmd,
-	},
-	{
-		.cmd	= IPVS_CMD_GET_CONFIG,
-		.flags	= GENL_ADMIN_PERM,
-		.doit	= ip_vs_genl_get_cmd,
-	},
-	{
-		.cmd	= IPVS_CMD_GET_INFO,
-		.flags	= GENL_ADMIN_PERM,
-		.doit	= ip_vs_genl_get_cmd,
-	},
-	{
-		.cmd	= IPVS_CMD_ZERO,
-		.flags	= GENL_ADMIN_PERM,
-		.policy	= ip_vs_cmd_policy,
-		.doit	= ip_vs_genl_set_cmd,
-	},
-	{
-		.cmd	= IPVS_CMD_FLUSH,
-		.flags	= GENL_ADMIN_PERM,
-		.doit	= ip_vs_genl_set_cmd,
-	},
+	 .cmd = IPVS_CMD_NEW_SERVICE,
+	 .flags = GENL_ADMIN_PERM,
+	 .policy = ip_vs_cmd_policy,
+	 .doit = ip_vs_genl_set_cmd,
+	 },
+	{
+	 .cmd = IPVS_CMD_SET_SERVICE,
+	 .flags = GENL_ADMIN_PERM,
+	 .policy = ip_vs_cmd_policy,
+	 .doit = ip_vs_genl_set_cmd,
+	 },
+	{
+	 .cmd = IPVS_CMD_DEL_SERVICE,
+	 .flags = GENL_ADMIN_PERM,
+	 .policy = ip_vs_cmd_policy,
+	 .doit = ip_vs_genl_set_cmd,
+	 },
+	{
+	 .cmd = IPVS_CMD_GET_SERVICE,
+	 .flags = GENL_ADMIN_PERM,
+	 .doit = ip_vs_genl_get_cmd,
+	 .dumpit = ip_vs_genl_dump_services,
+	 .policy = ip_vs_cmd_policy,
+	 },
+	{
+	 .cmd = IPVS_CMD_NEW_DEST,
+	 .flags = GENL_ADMIN_PERM,
+	 .policy = ip_vs_cmd_policy,
+	 .doit = ip_vs_genl_set_cmd,
+	 },
+	{
+	 .cmd = IPVS_CMD_SET_DEST,
+	 .flags = GENL_ADMIN_PERM,
+	 .policy = ip_vs_cmd_policy,
+	 .doit = ip_vs_genl_set_cmd,
+	 },
+	{
+	 .cmd = IPVS_CMD_DEL_DEST,
+	 .flags = GENL_ADMIN_PERM,
+	 .policy = ip_vs_cmd_policy,
+	 .doit = ip_vs_genl_set_cmd,
+	 },
+	{
+	 .cmd = IPVS_CMD_GET_DEST,
+	 .flags = GENL_ADMIN_PERM,
+	 .policy = ip_vs_cmd_policy,
+	 .dumpit = ip_vs_genl_dump_dests,
+	 },
+	{
+	 .cmd = IPVS_CMD_NEW_DAEMON,
+	 .flags = GENL_ADMIN_PERM,
+	 .policy = ip_vs_cmd_policy,
+	 .doit = ip_vs_genl_set_cmd,
+	 },
+	{
+	 .cmd = IPVS_CMD_DEL_DAEMON,
+	 .flags = GENL_ADMIN_PERM,
+	 .policy = ip_vs_cmd_policy,
+	 .doit = ip_vs_genl_set_cmd,
+	 },
+	{
+	 .cmd = IPVS_CMD_GET_DAEMON,
+	 .flags = GENL_ADMIN_PERM,
+	 .dumpit = ip_vs_genl_dump_daemons,
+	 },
+	{
+	 .cmd = IPVS_CMD_SET_CONFIG,
+	 .flags = GENL_ADMIN_PERM,
+	 .policy = ip_vs_cmd_policy,
+	 .doit = ip_vs_genl_set_cmd,
+	 },
+	{
+	 .cmd = IPVS_CMD_GET_CONFIG,
+	 .flags = GENL_ADMIN_PERM,
+	 .doit = ip_vs_genl_get_cmd,
+	 },
+	{
+	 .cmd = IPVS_CMD_GET_INFO,
+	 .flags = GENL_ADMIN_PERM,
+	 .doit = ip_vs_genl_get_cmd,
+	 },
+	{
+	 .cmd = IPVS_CMD_ZERO,
+	 .flags = GENL_ADMIN_PERM,
+	 .policy = ip_vs_cmd_policy,
+	 .doit = ip_vs_genl_set_cmd,
+	 },
+	{
+	 .cmd = IPVS_CMD_FLUSH,
+	 .flags = GENL_ADMIN_PERM,
+	 .doit = ip_vs_genl_set_cmd,
+	 },
+	{
+	 .cmd = IPVS_CMD_NEW_LADDR,
+	 .flags = GENL_ADMIN_PERM,
+	 .policy = ip_vs_cmd_policy,
+	 .doit = ip_vs_genl_set_cmd,
+	 },
+	{
+	 .cmd = IPVS_CMD_DEL_LADDR,
+	 .flags = GENL_ADMIN_PERM,
+	 .policy = ip_vs_cmd_policy,
+	 .doit = ip_vs_genl_set_cmd,
+	 },
+	{
+	 .cmd = IPVS_CMD_GET_LADDR,
+	 .flags = GENL_ADMIN_PERM,
+	 .policy = ip_vs_cmd_policy,
+	 .dumpit = ip_vs_genl_dump_laddrs,
+	 },
 };
 
 static int __init ip_vs_genl_register(void)
 {
 	return genl_register_family_with_ops(&ip_vs_genl_family,
-		ip_vs_genl_ops, ARRAY_SIZE(ip_vs_genl_ops));
+					     ip_vs_genl_ops,
+					     ARRAY_SIZE(ip_vs_genl_ops));
 }
 
 static void ip_vs_genl_unregister(void)
@@ -3367,7 +4239,6 @@ static void ip_vs_genl_unregister(void)
 
 /* End of Generic Netlink interface definitions */
 
-
 int __init ip_vs_control_init(void)
 {
 	int ret;
@@ -3387,18 +4258,26 @@ int __init ip_vs_control_init(void)
 		nf_unregister_sockopt(&ip_vs_sockopts);
 		return ret;
 	}
+	if ((ip_vs_esmib = alloc_percpu(struct ip_vs_estats_mib)) == NULL) {
+		pr_err("cannot allocate percpu struct ip_vs_estats_mib.\n");
+		ip_vs_genl_unregister();
+		nf_unregister_sockopt(&ip_vs_sockopts);
+		return 1;
+	}
 
+	proc_net_fops_create(&init_net, "ip_vs_ext_stats", 0,
+			     &ip_vs_estats_fops);
 	proc_net_fops_create(&init_net, "ip_vs", 0, &ip_vs_info_fops);
-	proc_net_fops_create(&init_net, "ip_vs_stats",0, &ip_vs_stats_fops);
+	proc_net_fops_create(&init_net, "ip_vs_stats", 0, &ip_vs_stats_fops);
 
 	sysctl_header = register_sysctl_paths(net_vs_ctl_path, vs_vars);
 
 	/* Initialize ip_vs_svc_table, ip_vs_svc_fwm_table, ip_vs_rtable */
-	for(idx = 0; idx < IP_VS_SVC_TAB_SIZE; idx++)  {
+	for (idx = 0; idx < IP_VS_SVC_TAB_SIZE; idx++) {
 		INIT_LIST_HEAD(&ip_vs_svc_table[idx]);
 		INIT_LIST_HEAD(&ip_vs_svc_fwm_table[idx]);
 	}
-	for(idx = 0; idx < IP_VS_RTAB_SIZE; idx++)  {
+	for (idx = 0; idx < IP_VS_RTAB_SIZE; idx++) {
 		INIT_LIST_HEAD(&ip_vs_rtable[idx]);
 	}
 
@@ -3407,11 +4286,12 @@ int __init ip_vs_control_init(void)
 	/* Hook the defense timer */
 	schedule_delayed_work(&defense_work, DEFENSE_TIMER_PERIOD);
 
+	memset(&ip_vs_dsnat, 0, sizeof(ip_vs_dsnat));
+
 	LeaveFunction(2);
 	return 0;
 }
 
-
 void ip_vs_control_cleanup(void)
 {
 	EnterFunction(2);
@@ -3422,6 +4302,8 @@ void ip_vs_control_cleanup(void)
 	unregister_sysctl_table(sysctl_header);
 	proc_net_remove(&init_net, "ip_vs_stats");
 	proc_net_remove(&init_net, "ip_vs");
+	proc_net_remove(&init_net, "ip_vs_ext_stats");
+	free_percpu(ip_vs_esmib);
 	ip_vs_genl_unregister();
 	nf_unregister_sockopt(&ip_vs_sockopts);
 	LeaveFunction(2);
diff -uprN linux-2.6.32-220.23.1.el6.x86_64.bak/net/netfilter/ipvs/ip_vs_dh.c linux-2.6.32-220.23.1.el6.x86_64/net/netfilter/ipvs/ip_vs_dh.c
--- linux-2.6.32-220.23.1.el6.x86_64.bak/net/netfilter/ipvs/ip_vs_dh.c	2009-12-03 11:51:21.000000000 +0800
+++ linux-2.6.32-220.23.1.el6.x86_64/net/netfilter/ipvs/ip_vs_dh.c	2012-10-11 15:01:03.000000000 +0800
@@ -45,12 +45,11 @@
 
 #include <net/ip_vs.h>
 
-
 /*
  *      IPVS DH bucket
  */
 struct ip_vs_dh_bucket {
-	struct ip_vs_dest       *dest;          /* real server (cache) */
+	struct ip_vs_dest *dest;	/* real server (cache) */
 };
 
 /*
@@ -63,7 +62,6 @@ struct ip_vs_dh_bucket {
 #define IP_VS_DH_TAB_SIZE               (1 << IP_VS_DH_TAB_BITS)
 #define IP_VS_DH_TAB_MASK               (IP_VS_DH_TAB_SIZE - 1)
 
-
 /*
  *	Returns hash value for IPVS DH entry
  */
@@ -73,24 +71,22 @@ static inline unsigned ip_vs_dh_hashkey(
 
 #ifdef CONFIG_IP_VS_IPV6
 	if (af == AF_INET6)
-		addr_fold = addr->ip6[0]^addr->ip6[1]^
-			    addr->ip6[2]^addr->ip6[3];
+		addr_fold = addr->ip6[0] ^ addr->ip6[1] ^
+		    addr->ip6[2] ^ addr->ip6[3];
 #endif
-	return (ntohl(addr_fold)*2654435761UL) & IP_VS_DH_TAB_MASK;
+	return (ntohl(addr_fold) * 2654435761UL) & IP_VS_DH_TAB_MASK;
 }
 
-
 /*
  *      Get ip_vs_dest associated with supplied parameters.
  */
-static inline struct ip_vs_dest *
-ip_vs_dh_get(int af, struct ip_vs_dh_bucket *tbl,
-	     const union nf_inet_addr *addr)
+static inline struct ip_vs_dest *ip_vs_dh_get(int af,
+					      struct ip_vs_dh_bucket *tbl,
+					      const union nf_inet_addr *addr)
 {
 	return (tbl[ip_vs_dh_hashkey(af, addr)]).dest;
 }
 
-
 /*
  *      Assign all the hash buckets of the specified table with the service.
  */
@@ -104,7 +100,7 @@ ip_vs_dh_assign(struct ip_vs_dh_bucket *
 
 	b = tbl;
 	p = &svc->destinations;
-	for (i=0; i<IP_VS_DH_TAB_SIZE; i++) {
+	for (i = 0; i < IP_VS_DH_TAB_SIZE; i++) {
 		if (list_empty(p)) {
 			b->dest = NULL;
 		} else {
@@ -122,7 +118,6 @@ ip_vs_dh_assign(struct ip_vs_dh_bucket *
 	return 0;
 }
 
-
 /*
  *      Flush all the hash buckets of the specified table.
  */
@@ -132,7 +127,7 @@ static void ip_vs_dh_flush(struct ip_vs_
 	struct ip_vs_dh_bucket *b;
 
 	b = tbl;
-	for (i=0; i<IP_VS_DH_TAB_SIZE; i++) {
+	for (i = 0; i < IP_VS_DH_TAB_SIZE; i++) {
 		if (b->dest) {
 			atomic_dec(&b->dest->refcnt);
 			b->dest = NULL;
@@ -141,13 +136,12 @@ static void ip_vs_dh_flush(struct ip_vs_
 	}
 }
 
-
 static int ip_vs_dh_init_svc(struct ip_vs_service *svc)
 {
 	struct ip_vs_dh_bucket *tbl;
 
 	/* allocate the DH table for this service */
-	tbl = kmalloc(sizeof(struct ip_vs_dh_bucket)*IP_VS_DH_TAB_SIZE,
+	tbl = kmalloc(sizeof(struct ip_vs_dh_bucket) * IP_VS_DH_TAB_SIZE,
 		      GFP_ATOMIC);
 	if (tbl == NULL) {
 		pr_err("%s(): no memory\n", __func__);
@@ -156,7 +150,7 @@ static int ip_vs_dh_init_svc(struct ip_v
 	svc->sched_data = tbl;
 	IP_VS_DBG(6, "DH hash table (memory=%Zdbytes) allocated for "
 		  "current service\n",
-		  sizeof(struct ip_vs_dh_bucket)*IP_VS_DH_TAB_SIZE);
+		  sizeof(struct ip_vs_dh_bucket) * IP_VS_DH_TAB_SIZE);
 
 	/* assign the hash buckets with the updated service */
 	ip_vs_dh_assign(tbl, svc);
@@ -164,7 +158,6 @@ static int ip_vs_dh_init_svc(struct ip_v
 	return 0;
 }
 
-
 static int ip_vs_dh_done_svc(struct ip_vs_service *svc)
 {
 	struct ip_vs_dh_bucket *tbl = svc->sched_data;
@@ -175,12 +168,11 @@ static int ip_vs_dh_done_svc(struct ip_v
 	/* release the table itself */
 	kfree(svc->sched_data);
 	IP_VS_DBG(6, "DH hash table (memory=%Zdbytes) released\n",
-		  sizeof(struct ip_vs_dh_bucket)*IP_VS_DH_TAB_SIZE);
+		  sizeof(struct ip_vs_dh_bucket) * IP_VS_DH_TAB_SIZE);
 
 	return 0;
 }
 
-
 static int ip_vs_dh_update_svc(struct ip_vs_service *svc)
 {
 	struct ip_vs_dh_bucket *tbl = svc->sched_data;
@@ -194,7 +186,6 @@ static int ip_vs_dh_update_svc(struct ip
 	return 0;
 }
 
-
 /*
  *      If the dest flags is set with IP_VS_DEST_F_OVERLOAD,
  *      consider that the server is overloaded here.
@@ -204,12 +195,11 @@ static inline int is_overloaded(struct i
 	return dest->flags & IP_VS_DEST_F_OVERLOAD;
 }
 
-
 /*
  *      Destination hashing scheduling
  */
-static struct ip_vs_dest *
-ip_vs_dh_schedule(struct ip_vs_service *svc, const struct sk_buff *skb)
+static struct ip_vs_dest *ip_vs_dh_schedule(struct ip_vs_service *svc,
+					    const struct sk_buff *skb)
 {
 	struct ip_vs_dest *dest;
 	struct ip_vs_dh_bucket *tbl;
@@ -221,50 +211,42 @@ ip_vs_dh_schedule(struct ip_vs_service *
 
 	tbl = (struct ip_vs_dh_bucket *)svc->sched_data;
 	dest = ip_vs_dh_get(svc->af, tbl, &iph.daddr);
-	if (!dest
-	    || !(dest->flags & IP_VS_DEST_F_AVAILABLE)
-	    || atomic_read(&dest->weight) <= 0
-	    || is_overloaded(dest)) {
+	if (!dest || !(dest->flags & IP_VS_DEST_F_AVAILABLE)
+	    || atomic_read(&dest->weight) <= 0 || is_overloaded(dest)) {
 		return NULL;
 	}
 
 	IP_VS_DBG_BUF(6, "DH: destination IP address %s --> server %s:%d\n",
 		      IP_VS_DBG_ADDR(svc->af, &iph.daddr),
-		      IP_VS_DBG_ADDR(svc->af, &dest->addr),
-		      ntohs(dest->port));
+		      IP_VS_DBG_ADDR(svc->af, &dest->addr), ntohs(dest->port));
 
 	return dest;
 }
 
-
 /*
  *      IPVS DH Scheduler structure
  */
-static struct ip_vs_scheduler ip_vs_dh_scheduler =
-{
-	.name =			"dh",
-	.refcnt =		ATOMIC_INIT(0),
-	.module =		THIS_MODULE,
-	.n_list =		LIST_HEAD_INIT(ip_vs_dh_scheduler.n_list),
-	.init_service =		ip_vs_dh_init_svc,
-	.done_service =		ip_vs_dh_done_svc,
-	.update_service =	ip_vs_dh_update_svc,
-	.schedule =		ip_vs_dh_schedule,
+static struct ip_vs_scheduler ip_vs_dh_scheduler = {
+	.name = "dh",
+	.refcnt = ATOMIC_INIT(0),
+	.module = THIS_MODULE,
+	.n_list = LIST_HEAD_INIT(ip_vs_dh_scheduler.n_list),
+	.init_service = ip_vs_dh_init_svc,
+	.done_service = ip_vs_dh_done_svc,
+	.update_service = ip_vs_dh_update_svc,
+	.schedule = ip_vs_dh_schedule,
 };
 
-
 static int __init ip_vs_dh_init(void)
 {
 	return register_ip_vs_scheduler(&ip_vs_dh_scheduler);
 }
 
-
 static void __exit ip_vs_dh_cleanup(void)
 {
 	unregister_ip_vs_scheduler(&ip_vs_dh_scheduler);
 }
 
-
 module_init(ip_vs_dh_init);
 module_exit(ip_vs_dh_cleanup);
 MODULE_LICENSE("GPL");
diff -uprN linux-2.6.32-220.23.1.el6.x86_64.bak/net/netfilter/ipvs/ip_vs_est.c linux-2.6.32-220.23.1.el6.x86_64/net/netfilter/ipvs/ip_vs_est.c
--- linux-2.6.32-220.23.1.el6.x86_64.bak/net/netfilter/ipvs/ip_vs_est.c	2009-12-03 11:51:21.000000000 +0800
+++ linux-2.6.32-220.23.1.el6.x86_64/net/netfilter/ipvs/ip_vs_est.c	2012-10-11 15:01:03.000000000 +0800
@@ -9,7 +9,8 @@
  *              2 of the License, or (at your option) any later version.
  *
  * Changes:
- *
+ *	Shunmin Zhu  <jianghe.zsm@taobao.com>
+ *	Yi Yang      <specific@gmail.com>      statistical variables U32 to U64
  */
 
 #define KMSG_COMPONENT "IPVS"
@@ -48,7 +49,6 @@
   * A lot code is taken from net/sched/estimator.c
  */
 
-
 static void estimation_timer(unsigned long arg);
 
 static LIST_HEAD(est_list);
@@ -59,10 +59,10 @@ static void estimation_timer(unsigned lo
 {
 	struct ip_vs_estimator *e;
 	struct ip_vs_stats *s;
-	u32 n_conns;
-	u32 n_inpkts, n_outpkts;
+	u64 n_conns;
+	u64 n_inpkts, n_outpkts;
 	u64 n_inbytes, n_outbytes;
-	u32 rate;
+	u64 rate;
 
 	spin_lock(&est_lock);
 	list_for_each_entry(e, &est_list, list) {
@@ -76,34 +76,34 @@ static void estimation_timer(unsigned lo
 		n_outbytes = s->ustats.outbytes;
 
 		/* scaled by 2^10, but divided 2 seconds */
-		rate = (n_conns - e->last_conns)<<9;
+		rate = (n_conns - e->last_conns) << 9;
 		e->last_conns = n_conns;
-		e->cps += ((long)rate - (long)e->cps)>>2;
-		s->ustats.cps = (e->cps+0x1FF)>>10;
+		e->cps += ((long)rate - (long)e->cps) >> 2;
+		s->ustats.cps = (e->cps + 0x1FF) >> 10;
 
-		rate = (n_inpkts - e->last_inpkts)<<9;
+		rate = (n_inpkts - e->last_inpkts) << 9;
 		e->last_inpkts = n_inpkts;
-		e->inpps += ((long)rate - (long)e->inpps)>>2;
-		s->ustats.inpps = (e->inpps+0x1FF)>>10;
+		e->inpps += ((long)rate - (long)e->inpps) >> 2;
+		s->ustats.inpps = (e->inpps + 0x1FF) >> 10;
 
-		rate = (n_outpkts - e->last_outpkts)<<9;
+		rate = (n_outpkts - e->last_outpkts) << 9;
 		e->last_outpkts = n_outpkts;
-		e->outpps += ((long)rate - (long)e->outpps)>>2;
-		s->ustats.outpps = (e->outpps+0x1FF)>>10;
+		e->outpps += ((long)rate - (long)e->outpps) >> 2;
+		s->ustats.outpps = (e->outpps + 0x1FF) >> 10;
 
-		rate = (n_inbytes - e->last_inbytes)<<4;
+		rate = (n_inbytes - e->last_inbytes) << 4;
 		e->last_inbytes = n_inbytes;
-		e->inbps += ((long)rate - (long)e->inbps)>>2;
-		s->ustats.inbps = (e->inbps+0xF)>>5;
+		e->inbps += ((long)rate - (long)e->inbps) >> 2;
+		s->ustats.inbps = (e->inbps + 0xF) >> 5;
 
-		rate = (n_outbytes - e->last_outbytes)<<4;
+		rate = (n_outbytes - e->last_outbytes) << 4;
 		e->last_outbytes = n_outbytes;
-		e->outbps += ((long)rate - (long)e->outbps)>>2;
-		s->ustats.outbps = (e->outbps+0xF)>>5;
+		e->outbps += ((long)rate - (long)e->outbps) >> 2;
+		s->ustats.outbps = (e->outbps + 0xF) >> 5;
 		spin_unlock(&s->lock);
 	}
 	spin_unlock(&est_lock);
-	mod_timer(&est_timer, jiffies + 2*HZ);
+	mod_timer(&est_timer, jiffies + 2 * HZ);
 }
 
 void ip_vs_new_estimator(struct ip_vs_stats *stats)
@@ -113,19 +113,19 @@ void ip_vs_new_estimator(struct ip_vs_st
 	INIT_LIST_HEAD(&est->list);
 
 	est->last_conns = stats->ustats.conns;
-	est->cps = stats->ustats.cps<<10;
+	est->cps = stats->ustats.cps << 10;
 
 	est->last_inpkts = stats->ustats.inpkts;
-	est->inpps = stats->ustats.inpps<<10;
+	est->inpps = stats->ustats.inpps << 10;
 
 	est->last_outpkts = stats->ustats.outpkts;
-	est->outpps = stats->ustats.outpps<<10;
+	est->outpps = stats->ustats.outpps << 10;
 
 	est->last_inbytes = stats->ustats.inbytes;
-	est->inbps = stats->ustats.inbps<<5;
+	est->inbps = (u64) (stats->ustats.inbps) << 5;
 
 	est->last_outbytes = stats->ustats.outbytes;
-	est->outbps = stats->ustats.outbps<<5;
+	est->outbps = (u64) (stats->ustats.outbps) << 5;
 
 	spin_lock_bh(&est_lock);
 	list_add(&est->list, &est_list);
diff -uprN linux-2.6.32-220.23.1.el6.x86_64.bak/net/netfilter/ipvs/ip_vs_ftp.c linux-2.6.32-220.23.1.el6.x86_64/net/netfilter/ipvs/ip_vs_ftp.c
--- linux-2.6.32-220.23.1.el6.x86_64.bak/net/netfilter/ipvs/ip_vs_ftp.c	2009-12-03 11:51:21.000000000 +0800
+++ linux-2.6.32-220.23.1.el6.x86_64/net/netfilter/ipvs/ip_vs_ftp.c	2012-10-11 15:01:03.000000000 +0800
@@ -38,38 +38,31 @@
 
 #include <net/ip_vs.h>
 
-
 #define SERVER_STRING "227 Entering Passive Mode ("
 #define CLIENT_STRING "PORT "
 
-
 /*
  * List of ports (up to IP_VS_APP_MAX_PORTS) to be handled by helper
  * First port is set to the default port.
  */
-static unsigned short ports[IP_VS_APP_MAX_PORTS] = {21, 0};
+static unsigned short ports[IP_VS_APP_MAX_PORTS] = { 21, 0 };
+
 module_param_array(ports, ushort, NULL, 0);
 MODULE_PARM_DESC(ports, "Ports to monitor for FTP control commands");
 
-
 /*	Dummy variable */
 static int ip_vs_ftp_pasv;
 
-
-static int
-ip_vs_ftp_init_conn(struct ip_vs_app *app, struct ip_vs_conn *cp)
+static int ip_vs_ftp_init_conn(struct ip_vs_app *app, struct ip_vs_conn *cp)
 {
 	return 0;
 }
 
-
-static int
-ip_vs_ftp_done_conn(struct ip_vs_app *app, struct ip_vs_conn *cp)
+static int ip_vs_ftp_done_conn(struct ip_vs_app *app, struct ip_vs_conn *cp)
 {
 	return 0;
 }
 
-
 /*
  * Get <addr,port> from the string "xxx.xxx.xxx.xxx,ppp,ppp", started
  * with the "pattern" and terminated with the "term" character.
@@ -77,7 +70,7 @@ ip_vs_ftp_done_conn(struct ip_vs_app *ap
  */
 static int ip_vs_ftp_get_addrport(char *data, char *data_limit,
 				  const char *pattern, size_t plen, char term,
-				  __be32 *addr, __be16 *port,
+				  __be32 * addr, __be16 * port,
 				  char **start, char **end)
 {
 	unsigned char p[6];
@@ -105,7 +98,7 @@ static int ip_vs_ftp_get_addrport(char *
 	memset(p, 0, sizeof(p));
 	for (data = *start; data != *end; data++) {
 		if (*data >= '0' && *data <= '9') {
-			p[i] = p[i]*10 + *data - '0';
+			p[i] = p[i] * 10 + *data - '0';
 		} else if (*data == ',' && i < 5) {
 			i++;
 		} else {
@@ -117,12 +110,11 @@ static int ip_vs_ftp_get_addrport(char *
 	if (i != 5)
 		return -1;
 
-	*addr = get_unaligned((__be32 *)p);
-	*port = get_unaligned((__be16 *)(p + 4));
+	*addr = get_unaligned((__be32 *) p);
+	*port = get_unaligned((__be16 *) (p + 4));
 	return 1;
 }
 
-
 /*
  * Look at outgoing ftp packets to catch the response to a PASV command
  * from the server (inside-to-outside).
@@ -148,7 +140,7 @@ static int ip_vs_ftp_out(struct ip_vs_ap
 	struct ip_vs_conn *n_cp;
 	char buf[24];		/* xxx.xxx.xxx.xxx,ppp,ppp\000 */
 	unsigned buf_len;
-	int ret;
+	int ret, res_dir;
 
 #ifdef CONFIG_IP_VS_IPV6
 	/* This application helper doesn't work with IPv6 yet,
@@ -170,15 +162,14 @@ static int ip_vs_ftp_out(struct ip_vs_ap
 
 	if (cp->app_data == &ip_vs_ftp_pasv) {
 		iph = ip_hdr(skb);
-		th = (struct tcphdr *)&(((char *)iph)[iph->ihl*4]);
+		th = (struct tcphdr *)&(((char *)iph)[iph->ihl * 4]);
 		data = (char *)th + (th->doff << 2);
 		data_limit = skb_tail_pointer(skb);
 
 		if (ip_vs_ftp_get_addrport(data, data_limit,
 					   SERVER_STRING,
-					   sizeof(SERVER_STRING)-1, ')',
-					   &from.ip, &port,
-					   &start, &end) != 1)
+					   sizeof(SERVER_STRING) - 1, ')',
+					   &from.ip, &port, &start, &end) != 1)
 			return 1;
 
 		IP_VS_DBG(7, "PASV response (%pI4:%d) -> %pI4:%d detected\n",
@@ -187,15 +178,15 @@ static int ip_vs_ftp_out(struct ip_vs_ap
 		/*
 		 * Now update or create an connection entry for it
 		 */
-		n_cp = ip_vs_conn_out_get(AF_INET, iph->protocol, &from, port,
-					  &cp->caddr, 0);
+		n_cp = ip_vs_conn_get(AF_INET, iph->protocol, &from, port,
+				      &cp->caddr, 0, &res_dir);
 		if (!n_cp) {
 			n_cp = ip_vs_conn_new(AF_INET, IPPROTO_TCP,
 					      &cp->caddr, 0,
 					      &cp->vaddr, port,
 					      &from, port,
 					      IP_VS_CONN_F_NO_CPORT,
-					      cp->dest);
+					      cp->dest, NULL, 0);
 			if (!n_cp)
 				return 0;
 
@@ -209,13 +200,13 @@ static int ip_vs_ftp_out(struct ip_vs_ap
 		from.ip = n_cp->vaddr.ip;
 		port = n_cp->vport;
 		sprintf(buf, "%d,%d,%d,%d,%d,%d", NIPQUAD(from.ip),
-			(ntohs(port)>>8)&255, ntohs(port)&255);
+			(ntohs(port) >> 8) & 255, ntohs(port) & 255);
 		buf_len = strlen(buf);
 
 		/*
 		 * Calculate required delta-offset to keep TCP happy
 		 */
-		*diff = buf_len - (end-start);
+		*diff = buf_len - (end - start);
 
 		if (*diff == 0) {
 			/* simply replace it with new passive address */
@@ -223,7 +214,7 @@ static int ip_vs_ftp_out(struct ip_vs_ap
 			ret = 1;
 		} else {
 			ret = !ip_vs_skb_replace(skb, GFP_ATOMIC, start,
-					  end-start, buf, buf_len);
+						 end - start, buf, buf_len);
 		}
 
 		cp->app_data = NULL;
@@ -234,8 +225,7 @@ static int ip_vs_ftp_out(struct ip_vs_ap
 	return 1;
 }
 
-
-/*
+/*/
  * Look at incoming ftp packets to catch the PASV/PORT command
  * (outside-to-inside).
  *
@@ -256,6 +246,7 @@ static int ip_vs_ftp_in(struct ip_vs_app
 	union nf_inet_addr to;
 	__be16 port;
 	struct ip_vs_conn *n_cp;
+	int res_dir;
 
 #ifdef CONFIG_IP_VS_IPV6
 	/* This application helper doesn't work with IPv6 yet,
@@ -280,7 +271,7 @@ static int ip_vs_ftp_in(struct ip_vs_app
 	 * Detecting whether it is passive
 	 */
 	iph = ip_hdr(skb);
-	th = (struct tcphdr *)&(((char *)iph)[iph->ihl*4]);
+	th = (struct tcphdr *)&(((char *)iph)[iph->ihl * 4]);
 
 	/* Since there may be OPTIONS in the TCP packet and the HLEN is
 	   the length of the header in 32-bit multiples, it is accurate
@@ -292,8 +283,7 @@ static int ip_vs_ftp_in(struct ip_vs_app
 		if (strnicmp(data, "PASV\r\n", 6) == 0) {
 			/* Passive mode on */
 			IP_VS_DBG(7, "got PASV at %td of %td\n",
-				  data - data_start,
-				  data_limit - data_start);
+				  data - data_start, data_limit - data_start);
 			cp->app_data = &ip_vs_ftp_pasv;
 			return 1;
 		}
@@ -308,9 +298,8 @@ static int ip_vs_ftp_in(struct ip_vs_app
 	 * connection.
 	 */
 	if (ip_vs_ftp_get_addrport(data_start, data_limit,
-				   CLIENT_STRING, sizeof(CLIENT_STRING)-1,
-				   '\r', &to.ip, &port,
-				   &start, &end) != 1)
+				   CLIENT_STRING, sizeof(CLIENT_STRING) - 1,
+				   '\r', &to.ip, &port, &start, &end) != 1)
 		return 1;
 
 	IP_VS_DBG(7, "PORT %pI4:%d detected\n", &to.ip, ntohs(port));
@@ -325,16 +314,16 @@ static int ip_vs_ftp_in(struct ip_vs_app
 		  ip_vs_proto_name(iph->protocol),
 		  &to.ip, ntohs(port), &cp->vaddr.ip, 0);
 
-	n_cp = ip_vs_conn_in_get(AF_INET, iph->protocol,
-				 &to, port,
-				 &cp->vaddr, htons(ntohs(cp->vport)-1));
+	n_cp = ip_vs_conn_get(AF_INET, iph->protocol,
+			      &to, port,
+			      &cp->vaddr, htons(ntohs(cp->vport) - 1),
+			      &res_dir);
 	if (!n_cp) {
 		n_cp = ip_vs_conn_new(AF_INET, IPPROTO_TCP,
 				      &to, port,
-				      &cp->vaddr, htons(ntohs(cp->vport)-1),
-				      &cp->daddr, htons(ntohs(cp->dport)-1),
-				      0,
-				      cp->dest);
+				      &cp->vaddr, htons(ntohs(cp->vport) - 1),
+				      &cp->daddr, htons(ntohs(cp->dport) - 1),
+				      0, cp->dest, NULL, 0);
 		if (!n_cp)
 			return 0;
 
@@ -343,7 +332,7 @@ static int ip_vs_ftp_in(struct ip_vs_app
 	}
 
 	/*
-	 *	Move tunnel to listen state
+	 *      Move tunnel to listen state
 	 */
 	ip_vs_tcp_conn_listen(n_cp);
 	ip_vs_conn_put(n_cp);
@@ -351,22 +340,20 @@ static int ip_vs_ftp_in(struct ip_vs_app
 	return 1;
 }
 
-
 static struct ip_vs_app ip_vs_ftp = {
-	.name =		"ftp",
-	.type =		IP_VS_APP_TYPE_FTP,
-	.protocol =	IPPROTO_TCP,
-	.module =	THIS_MODULE,
-	.incs_list =	LIST_HEAD_INIT(ip_vs_ftp.incs_list),
-	.init_conn =	ip_vs_ftp_init_conn,
-	.done_conn =	ip_vs_ftp_done_conn,
-	.bind_conn =	NULL,
-	.unbind_conn =	NULL,
-	.pkt_out =	ip_vs_ftp_out,
-	.pkt_in =	ip_vs_ftp_in,
+	.name = "ftp",
+	.type = IP_VS_APP_TYPE_FTP,
+	.protocol = IPPROTO_TCP,
+	.module = THIS_MODULE,
+	.incs_list = LIST_HEAD_INIT(ip_vs_ftp.incs_list),
+	.init_conn = ip_vs_ftp_init_conn,
+	.done_conn = ip_vs_ftp_done_conn,
+	.bind_conn = NULL,
+	.unbind_conn = NULL,
+	.pkt_out = ip_vs_ftp_out,
+	.pkt_in = ip_vs_ftp_in,
 };
 
-
 /*
  *	ip_vs_ftp initialization
  */
@@ -379,7 +366,7 @@ static int __init ip_vs_ftp_init(void)
 	if (ret)
 		return ret;
 
-	for (i=0; i<IP_VS_APP_MAX_PORTS; i++) {
+	for (i = 0; i < IP_VS_APP_MAX_PORTS; i++) {
 		if (!ports[i])
 			continue;
 		ret = register_ip_vs_app_inc(app, app->protocol, ports[i]);
@@ -395,7 +382,6 @@ static int __init ip_vs_ftp_init(void)
 	return ret;
 }
 
-
 /*
  *	ip_vs_ftp finish.
  */
@@ -404,7 +390,6 @@ static void __exit ip_vs_ftp_exit(void)
 	unregister_ip_vs_app(&ip_vs_ftp);
 }
 
-
 module_init(ip_vs_ftp_init);
 module_exit(ip_vs_ftp_exit);
 MODULE_LICENSE("GPL");
diff -uprN linux-2.6.32-220.23.1.el6.x86_64.bak/net/netfilter/ipvs/ip_vs_lblc.c linux-2.6.32-220.23.1.el6.x86_64/net/netfilter/ipvs/ip_vs_lblc.c
--- linux-2.6.32-220.23.1.el6.x86_64.bak/net/netfilter/ipvs/ip_vs_lblc.c	2009-12-03 11:51:21.000000000 +0800
+++ linux-2.6.32-220.23.1.el6.x86_64/net/netfilter/ipvs/ip_vs_lblc.c	2012-10-11 15:01:03.000000000 +0800
@@ -54,7 +54,6 @@
 
 #include <net/ip_vs.h>
 
-
 /*
  *    It is for garbage collection of stale IPVS lblc entries,
  *    when the table is full.
@@ -69,8 +68,7 @@
  *    entries that haven't been touched for a day.
  */
 #define COUNT_FOR_FULL_EXPIRATION   30
-static int sysctl_ip_vs_lblc_expiration = 24*60*60*HZ;
-
+static int sysctl_ip_vs_lblc_expiration = 24 * 60 * 60 * HZ;
 
 /*
  *     for IPVS lblc entry hash table
@@ -82,49 +80,46 @@ static int sysctl_ip_vs_lblc_expiration 
 #define IP_VS_LBLC_TAB_SIZE     (1 << IP_VS_LBLC_TAB_BITS)
 #define IP_VS_LBLC_TAB_MASK     (IP_VS_LBLC_TAB_SIZE - 1)
 
-
 /*
  *      IPVS lblc entry represents an association between destination
  *      IP address and its destination server
  */
 struct ip_vs_lblc_entry {
-	struct list_head        list;
-	int			af;		/* address family */
-	union nf_inet_addr      addr;           /* destination IP address */
-	struct ip_vs_dest       *dest;          /* real server (cache) */
-	unsigned long           lastuse;        /* last used time */
+	struct list_head list;
+	int af;			/* address family */
+	union nf_inet_addr addr;	/* destination IP address */
+	struct ip_vs_dest *dest;	/* real server (cache) */
+	unsigned long lastuse;	/* last used time */
 };
 
-
 /*
  *      IPVS lblc hash table
  */
 struct ip_vs_lblc_table {
-	struct list_head        bucket[IP_VS_LBLC_TAB_SIZE];  /* hash bucket */
-	atomic_t                entries;        /* number of entries */
-	int                     max_size;       /* maximum size of entries */
-	struct timer_list       periodic_timer; /* collect stale entries */
-	int                     rover;          /* rover for expire check */
-	int                     counter;        /* counter for no expire */
+	struct list_head bucket[IP_VS_LBLC_TAB_SIZE];	/* hash bucket */
+	atomic_t entries;	/* number of entries */
+	int max_size;		/* maximum size of entries */
+	struct timer_list periodic_timer;	/* collect stale entries */
+	int rover;		/* rover for expire check */
+	int counter;		/* counter for no expire */
 };
 
-
 /*
  *      IPVS LBLC sysctl table
  */
 
 static ctl_table vs_vars_table[] = {
 	{
-		.procname	= "lblc_expiration",
-		.data		= &sysctl_ip_vs_lblc_expiration,
-		.maxlen		= sizeof(int),
-		.mode		= 0644,
-		.proc_handler	= proc_dointvec_jiffies,
-	},
-	{ .ctl_name = 0 }
+	 .procname = "lblc_expiration",
+	 .data = &sysctl_ip_vs_lblc_expiration,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = proc_dointvec_jiffies,
+	 },
+	{.ctl_name = 0}
 };
 
-static struct ctl_table_header * sysctl_header;
+static struct ctl_table_header *sysctl_header;
 
 static inline void ip_vs_lblc_free(struct ip_vs_lblc_entry *en)
 {
@@ -137,7 +132,6 @@ static inline void ip_vs_lblc_free(struc
 	kfree(en);
 }
 
-
 /*
  *	Returns hash value for IPVS LBLC entry
  */
@@ -148,13 +142,12 @@ ip_vs_lblc_hashkey(int af, const union n
 
 #ifdef CONFIG_IP_VS_IPV6
 	if (af == AF_INET6)
-		addr_fold = addr->ip6[0]^addr->ip6[1]^
-			    addr->ip6[2]^addr->ip6[3];
+		addr_fold = addr->ip6[0] ^ addr->ip6[1] ^
+		    addr->ip6[2] ^ addr->ip6[3];
 #endif
-	return (ntohl(addr_fold)*2654435761UL) & IP_VS_LBLC_TAB_MASK;
+	return (ntohl(addr_fold) * 2654435761UL) & IP_VS_LBLC_TAB_MASK;
 }
 
-
 /*
  *	Hash an entry in the ip_vs_lblc_table.
  *	returns bool success.
@@ -168,33 +161,35 @@ ip_vs_lblc_hash(struct ip_vs_lblc_table 
 	atomic_inc(&tbl->entries);
 }
 
-
 /*
  *  Get ip_vs_lblc_entry associated with supplied parameters. Called under read
  *  lock
  */
-static inline struct ip_vs_lblc_entry *
-ip_vs_lblc_get(int af, struct ip_vs_lblc_table *tbl,
-	       const union nf_inet_addr *addr)
+static inline struct ip_vs_lblc_entry *ip_vs_lblc_get(int af,
+						      struct ip_vs_lblc_table
+						      *tbl,
+						      const union nf_inet_addr
+						      *addr)
 {
 	unsigned hash = ip_vs_lblc_hashkey(af, addr);
 	struct ip_vs_lblc_entry *en;
 
 	list_for_each_entry(en, &tbl->bucket[hash], list)
-		if (ip_vs_addr_equal(af, &en->addr, addr))
-			return en;
+	    if (ip_vs_addr_equal(af, &en->addr, addr))
+		return en;
 
 	return NULL;
 }
 
-
 /*
  * Create or update an ip_vs_lblc_entry, which is a mapping of a destination IP
  * address to a server. Called under write lock.
  */
-static inline struct ip_vs_lblc_entry *
-ip_vs_lblc_new(struct ip_vs_lblc_table *tbl, const union nf_inet_addr *daddr,
-	       struct ip_vs_dest *dest)
+static inline struct ip_vs_lblc_entry *ip_vs_lblc_new(struct ip_vs_lblc_table
+						      *tbl,
+						      const union nf_inet_addr
+						      *daddr,
+						      struct ip_vs_dest *dest)
 {
 	struct ip_vs_lblc_entry *en;
 
@@ -223,7 +218,6 @@ ip_vs_lblc_new(struct ip_vs_lblc_table *
 	return en;
 }
 
-
 /*
  *      Flush all the entries of the specified table.
  */
@@ -232,7 +226,7 @@ static void ip_vs_lblc_flush(struct ip_v
 	struct ip_vs_lblc_entry *en, *nxt;
 	int i;
 
-	for (i=0; i<IP_VS_LBLC_TAB_SIZE; i++) {
+	for (i = 0; i < IP_VS_LBLC_TAB_SIZE; i++) {
 		list_for_each_entry_safe(en, nxt, &tbl->bucket[i], list) {
 			ip_vs_lblc_free(en);
 			atomic_dec(&tbl->entries);
@@ -240,7 +234,6 @@ static void ip_vs_lblc_flush(struct ip_v
 	}
 }
 
-
 static inline void ip_vs_lblc_full_check(struct ip_vs_service *svc)
 {
 	struct ip_vs_lblc_table *tbl = svc->sched_data;
@@ -248,13 +241,14 @@ static inline void ip_vs_lblc_full_check
 	unsigned long now = jiffies;
 	int i, j;
 
-	for (i=0, j=tbl->rover; i<IP_VS_LBLC_TAB_SIZE; i++) {
+	for (i = 0, j = tbl->rover; i < IP_VS_LBLC_TAB_SIZE; i++) {
 		j = (j + 1) & IP_VS_LBLC_TAB_MASK;
 
 		write_lock(&svc->sched_lock);
 		list_for_each_entry_safe(en, nxt, &tbl->bucket[j], list) {
 			if (time_before(now,
-					en->lastuse + sysctl_ip_vs_lblc_expiration))
+					en->lastuse +
+					sysctl_ip_vs_lblc_expiration))
 				continue;
 
 			ip_vs_lblc_free(en);
@@ -265,7 +259,6 @@ static inline void ip_vs_lblc_full_check
 	tbl->rover = j;
 }
 
-
 /*
  *      Periodical timer handler for IPVS lblc table
  *      It is used to collect stale entries when the number of entries
@@ -279,7 +272,7 @@ static inline void ip_vs_lblc_full_check
  */
 static void ip_vs_lblc_check_expire(unsigned long data)
 {
-	struct ip_vs_service *svc = (struct ip_vs_service *) data;
+	struct ip_vs_service *svc = (struct ip_vs_service *)data;
 	struct ip_vs_lblc_table *tbl = svc->sched_data;
 	unsigned long now = jiffies;
 	int goal;
@@ -298,11 +291,11 @@ static void ip_vs_lblc_check_expire(unsi
 		goto out;
 	}
 
-	goal = (atomic_read(&tbl->entries) - tbl->max_size)*4/3;
-	if (goal > tbl->max_size/2)
-		goal = tbl->max_size/2;
+	goal = (atomic_read(&tbl->entries) - tbl->max_size) * 4 / 3;
+	if (goal > tbl->max_size / 2)
+		goal = tbl->max_size / 2;
 
-	for (i=0, j=tbl->rover; i<IP_VS_LBLC_TAB_SIZE; i++) {
+	for (i = 0, j = tbl->rover; i < IP_VS_LBLC_TAB_SIZE; i++) {
 		j = (j + 1) & IP_VS_LBLC_TAB_MASK;
 
 		write_lock(&svc->sched_lock);
@@ -320,11 +313,10 @@ static void ip_vs_lblc_check_expire(unsi
 	}
 	tbl->rover = j;
 
-  out:
-	mod_timer(&tbl->periodic_timer, jiffies+CHECK_EXPIRE_INTERVAL);
+      out:
+	mod_timer(&tbl->periodic_timer, jiffies + CHECK_EXPIRE_INTERVAL);
 }
 
-
 static int ip_vs_lblc_init_svc(struct ip_vs_service *svc)
 {
 	int i;
@@ -345,10 +337,10 @@ static int ip_vs_lblc_init_svc(struct ip
 	/*
 	 *    Initialize the hash buckets
 	 */
-	for (i=0; i<IP_VS_LBLC_TAB_SIZE; i++) {
+	for (i = 0; i < IP_VS_LBLC_TAB_SIZE; i++) {
 		INIT_LIST_HEAD(&tbl->bucket[i]);
 	}
-	tbl->max_size = IP_VS_LBLC_TAB_SIZE*16;
+	tbl->max_size = IP_VS_LBLC_TAB_SIZE * 16;
 	tbl->rover = 0;
 	tbl->counter = 1;
 
@@ -356,13 +348,12 @@ static int ip_vs_lblc_init_svc(struct ip
 	 *    Hook periodic timer for garbage collection
 	 */
 	setup_timer(&tbl->periodic_timer, ip_vs_lblc_check_expire,
-			(unsigned long)svc);
+		    (unsigned long)svc);
 	mod_timer(&tbl->periodic_timer, jiffies + CHECK_EXPIRE_INTERVAL);
 
 	return 0;
 }
 
-
 static int ip_vs_lblc_done_svc(struct ip_vs_service *svc)
 {
 	struct ip_vs_lblc_table *tbl = svc->sched_data;
@@ -381,9 +372,8 @@ static int ip_vs_lblc_done_svc(struct ip
 	return 0;
 }
 
-
-static inline struct ip_vs_dest *
-__ip_vs_lblc_schedule(struct ip_vs_service *svc)
+static inline struct ip_vs_dest *__ip_vs_lblc_schedule(struct ip_vs_service
+						       *svc)
 {
 	struct ip_vs_dest *dest, *least;
 	int loh, doh;
@@ -411,7 +401,7 @@ __ip_vs_lblc_schedule(struct ip_vs_servi
 		if (atomic_read(&dest->weight) > 0) {
 			least = dest;
 			loh = atomic_read(&least->activeconns) * 50
-				+ atomic_read(&least->inactconns);
+			    + atomic_read(&least->inactconns);
 			goto nextstage;
 		}
 	}
@@ -420,13 +410,13 @@ __ip_vs_lblc_schedule(struct ip_vs_servi
 	/*
 	 *    Find the destination with the least load.
 	 */
-  nextstage:
+      nextstage:
 	list_for_each_entry_continue(dest, &svc->destinations, n_list) {
 		if (dest->flags & IP_VS_DEST_F_OVERLOAD)
 			continue;
 
 		doh = atomic_read(&dest->activeconns) * 50
-			+ atomic_read(&dest->inactconns);
+		    + atomic_read(&dest->inactconns);
 		if (loh * atomic_read(&dest->weight) >
 		    doh * atomic_read(&least->weight)) {
 			least = dest;
@@ -445,7 +435,6 @@ __ip_vs_lblc_schedule(struct ip_vs_servi
 	return least;
 }
 
-
 /*
  *   If this destination server is overloaded and there is a less loaded
  *   server, then return true.
@@ -457,7 +446,7 @@ is_overloaded(struct ip_vs_dest *dest, s
 		struct ip_vs_dest *d;
 
 		list_for_each_entry(d, &svc->destinations, n_list) {
-			if (atomic_read(&d->activeconns)*2
+			if (atomic_read(&d->activeconns) * 2
 			    < atomic_read(&d->weight)) {
 				return 1;
 			}
@@ -466,12 +455,11 @@ is_overloaded(struct ip_vs_dest *dest, s
 	return 0;
 }
 
-
 /*
  *    Locality-Based (weighted) Least-Connection scheduling
  */
-static struct ip_vs_dest *
-ip_vs_lblc_schedule(struct ip_vs_service *svc, const struct sk_buff *skb)
+static struct ip_vs_dest *ip_vs_lblc_schedule(struct ip_vs_service *svc,
+					      const struct sk_buff *skb)
 {
 	struct ip_vs_lblc_table *tbl = svc->sched_data;
 	struct ip_vs_iphdr iph;
@@ -519,7 +507,7 @@ ip_vs_lblc_schedule(struct ip_vs_service
 	ip_vs_lblc_new(tbl, &iph.daddr, dest);
 	write_unlock(&svc->sched_lock);
 
-out:
+      out:
 	IP_VS_DBG_BUF(6, "LBLC: destination IP address %s --> server %s:%d\n",
 		      IP_VS_DBG_ADDR(svc->af, &iph.daddr),
 		      IP_VS_DBG_ADDR(svc->af, &dest->addr), ntohs(dest->port));
@@ -527,22 +515,19 @@ out:
 	return dest;
 }
 
-
 /*
  *      IPVS LBLC Scheduler structure
  */
-static struct ip_vs_scheduler ip_vs_lblc_scheduler =
-{
-	.name =			"lblc",
-	.refcnt =		ATOMIC_INIT(0),
-	.module =		THIS_MODULE,
-	.n_list =		LIST_HEAD_INIT(ip_vs_lblc_scheduler.n_list),
-	.init_service =		ip_vs_lblc_init_svc,
-	.done_service =		ip_vs_lblc_done_svc,
-	.schedule =		ip_vs_lblc_schedule,
+static struct ip_vs_scheduler ip_vs_lblc_scheduler = {
+	.name = "lblc",
+	.refcnt = ATOMIC_INIT(0),
+	.module = THIS_MODULE,
+	.n_list = LIST_HEAD_INIT(ip_vs_lblc_scheduler.n_list),
+	.init_service = ip_vs_lblc_init_svc,
+	.done_service = ip_vs_lblc_done_svc,
+	.schedule = ip_vs_lblc_schedule,
 };
 
-
 static int __init ip_vs_lblc_init(void)
 {
 	int ret;
@@ -554,14 +539,12 @@ static int __init ip_vs_lblc_init(void)
 	return ret;
 }
 
-
 static void __exit ip_vs_lblc_cleanup(void)
 {
 	unregister_sysctl_table(sysctl_header);
 	unregister_ip_vs_scheduler(&ip_vs_lblc_scheduler);
 }
 
-
 module_init(ip_vs_lblc_init);
 module_exit(ip_vs_lblc_cleanup);
 MODULE_LICENSE("GPL");
diff -uprN linux-2.6.32-220.23.1.el6.x86_64.bak/net/netfilter/ipvs/ip_vs_lblcr.c linux-2.6.32-220.23.1.el6.x86_64/net/netfilter/ipvs/ip_vs_lblcr.c
--- linux-2.6.32-220.23.1.el6.x86_64.bak/net/netfilter/ipvs/ip_vs_lblcr.c	2009-12-03 11:51:21.000000000 +0800
+++ linux-2.6.32-220.23.1.el6.x86_64/net/netfilter/ipvs/ip_vs_lblcr.c	2012-10-11 15:01:03.000000000 +0800
@@ -53,7 +53,6 @@
 
 #include <net/ip_vs.h>
 
-
 /*
  *    It is for garbage collection of stale IPVS lblcr entries,
  *    when the table is full.
@@ -68,8 +67,7 @@
  *    entries that haven't been touched for a day.
  */
 #define COUNT_FOR_FULL_EXPIRATION   30
-static int sysctl_ip_vs_lblcr_expiration = 24*60*60*HZ;
-
+static int sysctl_ip_vs_lblcr_expiration = 24 * 60 * 60 * HZ;
 
 /*
  *     for IPVS lblcr entry hash table
@@ -81,29 +79,27 @@ static int sysctl_ip_vs_lblcr_expiration
 #define IP_VS_LBLCR_TAB_SIZE     (1 << IP_VS_LBLCR_TAB_BITS)
 #define IP_VS_LBLCR_TAB_MASK     (IP_VS_LBLCR_TAB_SIZE - 1)
 
-
 /*
  *      IPVS destination set structure and operations
  */
 struct ip_vs_dest_list {
-	struct ip_vs_dest_list  *next;          /* list link */
-	struct ip_vs_dest       *dest;          /* destination server */
+	struct ip_vs_dest_list *next;	/* list link */
+	struct ip_vs_dest *dest;	/* destination server */
 };
 
 struct ip_vs_dest_set {
-	atomic_t                size;           /* set size */
-	unsigned long           lastmod;        /* last modified time */
-	struct ip_vs_dest_list  *list;          /* destination list */
-	rwlock_t	        lock;           /* lock for this list */
+	atomic_t size;		/* set size */
+	unsigned long lastmod;	/* last modified time */
+	struct ip_vs_dest_list *list;	/* destination list */
+	rwlock_t lock;		/* lock for this list */
 };
 
-
-static struct ip_vs_dest_list *
-ip_vs_dest_set_insert(struct ip_vs_dest_set *set, struct ip_vs_dest *dest)
+static struct ip_vs_dest_list *ip_vs_dest_set_insert(struct ip_vs_dest_set *set,
+						     struct ip_vs_dest *dest)
 {
 	struct ip_vs_dest_list *e;
 
-	for (e=set->list; e!=NULL; e=e->next) {
+	for (e = set->list; e != NULL; e = e->next) {
 		if (e->dest == dest)
 			/* already existed */
 			return NULL;
@@ -132,7 +128,7 @@ ip_vs_dest_set_erase(struct ip_vs_dest_s
 {
 	struct ip_vs_dest_list *e, **ep;
 
-	for (ep=&set->list, e=*ep; e!=NULL; e=*ep) {
+	for (ep = &set->list, e = *ep; e != NULL; e = *ep) {
 		if (e->dest == dest) {
 			/* HIT */
 			*ep = e->next;
@@ -151,7 +147,7 @@ static void ip_vs_dest_set_eraseall(stru
 	struct ip_vs_dest_list *e, **ep;
 
 	write_lock(&set->lock);
-	for (ep=&set->list, e=*ep; e!=NULL; e=*ep) {
+	for (ep = &set->list, e = *ep; e != NULL; e = *ep) {
 		*ep = e->next;
 		/*
 		 * We don't kfree dest because it is refered either
@@ -174,7 +170,7 @@ static inline struct ip_vs_dest *ip_vs_d
 		return NULL;
 
 	/* select the first destination server, whose weight > 0 */
-	for (e=set->list; e!=NULL; e=e->next) {
+	for (e = set->list; e != NULL; e = e->next) {
 		least = e->dest;
 		if (least->flags & IP_VS_DEST_F_OVERLOAD)
 			continue;
@@ -182,21 +178,21 @@ static inline struct ip_vs_dest *ip_vs_d
 		if ((atomic_read(&least->weight) > 0)
 		    && (least->flags & IP_VS_DEST_F_AVAILABLE)) {
 			loh = atomic_read(&least->activeconns) * 50
-				+ atomic_read(&least->inactconns);
+			    + atomic_read(&least->inactconns);
 			goto nextstage;
 		}
 	}
 	return NULL;
 
 	/* find the destination with the weighted least load */
-  nextstage:
-	for (e=e->next; e!=NULL; e=e->next) {
+      nextstage:
+	for (e = e->next; e != NULL; e = e->next) {
 		dest = e->dest;
 		if (dest->flags & IP_VS_DEST_F_OVERLOAD)
 			continue;
 
 		doh = atomic_read(&dest->activeconns) * 50
-			+ atomic_read(&dest->inactconns);
+		    + atomic_read(&dest->inactconns);
 		if ((loh * atomic_read(&dest->weight) >
 		     doh * atomic_read(&least->weight))
 		    && (dest->flags & IP_VS_DEST_F_AVAILABLE)) {
@@ -216,7 +212,6 @@ static inline struct ip_vs_dest *ip_vs_d
 	return least;
 }
 
-
 /* get weighted most-connection node in the destination set */
 static inline struct ip_vs_dest *ip_vs_dest_set_max(struct ip_vs_dest_set *set)
 {
@@ -228,22 +223,22 @@ static inline struct ip_vs_dest *ip_vs_d
 		return NULL;
 
 	/* select the first destination server, whose weight > 0 */
-	for (e=set->list; e!=NULL; e=e->next) {
+	for (e = set->list; e != NULL; e = e->next) {
 		most = e->dest;
 		if (atomic_read(&most->weight) > 0) {
 			moh = atomic_read(&most->activeconns) * 50
-				+ atomic_read(&most->inactconns);
+			    + atomic_read(&most->inactconns);
 			goto nextstage;
 		}
 	}
 	return NULL;
 
 	/* find the destination with the weighted most load */
-  nextstage:
-	for (e=e->next; e!=NULL; e=e->next) {
+      nextstage:
+	for (e = e->next; e != NULL; e = e->next) {
 		dest = e->dest;
 		doh = atomic_read(&dest->activeconns) * 50
-			+ atomic_read(&dest->inactconns);
+		    + atomic_read(&dest->inactconns);
 		/* moh/mw < doh/dw ==> moh*dw < doh*mw, where mw,dw>0 */
 		if ((moh * atomic_read(&dest->weight) <
 		     doh * atomic_read(&most->weight))
@@ -263,49 +258,46 @@ static inline struct ip_vs_dest *ip_vs_d
 	return most;
 }
 
-
 /*
  *      IPVS lblcr entry represents an association between destination
  *      IP address and its destination server set
  */
 struct ip_vs_lblcr_entry {
-	struct list_head        list;
-	int			af;		/* address family */
-	union nf_inet_addr      addr;           /* destination IP address */
-	struct ip_vs_dest_set   set;            /* destination server set */
-	unsigned long           lastuse;        /* last used time */
+	struct list_head list;
+	int af;			/* address family */
+	union nf_inet_addr addr;	/* destination IP address */
+	struct ip_vs_dest_set set;	/* destination server set */
+	unsigned long lastuse;	/* last used time */
 };
 
-
 /*
  *      IPVS lblcr hash table
  */
 struct ip_vs_lblcr_table {
-	struct list_head        bucket[IP_VS_LBLCR_TAB_SIZE];  /* hash bucket */
-	atomic_t                entries;        /* number of entries */
-	int                     max_size;       /* maximum size of entries */
-	struct timer_list       periodic_timer; /* collect stale entries */
-	int                     rover;          /* rover for expire check */
-	int                     counter;        /* counter for no expire */
+	struct list_head bucket[IP_VS_LBLCR_TAB_SIZE];	/* hash bucket */
+	atomic_t entries;	/* number of entries */
+	int max_size;		/* maximum size of entries */
+	struct timer_list periodic_timer;	/* collect stale entries */
+	int rover;		/* rover for expire check */
+	int counter;		/* counter for no expire */
 };
 
-
 /*
  *      IPVS LBLCR sysctl table
  */
 
 static ctl_table vs_vars_table[] = {
 	{
-		.procname	= "lblcr_expiration",
-		.data		= &sysctl_ip_vs_lblcr_expiration,
-		.maxlen		= sizeof(int),
-		.mode		= 0644,
-		.proc_handler	= proc_dointvec_jiffies,
-	},
-	{ .ctl_name = 0 }
+	 .procname = "lblcr_expiration",
+	 .data = &sysctl_ip_vs_lblcr_expiration,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = proc_dointvec_jiffies,
+	 },
+	{.ctl_name = 0}
 };
 
-static struct ctl_table_header * sysctl_header;
+static struct ctl_table_header *sysctl_header;
 
 static inline void ip_vs_lblcr_free(struct ip_vs_lblcr_entry *en)
 {
@@ -314,7 +306,6 @@ static inline void ip_vs_lblcr_free(stru
 	kfree(en);
 }
 
-
 /*
  *	Returns hash value for IPVS LBLCR entry
  */
@@ -325,13 +316,12 @@ ip_vs_lblcr_hashkey(int af, const union 
 
 #ifdef CONFIG_IP_VS_IPV6
 	if (af == AF_INET6)
-		addr_fold = addr->ip6[0]^addr->ip6[1]^
-			    addr->ip6[2]^addr->ip6[3];
+		addr_fold = addr->ip6[0] ^ addr->ip6[1] ^
+		    addr->ip6[2] ^ addr->ip6[3];
 #endif
-	return (ntohl(addr_fold)*2654435761UL) & IP_VS_LBLCR_TAB_MASK;
+	return (ntohl(addr_fold) * 2654435761UL) & IP_VS_LBLCR_TAB_MASK;
 }
 
-
 /*
  *	Hash an entry in the ip_vs_lblcr_table.
  *	returns bool success.
@@ -345,33 +335,35 @@ ip_vs_lblcr_hash(struct ip_vs_lblcr_tabl
 	atomic_inc(&tbl->entries);
 }
 
-
 /*
  *  Get ip_vs_lblcr_entry associated with supplied parameters. Called under
  *  read lock.
  */
-static inline struct ip_vs_lblcr_entry *
-ip_vs_lblcr_get(int af, struct ip_vs_lblcr_table *tbl,
-		const union nf_inet_addr *addr)
+static inline struct ip_vs_lblcr_entry *ip_vs_lblcr_get(int af,
+							struct ip_vs_lblcr_table
+							*tbl,
+							const union nf_inet_addr
+							*addr)
 {
 	unsigned hash = ip_vs_lblcr_hashkey(af, addr);
 	struct ip_vs_lblcr_entry *en;
 
 	list_for_each_entry(en, &tbl->bucket[hash], list)
-		if (ip_vs_addr_equal(af, &en->addr, addr))
-			return en;
+	    if (ip_vs_addr_equal(af, &en->addr, addr))
+		return en;
 
 	return NULL;
 }
 
-
 /*
  * Create or update an ip_vs_lblcr_entry, which is a mapping of a destination
  * IP address to a server. Called under write lock.
  */
-static inline struct ip_vs_lblcr_entry *
-ip_vs_lblcr_new(struct ip_vs_lblcr_table *tbl, const union nf_inet_addr *daddr,
-		struct ip_vs_dest *dest)
+static inline struct ip_vs_lblcr_entry *ip_vs_lblcr_new(struct ip_vs_lblcr_table
+							*tbl,
+							const union nf_inet_addr
+							*daddr,
+							struct ip_vs_dest *dest)
 {
 	struct ip_vs_lblcr_entry *en;
 
@@ -402,7 +394,6 @@ ip_vs_lblcr_new(struct ip_vs_lblcr_table
 	return en;
 }
 
-
 /*
  *      Flush all the entries of the specified table.
  */
@@ -412,14 +403,13 @@ static void ip_vs_lblcr_flush(struct ip_
 	struct ip_vs_lblcr_entry *en, *nxt;
 
 	/* No locking required, only called during cleanup. */
-	for (i=0; i<IP_VS_LBLCR_TAB_SIZE; i++) {
+	for (i = 0; i < IP_VS_LBLCR_TAB_SIZE; i++) {
 		list_for_each_entry_safe(en, nxt, &tbl->bucket[i], list) {
 			ip_vs_lblcr_free(en);
 		}
 	}
 }
 
-
 static inline void ip_vs_lblcr_full_check(struct ip_vs_service *svc)
 {
 	struct ip_vs_lblcr_table *tbl = svc->sched_data;
@@ -427,13 +417,13 @@ static inline void ip_vs_lblcr_full_chec
 	int i, j;
 	struct ip_vs_lblcr_entry *en, *nxt;
 
-	for (i=0, j=tbl->rover; i<IP_VS_LBLCR_TAB_SIZE; i++) {
+	for (i = 0, j = tbl->rover; i < IP_VS_LBLCR_TAB_SIZE; i++) {
 		j = (j + 1) & IP_VS_LBLCR_TAB_MASK;
 
 		write_lock(&svc->sched_lock);
 		list_for_each_entry_safe(en, nxt, &tbl->bucket[j], list) {
-			if (time_after(en->lastuse+sysctl_ip_vs_lblcr_expiration,
-				       now))
+			if (time_after
+			    (en->lastuse + sysctl_ip_vs_lblcr_expiration, now))
 				continue;
 
 			ip_vs_lblcr_free(en);
@@ -444,7 +434,6 @@ static inline void ip_vs_lblcr_full_chec
 	tbl->rover = j;
 }
 
-
 /*
  *      Periodical timer handler for IPVS lblcr table
  *      It is used to collect stale entries when the number of entries
@@ -458,7 +447,7 @@ static inline void ip_vs_lblcr_full_chec
  */
 static void ip_vs_lblcr_check_expire(unsigned long data)
 {
-	struct ip_vs_service *svc = (struct ip_vs_service *) data;
+	struct ip_vs_service *svc = (struct ip_vs_service *)data;
 	struct ip_vs_lblcr_table *tbl = svc->sched_data;
 	unsigned long now = jiffies;
 	int goal;
@@ -477,16 +466,16 @@ static void ip_vs_lblcr_check_expire(uns
 		goto out;
 	}
 
-	goal = (atomic_read(&tbl->entries) - tbl->max_size)*4/3;
-	if (goal > tbl->max_size/2)
-		goal = tbl->max_size/2;
+	goal = (atomic_read(&tbl->entries) - tbl->max_size) * 4 / 3;
+	if (goal > tbl->max_size / 2)
+		goal = tbl->max_size / 2;
 
-	for (i=0, j=tbl->rover; i<IP_VS_LBLCR_TAB_SIZE; i++) {
+	for (i = 0, j = tbl->rover; i < IP_VS_LBLCR_TAB_SIZE; i++) {
 		j = (j + 1) & IP_VS_LBLCR_TAB_MASK;
 
 		write_lock(&svc->sched_lock);
 		list_for_each_entry_safe(en, nxt, &tbl->bucket[j], list) {
-			if (time_before(now, en->lastuse+ENTRY_TIMEOUT))
+			if (time_before(now, en->lastuse + ENTRY_TIMEOUT))
 				continue;
 
 			ip_vs_lblcr_free(en);
@@ -499,8 +488,8 @@ static void ip_vs_lblcr_check_expire(uns
 	}
 	tbl->rover = j;
 
-  out:
-	mod_timer(&tbl->periodic_timer, jiffies+CHECK_EXPIRE_INTERVAL);
+      out:
+	mod_timer(&tbl->periodic_timer, jiffies + CHECK_EXPIRE_INTERVAL);
 }
 
 static int ip_vs_lblcr_init_svc(struct ip_vs_service *svc)
@@ -523,10 +512,10 @@ static int ip_vs_lblcr_init_svc(struct i
 	/*
 	 *    Initialize the hash buckets
 	 */
-	for (i=0; i<IP_VS_LBLCR_TAB_SIZE; i++) {
+	for (i = 0; i < IP_VS_LBLCR_TAB_SIZE; i++) {
 		INIT_LIST_HEAD(&tbl->bucket[i]);
 	}
-	tbl->max_size = IP_VS_LBLCR_TAB_SIZE*16;
+	tbl->max_size = IP_VS_LBLCR_TAB_SIZE * 16;
 	tbl->rover = 0;
 	tbl->counter = 1;
 
@@ -534,13 +523,12 @@ static int ip_vs_lblcr_init_svc(struct i
 	 *    Hook periodic timer for garbage collection
 	 */
 	setup_timer(&tbl->periodic_timer, ip_vs_lblcr_check_expire,
-			(unsigned long)svc);
+		    (unsigned long)svc);
 	mod_timer(&tbl->periodic_timer, jiffies + CHECK_EXPIRE_INTERVAL);
 
 	return 0;
 }
 
-
 static int ip_vs_lblcr_done_svc(struct ip_vs_service *svc)
 {
 	struct ip_vs_lblcr_table *tbl = svc->sched_data;
@@ -559,9 +547,8 @@ static int ip_vs_lblcr_done_svc(struct i
 	return 0;
 }
 
-
-static inline struct ip_vs_dest *
-__ip_vs_lblcr_schedule(struct ip_vs_service *svc)
+static inline struct ip_vs_dest *__ip_vs_lblcr_schedule(struct ip_vs_service
+							*svc)
 {
 	struct ip_vs_dest *dest, *least;
 	int loh, doh;
@@ -590,7 +577,7 @@ __ip_vs_lblcr_schedule(struct ip_vs_serv
 		if (atomic_read(&dest->weight) > 0) {
 			least = dest;
 			loh = atomic_read(&least->activeconns) * 50
-				+ atomic_read(&least->inactconns);
+			    + atomic_read(&least->inactconns);
 			goto nextstage;
 		}
 	}
@@ -599,13 +586,13 @@ __ip_vs_lblcr_schedule(struct ip_vs_serv
 	/*
 	 *    Find the destination with the least load.
 	 */
-  nextstage:
+      nextstage:
 	list_for_each_entry_continue(dest, &svc->destinations, n_list) {
 		if (dest->flags & IP_VS_DEST_F_OVERLOAD)
 			continue;
 
 		doh = atomic_read(&dest->activeconns) * 50
-			+ atomic_read(&dest->inactconns);
+		    + atomic_read(&dest->inactconns);
 		if (loh * atomic_read(&dest->weight) >
 		    doh * atomic_read(&least->weight)) {
 			least = dest;
@@ -624,7 +611,6 @@ __ip_vs_lblcr_schedule(struct ip_vs_serv
 	return least;
 }
 
-
 /*
  *   If this destination server is overloaded and there is a less loaded
  *   server, then return true.
@@ -636,7 +622,7 @@ is_overloaded(struct ip_vs_dest *dest, s
 		struct ip_vs_dest *d;
 
 		list_for_each_entry(d, &svc->destinations, n_list) {
-			if (atomic_read(&d->activeconns)*2
+			if (atomic_read(&d->activeconns) * 2
 			    < atomic_read(&d->weight)) {
 				return 1;
 			}
@@ -645,12 +631,11 @@ is_overloaded(struct ip_vs_dest *dest, s
 	return 0;
 }
 
-
 /*
  *    Locality-Based (weighted) Least-Connection scheduling
  */
-static struct ip_vs_dest *
-ip_vs_lblcr_schedule(struct ip_vs_service *svc, const struct sk_buff *skb)
+static struct ip_vs_dest *ip_vs_lblcr_schedule(struct ip_vs_service *svc,
+					       const struct sk_buff *skb)
 {
 	struct ip_vs_lblcr_table *tbl = svc->sched_data;
 	struct ip_vs_iphdr iph;
@@ -675,8 +660,8 @@ ip_vs_lblcr_schedule(struct ip_vs_servic
 
 		/* More than one destination + enough time passed by, cleanup */
 		if (atomic_read(&en->set.size) > 1 &&
-				time_after(jiffies, en->set.lastmod +
-				sysctl_ip_vs_lblcr_expiration)) {
+		    time_after(jiffies, en->set.lastmod +
+			       sysctl_ip_vs_lblcr_expiration)) {
 			struct ip_vs_dest *m;
 
 			write_lock(&en->set.lock);
@@ -722,7 +707,7 @@ ip_vs_lblcr_schedule(struct ip_vs_servic
 	ip_vs_lblcr_new(tbl, &iph.daddr, dest);
 	write_unlock(&svc->sched_lock);
 
-out:
+      out:
 	IP_VS_DBG_BUF(6, "LBLCR: destination IP address %s --> server %s:%d\n",
 		      IP_VS_DBG_ADDR(svc->af, &iph.daddr),
 		      IP_VS_DBG_ADDR(svc->af, &dest->addr), ntohs(dest->port));
@@ -730,22 +715,19 @@ out:
 	return dest;
 }
 
-
 /*
  *      IPVS LBLCR Scheduler structure
  */
-static struct ip_vs_scheduler ip_vs_lblcr_scheduler =
-{
-	.name =			"lblcr",
-	.refcnt =		ATOMIC_INIT(0),
-	.module =		THIS_MODULE,
-	.n_list =		LIST_HEAD_INIT(ip_vs_lblcr_scheduler.n_list),
-	.init_service =		ip_vs_lblcr_init_svc,
-	.done_service =		ip_vs_lblcr_done_svc,
-	.schedule =		ip_vs_lblcr_schedule,
+static struct ip_vs_scheduler ip_vs_lblcr_scheduler = {
+	.name = "lblcr",
+	.refcnt = ATOMIC_INIT(0),
+	.module = THIS_MODULE,
+	.n_list = LIST_HEAD_INIT(ip_vs_lblcr_scheduler.n_list),
+	.init_service = ip_vs_lblcr_init_svc,
+	.done_service = ip_vs_lblcr_done_svc,
+	.schedule = ip_vs_lblcr_schedule,
 };
 
-
 static int __init ip_vs_lblcr_init(void)
 {
 	int ret;
@@ -757,14 +739,12 @@ static int __init ip_vs_lblcr_init(void)
 	return ret;
 }
 
-
 static void __exit ip_vs_lblcr_cleanup(void)
 {
 	unregister_sysctl_table(sysctl_header);
 	unregister_ip_vs_scheduler(&ip_vs_lblcr_scheduler);
 }
 
-
 module_init(ip_vs_lblcr_init);
 module_exit(ip_vs_lblcr_cleanup);
 MODULE_LICENSE("GPL");
diff -uprN linux-2.6.32-220.23.1.el6.x86_64.bak/net/netfilter/ipvs/ip_vs_lc.c linux-2.6.32-220.23.1.el6.x86_64/net/netfilter/ipvs/ip_vs_lc.c
--- linux-2.6.32-220.23.1.el6.x86_64.bak/net/netfilter/ipvs/ip_vs_lc.c	2009-12-03 11:51:21.000000000 +0800
+++ linux-2.6.32-220.23.1.el6.x86_64/net/netfilter/ipvs/ip_vs_lc.c	2012-10-11 15:01:03.000000000 +0800
@@ -22,27 +22,24 @@
 
 #include <net/ip_vs.h>
 
-
-static inline unsigned int
-ip_vs_lc_dest_overhead(struct ip_vs_dest *dest)
+static inline unsigned int ip_vs_lc_dest_overhead(struct ip_vs_dest *dest)
 {
 	/*
 	 * We think the overhead of processing active connections is 256
 	 * times higher than that of inactive connections in average. (This
 	 * 256 times might not be accurate, we will change it later) We
 	 * use the following formula to estimate the overhead now:
-	 *		  dest->activeconns*256 + dest->inactconns
+	 *                dest->activeconns*256 + dest->inactconns
 	 */
 	return (atomic_read(&dest->activeconns) << 8) +
-		atomic_read(&dest->inactconns);
+	    atomic_read(&dest->inactconns);
 }
 
-
 /*
  *	Least Connection scheduling
  */
-static struct ip_vs_dest *
-ip_vs_lc_schedule(struct ip_vs_service *svc, const struct sk_buff *skb)
+static struct ip_vs_dest *ip_vs_lc_schedule(struct ip_vs_service *svc,
+					    const struct sk_buff *skb)
 {
 	struct ip_vs_dest *dest, *least = NULL;
 	unsigned int loh = 0, doh;
@@ -82,19 +79,17 @@ ip_vs_lc_schedule(struct ip_vs_service *
 	return least;
 }
 
-
 static struct ip_vs_scheduler ip_vs_lc_scheduler = {
-	.name =			"lc",
-	.refcnt =		ATOMIC_INIT(0),
-	.module =		THIS_MODULE,
-	.n_list =		LIST_HEAD_INIT(ip_vs_lc_scheduler.n_list),
-	.schedule =		ip_vs_lc_schedule,
+	.name = "lc",
+	.refcnt = ATOMIC_INIT(0),
+	.module = THIS_MODULE,
+	.n_list = LIST_HEAD_INIT(ip_vs_lc_scheduler.n_list),
+	.schedule = ip_vs_lc_schedule,
 };
 
-
 static int __init ip_vs_lc_init(void)
 {
-	return register_ip_vs_scheduler(&ip_vs_lc_scheduler) ;
+	return register_ip_vs_scheduler(&ip_vs_lc_scheduler);
 }
 
 static void __exit ip_vs_lc_cleanup(void)
diff -uprN linux-2.6.32-220.23.1.el6.x86_64.bak/net/netfilter/ipvs/ip_vs_nq.c linux-2.6.32-220.23.1.el6.x86_64/net/netfilter/ipvs/ip_vs_nq.c
--- linux-2.6.32-220.23.1.el6.x86_64.bak/net/netfilter/ipvs/ip_vs_nq.c	2009-12-03 11:51:21.000000000 +0800
+++ linux-2.6.32-220.23.1.el6.x86_64/net/netfilter/ipvs/ip_vs_nq.c	2012-10-11 15:01:03.000000000 +0800
@@ -39,9 +39,7 @@
 
 #include <net/ip_vs.h>
 
-
-static inline unsigned int
-ip_vs_nq_dest_overhead(struct ip_vs_dest *dest)
+static inline unsigned int ip_vs_nq_dest_overhead(struct ip_vs_dest *dest)
 {
 	/*
 	 * We only use the active connection number in the cost
@@ -50,12 +48,11 @@ ip_vs_nq_dest_overhead(struct ip_vs_dest
 	return atomic_read(&dest->activeconns) + 1;
 }
 
-
 /*
  *	Weighted Least Connection scheduling
  */
-static struct ip_vs_dest *
-ip_vs_nq_schedule(struct ip_vs_service *svc, const struct sk_buff *skb)
+static struct ip_vs_dest *ip_vs_nq_schedule(struct ip_vs_service *svc,
+					    const struct sk_buff *skb)
 {
 	struct ip_vs_dest *dest, *least = NULL;
 	unsigned int loh = 0, doh;
@@ -64,11 +61,11 @@ ip_vs_nq_schedule(struct ip_vs_service *
 
 	/*
 	 * We calculate the load of each dest server as follows:
-	 *	(server expected overhead) / dest->weight
+	 *      (server expected overhead) / dest->weight
 	 *
 	 * Remember -- no floats in kernel mode!!!
 	 * The comparison of h1*w2 > h2*w1 is equivalent to that of
-	 *		  h1/w1 > h2/w2
+	 *                h1/w1 > h2/w2
 	 * if every weight is larger than zero.
 	 *
 	 * The server with weight=0 is quiesced and will not receive any
@@ -103,7 +100,7 @@ ip_vs_nq_schedule(struct ip_vs_service *
 		return NULL;
 	}
 
-  out:
+      out:
 	IP_VS_DBG_BUF(6, "NQ: server %s:%u "
 		      "activeconns %d refcnt %d weight %d overhead %d\n",
 		      IP_VS_DBG_ADDR(svc->af, &least->addr), ntohs(least->port),
@@ -114,17 +111,14 @@ ip_vs_nq_schedule(struct ip_vs_service *
 	return least;
 }
 
-
-static struct ip_vs_scheduler ip_vs_nq_scheduler =
-{
-	.name =			"nq",
-	.refcnt =		ATOMIC_INIT(0),
-	.module =		THIS_MODULE,
-	.n_list =		LIST_HEAD_INIT(ip_vs_nq_scheduler.n_list),
-	.schedule =		ip_vs_nq_schedule,
+static struct ip_vs_scheduler ip_vs_nq_scheduler = {
+	.name = "nq",
+	.refcnt = ATOMIC_INIT(0),
+	.module = THIS_MODULE,
+	.n_list = LIST_HEAD_INIT(ip_vs_nq_scheduler.n_list),
+	.schedule = ip_vs_nq_schedule,
 };
 
-
 static int __init ip_vs_nq_init(void)
 {
 	return register_ip_vs_scheduler(&ip_vs_nq_scheduler);
diff -uprN linux-2.6.32-220.23.1.el6.x86_64.bak/net/netfilter/ipvs/ip_vs_proto_ah_esp.c linux-2.6.32-220.23.1.el6.x86_64/net/netfilter/ipvs/ip_vs_proto_ah_esp.c
--- linux-2.6.32-220.23.1.el6.x86_64.bak/net/netfilter/ipvs/ip_vs_proto_ah_esp.c	2009-12-03 11:51:21.000000000 +0800
+++ linux-2.6.32-220.23.1.el6.x86_64/net/netfilter/ipvs/ip_vs_proto_ah_esp.c	2012-10-11 15:01:03.000000000 +0800
@@ -22,7 +22,6 @@
 
 #include <net/ip_vs.h>
 
-
 /* TODO:
 
 struct isakmp_hdr {
@@ -40,26 +39,24 @@ struct isakmp_hdr {
 
 #define PORT_ISAKMP	500
 
-
-static struct ip_vs_conn *
-ah_esp_conn_in_get(int af, const struct sk_buff *skb, struct ip_vs_protocol *pp,
-		   const struct ip_vs_iphdr *iph, unsigned int proto_off,
-		   int inverse)
+static struct ip_vs_conn *ah_esp_conn_in_get(int af, const struct sk_buff *skb,
+					     struct ip_vs_protocol *pp,
+					     const struct ip_vs_iphdr *iph,
+					     unsigned int proto_off,
+					     int inverse, int *res_dir)
 {
 	struct ip_vs_conn *cp;
 
 	if (likely(!inverse)) {
-		cp = ip_vs_conn_in_get(af, IPPROTO_UDP,
-				       &iph->saddr,
-				       htons(PORT_ISAKMP),
-				       &iph->daddr,
-				       htons(PORT_ISAKMP));
+		cp = ip_vs_conn_get(af, IPPROTO_UDP,
+				    &iph->saddr,
+				    htons(PORT_ISAKMP),
+				    &iph->daddr, htons(PORT_ISAKMP), res_dir);
 	} else {
-		cp = ip_vs_conn_in_get(af, IPPROTO_UDP,
-				       &iph->daddr,
-				       htons(PORT_ISAKMP),
-				       &iph->saddr,
-				       htons(PORT_ISAKMP));
+		cp = ip_vs_conn_get(af, IPPROTO_UDP,
+				    &iph->daddr,
+				    htons(PORT_ISAKMP),
+				    &iph->saddr, htons(PORT_ISAKMP), res_dir);
 	}
 
 	if (!cp) {
@@ -78,28 +75,24 @@ ah_esp_conn_in_get(int af, const struct 
 	return cp;
 }
 
-
-static struct ip_vs_conn *
-ah_esp_conn_out_get(int af, const struct sk_buff *skb,
-		    struct ip_vs_protocol *pp,
-		    const struct ip_vs_iphdr *iph,
-		    unsigned int proto_off,
-		    int inverse)
+static struct ip_vs_conn *ah_esp_conn_out_get(int af, const struct sk_buff *skb,
+					      struct ip_vs_protocol *pp,
+					      const struct ip_vs_iphdr *iph,
+					      unsigned int proto_off,
+					      int inverse, int *res_dir)
 {
 	struct ip_vs_conn *cp;
 
 	if (likely(!inverse)) {
-		cp = ip_vs_conn_out_get(af, IPPROTO_UDP,
-					&iph->saddr,
-					htons(PORT_ISAKMP),
-					&iph->daddr,
-					htons(PORT_ISAKMP));
+		cp = ip_vs_conn_get(af, IPPROTO_UDP,
+				    &iph->saddr,
+				    htons(PORT_ISAKMP),
+				    &iph->daddr, htons(PORT_ISAKMP), res_dir);
 	} else {
-		cp = ip_vs_conn_out_get(af, IPPROTO_UDP,
-					&iph->daddr,
-					htons(PORT_ISAKMP),
-					&iph->saddr,
-					htons(PORT_ISAKMP));
+		cp = ip_vs_conn_get(af, IPPROTO_UDP,
+				    &iph->daddr,
+				    htons(PORT_ISAKMP),
+				    &iph->saddr, htons(PORT_ISAKMP), res_dir);
 	}
 
 	if (!cp) {
@@ -114,7 +107,6 @@ ah_esp_conn_out_get(int af, const struct
 	return cp;
 }
 
-
 static int
 ah_esp_conn_schedule(int af, struct sk_buff *skb, struct ip_vs_protocol *pp,
 		     int *verdict, struct ip_vs_conn **cpp)
@@ -126,7 +118,6 @@ ah_esp_conn_schedule(int af, struct sk_b
 	return 0;
 }
 
-
 static void
 ah_esp_debug_packet_v4(struct ip_vs_protocol *pp, const struct sk_buff *skb,
 		       int offset, const char *msg)
@@ -138,8 +129,7 @@ ah_esp_debug_packet_v4(struct ip_vs_prot
 	if (ih == NULL)
 		sprintf(buf, "%s TRUNCATED", pp->name);
 	else
-		sprintf(buf, "%s %pI4->%pI4",
-			pp->name, &ih->saddr, &ih->daddr);
+		sprintf(buf, "%s %pI4->%pI4", pp->name, &ih->saddr, &ih->daddr);
 
 	pr_debug("%s: %s\n", msg, buf);
 }
@@ -156,8 +146,7 @@ ah_esp_debug_packet_v6(struct ip_vs_prot
 	if (ih == NULL)
 		sprintf(buf, "%s TRUNCATED", pp->name);
 	else
-		sprintf(buf, "%s %pI6->%pI6",
-			pp->name, &ih->saddr, &ih->daddr);
+		sprintf(buf, "%s %pI6->%pI6", pp->name, &ih->saddr, &ih->daddr);
 
 	pr_debug("%s: %s\n", msg, buf);
 }
@@ -175,62 +164,59 @@ ah_esp_debug_packet(struct ip_vs_protoco
 		ah_esp_debug_packet_v4(pp, skb, offset, msg);
 }
 
-
 static void ah_esp_init(struct ip_vs_protocol *pp)
 {
 	/* nothing to do now */
 }
 
-
 static void ah_esp_exit(struct ip_vs_protocol *pp)
 {
 	/* nothing to do now */
 }
 
-
 #ifdef CONFIG_IP_VS_PROTO_AH
 struct ip_vs_protocol ip_vs_protocol_ah = {
-	.name =			"AH",
-	.protocol =		IPPROTO_AH,
-	.num_states =		1,
-	.dont_defrag =		1,
-	.init =			ah_esp_init,
-	.exit =			ah_esp_exit,
-	.conn_schedule =	ah_esp_conn_schedule,
-	.conn_in_get =		ah_esp_conn_in_get,
-	.conn_out_get =		ah_esp_conn_out_get,
-	.snat_handler =		NULL,
-	.dnat_handler =		NULL,
-	.csum_check =		NULL,
-	.state_transition =	NULL,
-	.register_app =		NULL,
-	.unregister_app =	NULL,
-	.app_conn_bind =	NULL,
-	.debug_packet =		ah_esp_debug_packet,
-	.timeout_change =	NULL,		/* ISAKMP */
-	.set_state_timeout =	NULL,
+	.name = "AH",
+	.protocol = IPPROTO_AH,
+	.num_states = 1,
+	.dont_defrag = 1,
+	.init = ah_esp_init,
+	.exit = ah_esp_exit,
+	.conn_schedule = ah_esp_conn_schedule,
+	.conn_in_get = ah_esp_conn_in_get,
+	.conn_out_get = ah_esp_conn_out_get,
+	.snat_handler = NULL,
+	.dnat_handler = NULL,
+	.csum_check = NULL,
+	.state_transition = NULL,
+	.register_app = NULL,
+	.unregister_app = NULL,
+	.app_conn_bind = NULL,
+	.debug_packet = ah_esp_debug_packet,
+	.timeout_change = NULL,	/* ISAKMP */
+	.set_state_timeout = NULL,
 };
 #endif
 
 #ifdef CONFIG_IP_VS_PROTO_ESP
 struct ip_vs_protocol ip_vs_protocol_esp = {
-	.name =			"ESP",
-	.protocol =		IPPROTO_ESP,
-	.num_states =		1,
-	.dont_defrag =		1,
-	.init =			ah_esp_init,
-	.exit =			ah_esp_exit,
-	.conn_schedule =	ah_esp_conn_schedule,
-	.conn_in_get =		ah_esp_conn_in_get,
-	.conn_out_get =		ah_esp_conn_out_get,
-	.snat_handler =		NULL,
-	.dnat_handler =		NULL,
-	.csum_check =		NULL,
-	.state_transition =	NULL,
-	.register_app =		NULL,
-	.unregister_app =	NULL,
-	.app_conn_bind =	NULL,
-	.debug_packet =		ah_esp_debug_packet,
-	.timeout_change =	NULL,		/* ISAKMP */
+	.name = "ESP",
+	.protocol = IPPROTO_ESP,
+	.num_states = 1,
+	.dont_defrag = 1,
+	.init = ah_esp_init,
+	.exit = ah_esp_exit,
+	.conn_schedule = ah_esp_conn_schedule,
+	.conn_in_get = ah_esp_conn_in_get,
+	.conn_out_get = ah_esp_conn_out_get,
+	.snat_handler = NULL,
+	.dnat_handler = NULL,
+	.csum_check = NULL,
+	.state_transition = NULL,
+	.register_app = NULL,
+	.unregister_app = NULL,
+	.app_conn_bind = NULL,
+	.debug_packet = ah_esp_debug_packet,
+	.timeout_change = NULL,	/* ISAKMP */
 };
 #endif
diff -uprN linux-2.6.32-220.23.1.el6.x86_64.bak/net/netfilter/ipvs/ip_vs_proto.c linux-2.6.32-220.23.1.el6.x86_64/net/netfilter/ipvs/ip_vs_proto.c
--- linux-2.6.32-220.23.1.el6.x86_64.bak/net/netfilter/ipvs/ip_vs_proto.c	2009-12-03 11:51:21.000000000 +0800
+++ linux-2.6.32-220.23.1.el6.x86_64/net/netfilter/ipvs/ip_vs_proto.c	2012-10-11 15:01:03.000000000 +0800
@@ -30,7 +30,6 @@
 
 #include <net/ip_vs.h>
 
-
 /*
  * IPVS protocols can only be registered/unregistered when the ipvs
  * module is loaded/unloaded, so no lock is needed in accessing the
@@ -42,7 +41,6 @@
 
 static struct ip_vs_protocol *ip_vs_proto_table[IP_VS_PROTO_TAB_SIZE];
 
-
 /*
  *	register an ipvs protocol
  */
@@ -59,7 +57,6 @@ static int __used __init register_ip_vs_
 	return 0;
 }
 
-
 /*
  *	unregister an ipvs protocol
  */
@@ -81,11 +78,10 @@ static int unregister_ip_vs_protocol(str
 	return -ESRCH;
 }
 
-
 /*
  *	get ip_vs_protocol object by its proto.
  */
-struct ip_vs_protocol * ip_vs_proto_get(unsigned short proto)
+struct ip_vs_protocol *ip_vs_proto_get(unsigned short proto)
 {
 	struct ip_vs_protocol *pp;
 	unsigned hash = IP_VS_PROTO_HASH(proto);
@@ -98,7 +94,6 @@ struct ip_vs_protocol * ip_vs_proto_get(
 	return NULL;
 }
 
-
 /*
  *	Propagate event for state change to all protocols
  */
@@ -115,14 +110,11 @@ void ip_vs_protocol_timeout_change(int f
 	}
 }
 
-
-int *
-ip_vs_create_timeout_table(int *table, int size)
+int *ip_vs_create_timeout_table(int *table, int size)
 {
 	return kmemdup(table, size, GFP_ATOMIC);
 }
 
-
 /*
  *	Set timeout value for state specified by name
  */
@@ -144,8 +136,7 @@ ip_vs_set_state_timeout(int *table, int 
 	return -ENOENT;
 }
 
-
-const char * ip_vs_state_name(__u16 proto, int state)
+const char *ip_vs_state_name(__u16 proto, int state)
 {
 	struct ip_vs_protocol *pp = ip_vs_proto_get(proto);
 
@@ -154,12 +145,10 @@ const char * ip_vs_state_name(__u16 prot
 	return pp->state_name(state);
 }
 
-
 static void
 ip_vs_tcpudp_debug_packet_v4(struct ip_vs_protocol *pp,
 			     const struct sk_buff *skb,
-			     int offset,
-			     const char *msg)
+			     int offset, const char *msg)
 {
 	char buf[128];
 	struct iphdr _iph, *ih;
@@ -171,9 +160,8 @@ ip_vs_tcpudp_debug_packet_v4(struct ip_v
 		sprintf(buf, "%s %pI4->%pI4 frag",
 			pp->name, &ih->saddr, &ih->daddr);
 	else {
-		__be16 _ports[2], *pptr
-;
-		pptr = skb_header_pointer(skb, offset + ih->ihl*4,
+		__be16 _ports[2], *pptr;
+		pptr = skb_header_pointer(skb, offset + ih->ihl * 4,
 					  sizeof(_ports), _ports);
 		if (pptr == NULL)
 			sprintf(buf, "%s TRUNCATED %pI4->%pI4",
@@ -192,8 +180,7 @@ ip_vs_tcpudp_debug_packet_v4(struct ip_v
 static void
 ip_vs_tcpudp_debug_packet_v6(struct ip_vs_protocol *pp,
 			     const struct sk_buff *skb,
-			     int offset,
-			     const char *msg)
+			     int offset, const char *msg)
 {
 	char buf[192];
 	struct ipv6hdr _iph, *ih;
@@ -223,12 +210,10 @@ ip_vs_tcpudp_debug_packet_v6(struct ip_v
 }
 #endif
 
-
 void
 ip_vs_tcpudp_debug_packet(struct ip_vs_protocol *pp,
 			  const struct sk_buff *skb,
-			  int offset,
-			  const char *msg)
+			  int offset, const char *msg)
 {
 #ifdef CONFIG_IP_VS_IPV6
 	if (skb->protocol == htons(ETH_P_IPV6))
@@ -238,7 +223,6 @@ ip_vs_tcpudp_debug_packet(struct ip_vs_p
 		ip_vs_tcpudp_debug_packet_v4(pp, skb, offset, msg);
 }
 
-
 int __init ip_vs_protocol_init(void)
 {
 	char protocols[64];
@@ -268,7 +252,6 @@ int __init ip_vs_protocol_init(void)
 	return 0;
 }
 
-
 void ip_vs_protocol_cleanup(void)
 {
 	struct ip_vs_protocol *pp;
diff -uprN linux-2.6.32-220.23.1.el6.x86_64.bak/net/netfilter/ipvs/ip_vs_proto_icmp.c linux-2.6.32-220.23.1.el6.x86_64/net/netfilter/ipvs/ip_vs_proto_icmp.c
--- linux-2.6.32-220.23.1.el6.x86_64.bak/net/netfilter/ipvs/ip_vs_proto_icmp.c	1970-01-01 08:00:00.000000000 +0800
+++ linux-2.6.32-220.23.1.el6.x86_64/net/netfilter/ipvs/ip_vs_proto_icmp.c	2013-04-11 19:19:13.000000000 +0800
@@ -0,0 +1,610 @@
+/*
+ * ip_vs_proto_icmp.c:	ICMP load balancing support for IPVS
+ *
+ * Authors:     yu bo <yubo@xiaomi.com>
+ *
+ *              This program is free software; you can redistribute it and/or
+ *              modify it under the terms of the GNU General Public License
+ *              as published by the Free Software Foundation; either version
+ *              2 of the License, or (at your option) any later version.
+ *
+ * Changes:
+ *
+ */
+
+#define KMSG_COMPONENT "IPVS"
+#define pr_fmt(fmt) KMSG_COMPONENT ": " fmt
+
+#include <linux/in.h>
+#include <linux/ip.h>
+#include <linux/kernel.h>
+#include <linux/netfilter.h>
+#include <linux/netfilter_ipv4.h>
+#include <linux/icmp.h>
+
+#include <net/ip_vs.h>
+#include <net/ip.h>
+#include <net/ip6_checksum.h>
+
+static struct ip_vs_conn *icmp_conn_in_get(int af, const struct sk_buff *skb,
+					  struct ip_vs_protocol *pp,
+					  const struct ip_vs_iphdr *iph,
+					  unsigned int proto_off, int inverse,
+					  int *res_dir)
+{
+	struct ip_vs_conn *cp;
+	__be16 _ports[2], *pptr;
+
+	pptr = skb_header_pointer(skb, proto_off, sizeof(_ports), _ports);
+	if (pptr == NULL)
+		return NULL;
+
+	if (likely(!inverse)) {
+		cp = ip_vs_conn_get(af, iph->protocol,
+				    &iph->saddr, pptr[0],
+				    &iph->daddr, pptr[1], res_dir);
+	} else {
+		cp = ip_vs_conn_get(af, iph->protocol,
+				    &iph->daddr, pptr[1],
+				    &iph->saddr, pptr[0], res_dir);
+	}
+
+	return cp;
+}
+
+static struct ip_vs_conn *icmp_conn_out_get(int af, const struct sk_buff *skb,
+					   struct ip_vs_protocol *pp,
+					   const struct ip_vs_iphdr *iph,
+					   unsigned int proto_off, int inverse,
+					   int *res_dir)
+{
+	struct ip_vs_conn *cp;
+	__be16 _ports[2], *pptr;
+
+	pptr = skb_header_pointer(skb, proto_off, sizeof(_ports), _ports);
+	if (pptr == NULL)
+		return NULL;
+
+	if (likely(!inverse)) {
+		cp = ip_vs_conn_get(af, iph->protocol,
+				    &iph->saddr, pptr[0],
+				    &iph->daddr, pptr[1], res_dir);
+	} else {
+		cp = ip_vs_conn_get(af, iph->protocol,
+				    &iph->daddr, pptr[1],
+				    &iph->saddr, pptr[0], res_dir);
+	}
+
+	return cp;
+}
+
+static int
+icmp_conn_schedule(int af, struct sk_buff *skb, struct ip_vs_protocol *pp,
+		  int *verdict, struct ip_vs_conn **cpp)
+{
+	struct ip_vs_service *svc;
+	struct icmphdr _icmph, *uh;
+	struct ip_vs_iphdr iph;
+	
+
+	ip_vs_fill_iphdr(af, skb_network_header(skb), &iph);
+
+	uh = skb_header_pointer(skb, iph.len, sizeof(_icmph), &_icmph);
+	if (uh == NULL) {
+		*verdict = NF_DROP;
+		return 0;
+	}
+
+
+	svc = ip_vs_service_get(af, skb->mark, iph.protocol,
+			&iph.daddr, uh->dest);
+	
+	if (svc) {
+		if (ip_vs_todrop()) {
+			/*
+			 * It seems that we are very loaded.
+			 * We have to drop this packet :(
+			 */
+			ip_vs_service_put(svc);
+			*verdict = NF_DROP;
+			return 0;
+		}
+
+		/*
+		 * Let the virtual server select a real server for the
+		 * incoming connection, and create a connection entry.
+		 */
+		*cpp = ip_vs_schedule(svc, skb, 0);
+		if (!*cpp) {
+			*verdict = ip_vs_leave(svc, skb, pp);
+			return 0;
+		}
+		ip_vs_service_put(svc);
+	}
+	return 1;
+}
+
+static inline void
+icmp_fast_csum_update(int af, struct icmphdr *uhdr,
+		     const union nf_inet_addr *oldip,
+		     const union nf_inet_addr *newip,
+		     __be16 oldport, __be16 newport)
+{
+#ifdef CONFIG_IP_VS_IPV6
+	if (af == AF_INET6)
+		uhdr->check =
+		    csum_fold(ip_vs_check_diff16(oldip->ip6, newip->ip6,
+						 ip_vs_check_diff2(oldport,
+								   newport,
+								   ~csum_unfold
+								   (uhdr->
+								    check))));
+	else
+#endif
+		uhdr->check =
+		    csum_fold(ip_vs_check_diff4(oldip->ip, newip->ip,
+						ip_vs_check_diff2(oldport,
+								  newport,
+								  ~csum_unfold
+								  (uhdr->
+								   check))));
+	if (!uhdr->check)
+		uhdr->check = CSUM_MANGLED_0;
+}
+
+static inline void
+icmp_partial_csum_update(int af, struct icmphdr *uhdr,
+			const union nf_inet_addr *oldip,
+			const union nf_inet_addr *newip,
+			__be16 oldlen, __be16 newlen)
+{
+#ifdef CONFIG_IP_VS_IPV6
+	if (af == AF_INET6)
+		uhdr->check =
+		    csum_fold(ip_vs_check_diff16(oldip->ip6, newip->ip6,
+						 ip_vs_check_diff2(oldlen,
+								   newlen,
+								   ~csum_unfold
+								   (uhdr->
+								    check))));
+	else
+#endif
+		uhdr->check =
+		    csum_fold(ip_vs_check_diff4(oldip->ip, newip->ip,
+						ip_vs_check_diff2(oldlen,
+								  newlen,
+								  ~csum_unfold
+								  (uhdr->
+								   check))));
+}
+
+static int
+icmp_snat_handler(struct sk_buff *skb,
+		 struct ip_vs_protocol *pp, struct ip_vs_conn *cp)
+{
+	struct icmphdr *icmph;
+	unsigned int icmphoff;
+	int oldlen;
+
+#ifdef CONFIG_IP_VS_IPV6
+	if (cp->af == AF_INET6)
+		icmphoff = sizeof(struct ipv6hdr);
+	else
+#endif
+		icmphoff = ip_hdrlen(skb);
+	oldlen = skb->len - icmphoff;
+
+	/* csum_check requires unshared skb */
+	if (!skb_make_writable(skb, icmphoff + sizeof(*icmph)))
+		return 0;
+
+	if (unlikely(cp->app != NULL)) {
+		/* Some checks before mangling */
+		if (pp->csum_check && !pp->csum_check(cp->af, skb, pp))
+			return 0;
+
+		/*
+		 *      Call application helper if needed
+		 */
+		if (!ip_vs_app_pkt_out(cp, skb))
+			return 0;
+	}
+
+	icmph = (void *)skb_network_header(skb) + icmphoff;
+	icmph->source = cp->vport;
+	icmph->dest = cp->cport;
+
+	/*
+	 *      Adjust ICMP checksums
+	 */
+	if (skb->ip_summed == CHECKSUM_PARTIAL) {
+		icmp_partial_csum_update(cp->af, icmph, &cp->daddr, &cp->vaddr,
+					htons(oldlen),
+					htons(skb->len - icmphoff));
+		icmp_partial_csum_update(cp->af, icmph, &cp->laddr, &cp->caddr,
+					htons(oldlen),
+					htons(skb->len - icmphoff));
+	} else if (!cp->app && (icmph->check != 0)) {
+		/* Only port and addr are changed, do fast csum update */
+		icmp_fast_csum_update(cp->af, icmph, &cp->daddr, &cp->vaddr,
+				     cp->dport, cp->vport);
+		icmp_fast_csum_update(cp->af, icmph, &cp->laddr, &cp->caddr,
+				     cp->lport, cp->cport);
+		if (skb->ip_summed == CHECKSUM_COMPLETE)
+			skb->ip_summed = CHECKSUM_NONE;
+	} else {
+		/* full checksum calculation */
+		icmph->check = 0;
+		skb->csum = skb_checksum(skb, icmphoff, skb->len - icmphoff, 0);
+#ifdef CONFIG_IP_VS_IPV6
+		if (cp->af == AF_INET6)
+			icmph->check = csum_ipv6_magic(&cp->vaddr.in6,
+						      &cp->caddr.in6,
+						      skb->len - icmphoff,
+						      cp->protocol, skb->csum);
+		else
+#endif
+			icmph->check = csum_tcpicmp_magic(cp->vaddr.ip,
+							cp->caddr.ip,
+							skb->len - icmphoff,
+							cp->protocol,
+							skb->csum);
+		if (icmph->check == 0)
+			icmph->check = CSUM_MANGLED_0;
+		IP_VS_DBG(11, "O-pkt: %s O-csum=%d (+%zd)\n",
+			  pp->name, icmph->check,
+			  (char *)&(icmph->check) - (char *)icmph);
+	}
+	return 1;
+}
+
+static int
+icmp_dnat_handler(struct sk_buff *skb,
+		 struct ip_vs_protocol *pp, struct ip_vs_conn *cp)
+{
+	struct icmphdr *icmph;
+	unsigned int icmphoff;
+	int oldlen;
+
+#ifdef CONFIG_IP_VS_IPV6
+	if (cp->af == AF_INET6)
+		icmphoff = sizeof(struct ipv6hdr);
+	else
+#endif
+		icmphoff = ip_hdrlen(skb);
+	oldlen = skb->len - icmphoff;
+
+	/* csum_check requires unshared skb */
+	if (!skb_make_writable(skb, icmphoff + sizeof(*icmph)))
+		return 0;
+
+	if (unlikely(cp->app != NULL)) {
+		/* Some checks before mangling */
+		if (pp->csum_check && !pp->csum_check(cp->af, skb, pp))
+			return 0;
+
+		/*
+		 *      Attempt ip_vs_app call.
+		 *      It will fix ip_vs_conn
+		 */
+		if (!ip_vs_app_pkt_in(cp, skb))
+			return 0;
+	}
+
+	icmph = (void *)skb_network_header(skb) + icmphoff;
+	icmph->source = cp->lport;
+	icmph->dest = cp->dport;
+
+	/*
+	 *      Adjust ICMP checksums
+	 */
+	if (skb->ip_summed == CHECKSUM_PARTIAL) {
+		icmp_partial_csum_update(cp->af, icmph, &cp->vaddr, &cp->daddr,
+					htons(oldlen),
+					htons(skb->len - icmphoff));
+		icmp_partial_csum_update(cp->af, icmph, &cp->caddr, &cp->laddr,
+					htons(oldlen),
+					htons(skb->len - icmphoff));
+	} else if (!cp->app && (icmph->check != 0)) {
+		/* Only port and addr are changed, do fast csum update */
+		icmp_fast_csum_update(cp->af, icmph, &cp->vaddr, &cp->daddr,
+				     cp->vport, cp->dport);
+		icmp_fast_csum_update(cp->af, icmph, &cp->caddr, &cp->laddr,
+				     cp->cport, cp->lport);
+		if (skb->ip_summed == CHECKSUM_COMPLETE)
+			skb->ip_summed = CHECKSUM_NONE;
+	} else {
+		/* full checksum calculation */
+		icmph->check = 0;
+		skb->csum = skb_checksum(skb, icmphoff, skb->len - icmphoff, 0);
+#ifdef CONFIG_IP_VS_IPV6
+		if (cp->af == AF_INET6)
+			icmph->check = csum_ipv6_magic(&cp->caddr.in6,
+						      &cp->daddr.in6,
+						      skb->len - icmphoff,
+						      cp->protocol, skb->csum);
+		else
+#endif
+			icmph->check = csum_tcpicmp_magic(cp->caddr.ip,
+							cp->daddr.ip,
+							skb->len - icmphoff,
+							cp->protocol,
+							skb->csum);
+		if (icmph->check == 0)
+			icmph->check = CSUM_MANGLED_0;
+		skb->ip_summed = CHECKSUM_UNNECESSARY;
+	}
+	return 1;
+}
+
+static int
+icmp_csum_check(int af, struct sk_buff *skb, struct ip_vs_protocol *pp)
+{
+	struct icmphdr _icmph, *uh;
+	unsigned int icmphoff;
+
+#ifdef CONFIG_IP_VS_IPV6
+	if (af == AF_INET6)
+		icmphoff = sizeof(struct ipv6hdr);
+	else
+#endif
+		icmphoff = ip_hdrlen(skb);
+
+	uh = skb_header_pointer(skb, icmphoff, sizeof(_icmph), &_icmph);
+	if (uh == NULL)
+		return 0;
+
+	if (uh->check != 0) {
+		switch (skb->ip_summed) {
+		case CHECKSUM_NONE:
+			skb->csum = skb_checksum(skb, icmphoff,
+						 skb->len - icmphoff, 0);
+		case CHECKSUM_COMPLETE:
+#ifdef CONFIG_IP_VS_IPV6
+			if (af == AF_INET6) {
+				if (csum_ipv6_magic(&ipv6_hdr(skb)->saddr,
+						    &ipv6_hdr(skb)->daddr,
+						    skb->len - icmphoff,
+						    ipv6_hdr(skb)->nexthdr,
+						    skb->csum)) {
+					IP_VS_DBG_RL_PKT(0, pp, skb, 0,
+							 "Failed checksum for");
+					return 0;
+				}
+			} else
+#endif
+			if (csum_tcpicmp_magic(ip_hdr(skb)->saddr,
+						      ip_hdr(skb)->
+						      daddr,
+						      skb->len -
+						      icmphoff,
+						      ip_hdr(skb)->
+						      protocol, skb->csum)) {
+				IP_VS_DBG_RL_PKT(0, pp, skb, 0,
+						 "Failed checksum for");
+				return 0;
+			}
+			break;
+		default:
+			/* No need to checksum. */
+			break;
+		}
+	}
+	return 1;
+}
+
+/*
+ *	Note: the caller guarantees that only one of register_app,
+ *	unregister_app or app_conn_bind is called each time.
+ */
+
+#define	ICMP_APP_TAB_BITS	4
+#define	ICMP_APP_TAB_SIZE	(1 << ICMP_APP_TAB_BITS)
+#define	ICMP_APP_TAB_MASK	(ICMP_APP_TAB_SIZE - 1)
+
+static struct list_head icmp_apps[ICMP_APP_TAB_SIZE];
+static DEFINE_SPINLOCK(icmp_app_lock);
+
+static inline __u16 icmp_app_hashkey(__be16 port)
+{
+	return (((__force u16) port >> ICMP_APP_TAB_BITS) ^ (__force u16) port)
+	    & ICMP_APP_TAB_MASK;
+}
+
+static int icmp_register_app(struct ip_vs_app *inc)
+{
+	struct ip_vs_app *i;
+	__u16 hash;
+	__be16 port = inc->port;
+	int ret = 0;
+
+	hash = icmp_app_hashkey(port);
+
+	spin_lock_bh(&icmp_app_lock);
+	list_for_each_entry(i, &icmp_apps[hash], p_list) {
+		if (i->port == port) {
+			ret = -EEXIST;
+			goto out;
+		}
+	}
+	list_add(&inc->p_list, &icmp_apps[hash]);
+	atomic_inc(&ip_vs_protocol_icmp.appcnt);
+
+      out:
+	spin_unlock_bh(&icmp_app_lock);
+	return ret;
+}
+
+static void icmp_unregister_app(struct ip_vs_app *inc)
+{
+	spin_lock_bh(&icmp_app_lock);
+	atomic_dec(&ip_vs_protocol_icmp.appcnt);
+	list_del(&inc->p_list);
+	spin_unlock_bh(&icmp_app_lock);
+}
+
+static int icmp_app_conn_bind(struct ip_vs_conn *cp)
+{
+	int hash;
+	struct ip_vs_app *inc;
+	int result = 0;
+
+	/* Default binding: bind app only for NAT */
+	if (IP_VS_FWD_METHOD(cp) != IP_VS_CONN_F_MASQ)
+		return 0;
+
+	/* Lookup application incarnations and bind the right one */
+	hash = icmp_app_hashkey(cp->vport);
+
+	spin_lock(&icmp_app_lock);
+	list_for_each_entry(inc, &icmp_apps[hash], p_list) {
+		if (inc->port == cp->vport) {
+			if (unlikely(!ip_vs_app_inc_get(inc)))
+				break;
+			spin_unlock(&icmp_app_lock);
+
+			IP_VS_DBG_BUF(9, "%s(): Binding conn %s:%u->"
+				      "%s:%u to app %s on port %u\n",
+				      __func__,
+				      IP_VS_DBG_ADDR(cp->af, &cp->caddr),
+				      ntohs(cp->cport),
+				      IP_VS_DBG_ADDR(cp->af, &cp->vaddr),
+				      ntohs(cp->vport),
+				      inc->name, ntohs(inc->port));
+
+			cp->app = inc;
+			if (inc->init_conn)
+				result = inc->init_conn(inc, cp);
+			goto out;
+		}
+	}
+	spin_unlock(&icmp_app_lock);
+
+      out:
+	return result;
+}
+
+static int icmp_timeouts[IP_VS_ICMP_S_LAST + 1] = {
+	[IP_VS_ICMP_S_NORMAL] = 5 * 60 * HZ,
+	[IP_VS_ICMP_S_LAST] = 2 * HZ,
+};
+
+static const char *const icmp_state_name_table[IP_VS_ICMP_S_LAST + 1] = {
+	[IP_VS_ICMP_S_NORMAL] = "ICMP",
+	[IP_VS_ICMP_S_LAST] = "BUG!",
+};
+
+static int icmp_set_state_timeout(struct ip_vs_protocol *pp, char *sname, int to)
+{
+	return ip_vs_set_state_timeout(pp->timeout_table, IP_VS_ICMP_S_LAST,
+				       icmp_state_name_table, sname, to);
+}
+
+static const char *icmp_state_name(int state)
+{
+	if (state >= IP_VS_ICMP_S_LAST)
+		return "ERR!";
+	return icmp_state_name_table[state] ? icmp_state_name_table[state] : "?";
+}
+
+static int
+icmp_state_transition(struct ip_vs_conn *cp, int direction,
+		     const struct sk_buff *skb, struct ip_vs_protocol *pp)
+{
+	cp->timeout = pp->timeout_table[IP_VS_ICMP_S_NORMAL];
+	return 1;
+}
+
+static void icmp_init(struct ip_vs_protocol *pp)
+{
+	IP_VS_INIT_HASH_TABLE(icmp_apps);
+	pp->timeout_table = icmp_timeouts;
+}
+
+static void icmp_exit(struct ip_vs_protocol *pp)
+{
+}
+
+
+
+
+static void
+ip_vs_icmp_debug_packet_v4(struct ip_vs_protocol *pp, const struct sk_buff *skb,
+		       int offset, const char *msg)
+{
+	char buf[256];
+	struct iphdr *iph;
+	struct icmphdr _icmph, *ic;
+
+
+	iph = ip_hdr(skb);
+	offset = ihl = iph->ihl * 4;
+	ic = skb_header_pointer(skb, offset, sizeof(_icmph), &_icmph);
+	if (ic == NULL)
+		sprintf(buf, "%s TRUNCATED", pp->name);
+	else
+		sprintf(buf, "%s ICMP (%d,%d) %pI4->%pI4", ic->type, ntohs(icmp_id(ic)), 
+			&iph->saddr, &iph->daddr);
+
+	pr_debug("%s: %s\n", msg, buf);
+}
+
+#ifdef CONFIG_IP_VS_IPV6
+static void
+ip_vs_icmp_debug_packet_v6(struct ip_vs_protocol *pp, const struct sk_buff *skb,
+		       int offset, const char *msg)
+{
+	char buf[256];
+	struct ipv6hdr *iph;
+	struct icmp6hdr _icmph, *ic;
+
+	iph = ipv6_hdr(skb);
+	offset = sizeof(struct ipv6hdr);
+	ic = skb_header_pointer(skb, offset, sizeof(_icmph), &_icmph);
+	if (ic == NULL)
+		sprintf(buf, "%s TRUNCATED", pp->name);
+	else
+		sprintf(buf, "%s ICMPv6 (%d,%d) %pI6->%pI6", pp->name, ic->icmp6_type,
+			ntohs(icmpv6_id(ic)), &iph->saddr, &iph->daddr);
+	pr_debug("%s: %s\n", msg, buf);
+}
+#endif
+
+
+
+
+static void
+ip_vs_icmp_debug_packet(struct ip_vs_protocol *pp, const struct sk_buff *skb,
+		    int offset, const char *msg)
+{
+#ifdef CONFIG_IP_VS_IPV6
+	if (skb->protocol == htons(ETH_P_IPV6))
+		ip_vs_icmp_debug_packet_v6(pp, skb, offset, msg);
+	else
+#endif
+		ip_vs_icmp_debug_packet_v4(pp, skb, offset, msg);
+}
+
+
+struct ip_vs_protocol ip_vs_protocol_icmp = {
+	.name = "ICMP",
+	.protocol = IPPROTO_ICMP,
+	.num_states = IP_VS_ICMP_S_LAST,
+	.dont_defrag = 0,
+	.init = icmp_init,
+	.exit = icmp_exit,
+	.conn_schedule = icmp_conn_schedule,
+	.conn_in_get = icmp_conn_in_get,
+	.conn_out_get = icmp_conn_out_get,
+	.snat_handler = icmp_snat_handler,
+	.dnat_handler = icmp_dnat_handler,
+	.csum_check = icmp_csum_check,
+	.state_transition = icmp_state_transition,
+	.state_name = icmp_state_name,
+	.register_app = icmp_register_app,
+	.unregister_app = icmp_unregister_app,
+	.app_conn_bind = icmp_app_conn_bind,
+	.debug_packet = ip_vs_icmp_debug_packet,
+	.timeout_change = NULL,
+	.set_state_timeout = icmp_set_state_timeout,
+};
diff -uprN linux-2.6.32-220.23.1.el6.x86_64.bak/net/netfilter/ipvs/ip_vs_proto_tcp.c linux-2.6.32-220.23.1.el6.x86_64/net/netfilter/ipvs/ip_vs_proto_tcp.c
--- linux-2.6.32-220.23.1.el6.x86_64.bak/net/netfilter/ipvs/ip_vs_proto_tcp.c	2009-12-03 11:51:21.000000000 +0800
+++ linux-2.6.32-220.23.1.el6.x86_64/net/netfilter/ipvs/ip_vs_proto_tcp.c	2013-03-18 17:26:28.000000000 +0800
@@ -10,7 +10,11 @@
  *              2 of the License, or (at your option) any later version.
  *
  * Changes:
+ *	Yi Yang      <specific@gmail.com>
+ *	Jiajun Chen  <mofan.cjj@taobao.com>
+ *	Jiaming Wu   <pukong.wjm@taobao.com>	support FULLNAT and SYNPROXY
  *
+ *   Yu Bo        <yubo@xiaomi.com>
  */
 
 #define KMSG_COMPONENT "IPVS"
@@ -18,20 +22,28 @@
 
 #include <linux/kernel.h>
 #include <linux/ip.h>
-#include <linux/tcp.h>                  /* for tcphdr */
+#include <linux/tcp.h>		/* for tcphdr */
 #include <net/ip.h>
-#include <net/tcp.h>                    /* for csum_tcpudp_magic */
-#include <net/ip6_checksum.h>
+#include <net/tcp.h>		/* for csum_tcpudp_magic */
 #include <linux/netfilter.h>
 #include <linux/netfilter_ipv4.h>
+#include <net/secure_seq.h>
+
+#ifdef CONFIG_IP_VS_IPV6
+#include <net/ipv6.h>
+#include <net/ip6_checksum.h>
+#endif
 
 #include <net/ip_vs.h>
+#include <net/ip_vs_synproxy.h>
 
 
-static struct ip_vs_conn *
-tcp_conn_in_get(int af, const struct sk_buff *skb, struct ip_vs_protocol *pp,
-		const struct ip_vs_iphdr *iph, unsigned int proto_off,
-		int inverse)
+
+static struct ip_vs_conn *tcp_conn_in_get(int af, const struct sk_buff *skb,
+					  struct ip_vs_protocol *pp,
+					  const struct ip_vs_iphdr *iph,
+					  unsigned int proto_off, int inverse,
+					  int *res_dir)
 {
 	__be16 _ports[2], *pptr;
 
@@ -40,20 +52,21 @@ tcp_conn_in_get(int af, const struct sk_
 		return NULL;
 
 	if (likely(!inverse)) {
-		return ip_vs_conn_in_get(af, iph->protocol,
-					 &iph->saddr, pptr[0],
-					 &iph->daddr, pptr[1]);
+		return ip_vs_conn_get(af, iph->protocol,
+				      &iph->saddr, pptr[0],
+				      &iph->daddr, pptr[1], res_dir);
 	} else {
-		return ip_vs_conn_in_get(af, iph->protocol,
-					 &iph->daddr, pptr[1],
-					 &iph->saddr, pptr[0]);
+		return ip_vs_conn_get(af, iph->protocol,
+				      &iph->daddr, pptr[1],
+				      &iph->saddr, pptr[0], res_dir);
 	}
 }
 
-static struct ip_vs_conn *
-tcp_conn_out_get(int af, const struct sk_buff *skb, struct ip_vs_protocol *pp,
-		 const struct ip_vs_iphdr *iph, unsigned int proto_off,
-		 int inverse)
+static struct ip_vs_conn *tcp_conn_out_get(int af, const struct sk_buff *skb,
+					   struct ip_vs_protocol *pp,
+					   const struct ip_vs_iphdr *iph,
+					   unsigned int proto_off, int inverse,
+					   int *res_dir)
 {
 	__be16 _ports[2], *pptr;
 
@@ -62,17 +75,16 @@ tcp_conn_out_get(int af, const struct sk
 		return NULL;
 
 	if (likely(!inverse)) {
-		return ip_vs_conn_out_get(af, iph->protocol,
-					  &iph->saddr, pptr[0],
-					  &iph->daddr, pptr[1]);
+		return ip_vs_conn_get(af, iph->protocol,
+				      &iph->saddr, pptr[0],
+				      &iph->daddr, pptr[1], res_dir);
 	} else {
-		return ip_vs_conn_out_get(af, iph->protocol,
-					  &iph->daddr, pptr[1],
-					  &iph->saddr, pptr[0]);
+		return ip_vs_conn_get(af, iph->protocol,
+				      &iph->daddr, pptr[1],
+				      &iph->saddr, pptr[0], res_dir);
 	}
 }
 
-
 static int
 tcp_conn_schedule(int af, struct sk_buff *skb, struct ip_vs_protocol *pp,
 		  int *verdict, struct ip_vs_conn **cpp)
@@ -89,7 +101,39 @@ tcp_conn_schedule(int af, struct sk_buff
 		return 0;
 	}
 
-	if (th->syn &&
+
+	/*
+	 * Syn-proxy step 2 logic: receive client's
+	 * 3-handshake Ack packet
+	 */
+	if (ip_vs_synproxy_ack_rcv(af, skb, th, pp, cpp, &iph, verdict) == 0) {
+		return 0;
+	}
+
+
+	if( af & IP_VS_CONN_F_DSNAT ){
+		if (th->syn && 
+			(svc = ip_vs_service_get(af, 
+			skb->mark, iph.protocol, &iph.daddr,th->dest))) {
+			if (ip_vs_todrop()) {
+				ip_vs_service_put(svc);			
+				*verdict = NF_DROP;			
+				return 0;		
+			}
+			*cpp = ip_vs_schedule(svc, skb, 0);
+			if (!*cpp) {			
+				*verdict = ip_vs_leave(svc, skb, pp);			
+				return 0;		
+			}		
+			ip_vs_service_put(svc);	
+		}
+		return 1;
+
+		
+	}
+
+
+	if (th->syn && !th->ack && !th->fin && !th->rst &&
 	    (svc = ip_vs_service_get(af, skb->mark, iph.protocol, &iph.daddr,
 				     th->dest))) {
 		if (ip_vs_todrop()) {
@@ -102,21 +146,31 @@ tcp_conn_schedule(int af, struct sk_buff
 			return 0;
 		}
 
+
 		/*
 		 * Let the virtual server select a real server for the
 		 * incoming connection, and create a connection entry.
-		 */
-		*cpp = ip_vs_schedule(svc, skb);
+		 */		
+		*cpp = ip_vs_schedule(svc, skb, 0);
 		if (!*cpp) {
 			*verdict = ip_vs_leave(svc, skb, pp);
 			return 0;
 		}
 		ip_vs_service_put(svc);
+		return 1;
+	}
+
+	/* drop tcp packet which send to vip and !vport */
+	if (sysctl_ip_vs_tcp_drop_entry &&
+	    (svc = ip_vs_lookup_vip(af, iph.protocol, &iph.daddr))) {
+		IP_VS_INC_ESTATS(ip_vs_esmib, DEFENCE_TCP_DROP);
+		*verdict = NF_DROP;
+		return 0;
 	}
+
 	return 1;
 }
 
-
 static inline void
 tcp_fast_csum_update(int af, struct tcphdr *tcph,
 		     const union nf_inet_addr *oldip,
@@ -126,38 +180,184 @@ tcp_fast_csum_update(int af, struct tcph
 #ifdef CONFIG_IP_VS_IPV6
 	if (af == AF_INET6)
 		tcph->check =
-			csum_fold(ip_vs_check_diff16(oldip->ip6, newip->ip6,
-					 ip_vs_check_diff2(oldport, newport,
-						~csum_unfold(tcph->check))));
+		    csum_fold(ip_vs_check_diff16(oldip->ip6, newip->ip6,
+						 ip_vs_check_diff2(oldport,
+								   newport,
+								   ~csum_unfold
+								   (tcph->
+								    check))));
 	else
 #endif
-	tcph->check =
-		csum_fold(ip_vs_check_diff4(oldip->ip, newip->ip,
-				 ip_vs_check_diff2(oldport, newport,
-						~csum_unfold(tcph->check))));
+		tcph->check =
+		    csum_fold(ip_vs_check_diff4(oldip->ip, newip->ip,
+						ip_vs_check_diff2(oldport,
+								  newport,
+								  ~csum_unfold
+								  (tcph->
+								   check))));
 }
 
-
 static inline void
 tcp_partial_csum_update(int af, struct tcphdr *tcph,
-		     const union nf_inet_addr *oldip,
-		     const union nf_inet_addr *newip,
-		     __be16 oldlen, __be16 newlen)
+			const union nf_inet_addr *oldip,
+			const union nf_inet_addr *newip,
+			__be16 oldlen, __be16 newlen)
 {
 #ifdef CONFIG_IP_VS_IPV6
 	if (af == AF_INET6)
 		tcph->check =
-			csum_fold(ip_vs_check_diff16(oldip->ip6, newip->ip6,
-					 ip_vs_check_diff2(oldlen, newlen,
-						~csum_unfold(tcph->check))));
+		    csum_fold(ip_vs_check_diff16(oldip->ip6, newip->ip6,
+						 ip_vs_check_diff2(oldlen,
+								   newlen,
+								   ~csum_unfold
+								   (tcph->
+								    check))));
 	else
 #endif
-	tcph->check =
-		csum_fold(ip_vs_check_diff4(oldip->ip, newip->ip,
-				ip_vs_check_diff2(oldlen, newlen,
-						~csum_unfold(tcph->check))));
+		tcph->check =
+		    csum_fold(ip_vs_check_diff4(oldip->ip, newip->ip,
+						ip_vs_check_diff2(oldlen,
+								  newlen,
+								  ~csum_unfold
+								  (tcph->
+								   check))));
+}
+
+/* adjust tcp opt mss, sub TCPOLEN_CIP */
+static void tcp_opt_adjust_mss(struct tcphdr *tcph)
+{
+	unsigned char *ptr;
+	int length;
+
+	if (sysctl_ip_vs_mss_adjust_entry == 0)
+		return;
+
+	ptr = (unsigned char *)(tcph + 1);
+	length = (tcph->doff * 4) - sizeof(struct tcphdr);
+
+	while (length > 0) {
+		int opcode = *ptr++;
+		int opsize;
+
+		switch (opcode) {
+		case TCPOPT_EOL:
+			return;
+		case TCPOPT_NOP:	/* Ref: RFC 793 section 3.1 */
+			length--;
+			continue;
+		default:
+			opsize = *ptr++;
+			if (opsize < 2)	/* "silly options" */
+				return;
+			if (opsize > length)
+				return;	/* don't parse partial options */
+			if ((opcode == TCPOPT_MSS) && (opsize == TCPOLEN_MSS)) {
+				__u16 in_mss = ntohs(*(__u16 *) ptr);
+				in_mss -= TCPOLEN_ADDR;
+				*((__u16 *) ptr) = htons(in_mss);	/* set mss, 16bit */
+				return;
+			}
+
+			ptr += opsize - 2;
+			length -= opsize;
+		}
+	}
 }
 
+/* save tcp sequense for fullnat/nat, INside to OUTside */
+static void
+tcp_save_out_seq(struct sk_buff *skb, struct ip_vs_conn *cp,
+		 struct tcphdr *th, int ihl)
+{
+	if (unlikely(th == NULL) || unlikely(cp == NULL) ||
+	    unlikely(skb == NULL))
+		return;
+
+	if (sysctl_ip_vs_conn_expire_tcp_rst && !th->rst) {
+
+		/* seq out of order. just skip */
+		if (before(ntohl(th->ack_seq), ntohl(cp->rs_ack_seq)) &&
+							(cp->rs_ack_seq != 0))
+			return;
+
+		if (th->syn && th->ack)
+			cp->rs_end_seq = htonl(ntohl(th->seq) + 1);
+		else
+			cp->rs_end_seq = htonl(ntohl(th->seq) + skb->len
+					       - ihl - (th->doff << 2));
+		cp->rs_ack_seq = th->ack_seq;
+		IP_VS_DBG_RL("packet from RS, seq:%u ack_seq:%u.",
+			     ntohl(th->seq), ntohl(th->ack_seq));
+		IP_VS_DBG_RL("port:%u->%u", ntohs(th->source), ntohs(th->dest));
+	}
+}
+
+/*
+ * 1. adjust tcp ack/sack sequence for FULL-NAT, INside to OUTside
+ * 2. adjust tcp sequence for SYNPROXY, OUTside to INside
+ */
+static int tcp_out_adjust_seq(struct ip_vs_conn *cp, struct tcphdr *tcph)
+{
+	__u8 i;
+	__u8 *ptr;
+	int length;
+
+	/*
+	 * Syn-proxy seq change, include tcp hdr and
+	 * check ack storm.
+	 */
+	if (ip_vs_synproxy_snat_handler(tcph, cp) == 0) {
+		return 0;
+	}
+
+	/*
+	 * FULLNAT ack-seq change
+	 */
+
+	/* adjust ack sequence */
+	tcph->ack_seq = htonl(ntohl(tcph->ack_seq) - cp->fnat_seq.delta);
+
+	/* adjust sack sequence */
+	ptr = (__u8 *) (tcph + 1);
+	length = (tcph->doff * 4) - sizeof(struct tcphdr);
+
+	while (length > 0) {
+		int opcode = *ptr++;
+		int opsize;
+
+		switch (opcode) {
+		case TCPOPT_EOL:
+			return 1;
+		case TCPOPT_NOP:	/* Ref: RFC 793 section 3.1 */
+			length--;
+			continue;
+		default:
+			opsize = *ptr++;
+			if (opsize < 2)	/* "silly options" */
+				return 1;
+			if (opsize > length)
+				return 1;	/* don't parse partial options */
+			if ((opcode == TCPOPT_SACK) &&
+			    (opsize >=
+			     (TCPOLEN_SACK_BASE + TCPOLEN_SACK_PERBLOCK))
+			    && !((opsize - TCPOLEN_SACK_BASE) %
+				 TCPOLEN_SACK_PERBLOCK)) {
+				for (i = 0; i < opsize - TCPOLEN_SACK_BASE;
+				     i += 4) {
+					*((__u32 *) ptr + i) =
+					    htonl(ntohl(*((__u32 *) ptr + i)) -
+						  cp->fnat_seq.delta);
+				}
+				return 1;
+			}
+
+			ptr += opsize - 2;
+			length -= opsize;
+		}
+	}
+
+	return 1;
+}
 
 static int
 tcp_snat_handler(struct sk_buff *skb,
@@ -176,7 +376,7 @@ tcp_snat_handler(struct sk_buff *skb,
 	oldlen = skb->len - tcphoff;
 
 	/* csum_check requires unshared skb */
-	if (!skb_make_writable(skb, tcphoff+sizeof(*tcph)))
+	if (!skb_make_writable(skb, tcphoff + sizeof(*tcph)))
 		return 0;
 
 	if (unlikely(cp->app != NULL)) {
@@ -190,8 +390,17 @@ tcp_snat_handler(struct sk_buff *skb,
 	}
 
 	tcph = (void *)skb_network_header(skb) + tcphoff;
+	tcp_save_out_seq(skb, cp, tcph, tcphoff);
 	tcph->source = cp->vport;
 
+	/*
+	 * Syn-proxy seq change, include tcp hdr and
+	 * check ack storm.
+	 */
+	if (ip_vs_synproxy_snat_handler(tcph, cp) == 0) {
+		return 0;
+	}
+
 	/* Adjust TCP checksums */
 	if (skb->ip_summed == CHECKSUM_PARTIAL) {
 		tcp_partial_csum_update(cp->af, tcph, &cp->daddr, &cp->vaddr,
@@ -223,11 +432,299 @@ tcp_snat_handler(struct sk_buff *skb,
 
 		IP_VS_DBG(11, "O-pkt: %s O-csum=%d (+%zd)\n",
 			  pp->name, tcph->check,
-			  (char*)&(tcph->check) - (char*)tcph);
+			  (char *)&(tcph->check) - (char *)tcph);
 	}
 	return 1;
 }
 
+static int
+tcp_fnat_out_handler(struct sk_buff *skb,
+		     struct ip_vs_protocol *pp, struct ip_vs_conn *cp)
+{
+	struct tcphdr *tcph;
+	unsigned int tcphoff;
+	int oldlen;
+
+#ifdef CONFIG_IP_VS_IPV6
+	if (cp->af == AF_INET6)
+		tcphoff = sizeof(struct ipv6hdr);
+	else
+#endif
+		tcphoff = ip_hdrlen(skb);
+	oldlen = skb->len - tcphoff;
+
+	/* csum_check requires unshared skb */
+	if (!skb_make_writable(skb, tcphoff + sizeof(*tcph)))
+		return 0;
+
+	if (unlikely(cp->app != NULL)) {
+		/* Some checks before mangling */
+		if (pp->csum_check && !pp->csum_check(cp->af, skb, pp))
+			return 0;
+
+		/* Call application helper if needed */
+		if (!ip_vs_app_pkt_out(cp, skb))
+			return 0;
+	}
+
+	tcph = (void *)skb_network_header(skb) + tcphoff;
+	tcp_save_out_seq(skb, cp, tcph, tcphoff);
+	tcph->source = cp->vport;
+	tcph->dest = cp->cport;
+
+	/*
+	 * adjust tcp opt mss in rs->client syn_ack packet
+	 */
+	if (tcph->syn && tcph->ack) {
+		tcp_opt_adjust_mss(tcph);
+	}
+
+	/* adjust tcp ack/sack sequence */
+	if (tcp_out_adjust_seq(cp, tcph) == 0) {
+		return 0;
+	}
+
+	/* full checksum calculation */
+	tcph->check = 0;
+	skb->csum = skb_checksum(skb, tcphoff, skb->len - tcphoff, 0);
+#ifdef CONFIG_IP_VS_IPV6
+	if (cp->af == AF_INET6)
+		tcph->check = csum_ipv6_magic(&cp->vaddr.in6,
+					      &cp->caddr.in6,
+					      skb->len - tcphoff,
+					      cp->protocol, skb->csum);
+	else
+#endif
+		tcph->check = csum_tcpudp_magic(cp->vaddr.ip,
+						cp->caddr.ip,
+						skb->len - tcphoff,
+						cp->protocol, skb->csum);
+
+	IP_VS_DBG(11, "O-pkt: %s O-csum=%d (+%zd)\n",
+		  pp->name, tcph->check, (char *)&(tcph->check) - (char *)tcph);
+
+	return 1;
+}
+
+/*
+ * remove tcp timestamp opt in one packet, just set it to TCPOPT_NOP
+ * reference to tcp_parse_options in tcp_input.c
+ */
+static void tcp_opt_remove_timestamp(struct tcphdr *tcph)
+{
+	unsigned char *ptr;
+	int length;
+	int i;
+
+	if (sysctl_ip_vs_timestamp_remove_entry == 0)
+		return;
+
+	ptr = (unsigned char *)(tcph + 1);
+	length = (tcph->doff * 4) - sizeof(struct tcphdr);
+
+	while (length > 0) {
+		int opcode = *ptr++;
+		int opsize;
+
+		switch (opcode) {
+		case TCPOPT_EOL:
+			return;
+		case TCPOPT_NOP:	/* Ref: RFC 793 section 3.1 */
+			length--;
+			continue;
+		default:
+			opsize = *ptr++;
+			if (opsize < 2)	/* "silly options" */
+				return;
+			if (opsize > length)
+				return;	/* don't parse partial options */
+			if ((opcode == TCPOPT_TIMESTAMP)
+			    && (opsize == TCPOLEN_TIMESTAMP)) {
+				for (i = 0; i < TCPOLEN_TIMESTAMP; i++) {
+					*(ptr - 2 + i) = TCPOPT_NOP;	/* TCPOPT_NOP replace timestamp opt */
+				}
+				return;
+			}
+
+			ptr += opsize - 2;
+			length -= opsize;
+		}
+	}
+}
+
+/*
+ * 1. recompute tcp sequence, OUTside to INside;
+ * 2. init first data sequence;
+ */
+static void
+tcp_in_init_seq(struct ip_vs_conn *cp, struct sk_buff *skb, struct tcphdr *tcph)
+{
+	struct ip_vs_seq *fseq = &(cp->fnat_seq);
+	__u32 seq = ntohl(tcph->seq);
+	int conn_reused_entry;
+
+	/* init first data seq and reset toa flag */
+	fseq->fdata_seq = seq + 1;
+	cp->flags &= ~IP_VS_CONN_F_CIP_INSERTED;
+
+	/* init syn seq, lvs2rs */
+	conn_reused_entry = (sysctl_ip_vs_conn_reused_entry == 1)
+	    && (fseq->init_seq != 0)
+	    && ((cp->state == IP_VS_TCP_S_SYN_RECV)
+		|| (cp->state == IP_VS_TCP_S_SYN_SENT));
+	if ((fseq->init_seq == 0) || conn_reused_entry) {
+#ifdef CONFIG_IP_VS_IPV6
+		if (cp->af == AF_INET6)
+			fseq->init_seq =
+			    secure_tcpv6_sequence_number(cp->laddr.ip6,
+							 cp->daddr.ip6,
+							 cp->lport, cp->dport);
+		else
+#endif
+			fseq->init_seq =
+			    secure_tcp_sequence_number(cp->laddr.ip,
+						       cp->daddr.ip, cp->lport,
+						       cp->dport);
+		fseq->delta = fseq->init_seq - seq;
+
+		if (conn_reused_entry) {
+			IP_VS_INC_ESTATS(ip_vs_esmib, FULLNAT_CONN_REUSED);
+			switch (cp->old_state) {
+			case IP_VS_TCP_S_CLOSE:
+				IP_VS_INC_ESTATS(ip_vs_esmib,
+						 FULLNAT_CONN_REUSED_CLOSE);
+				break;
+			case IP_VS_TCP_S_TIME_WAIT:
+				IP_VS_INC_ESTATS(ip_vs_esmib,
+						 FULLNAT_CONN_REUSED_TIMEWAIT);
+				break;
+			case IP_VS_TCP_S_FIN_WAIT:
+				IP_VS_INC_ESTATS(ip_vs_esmib,
+						 FULLNAT_CONN_REUSED_FINWAIT);
+				break;
+			case IP_VS_TCP_S_CLOSE_WAIT:
+				IP_VS_INC_ESTATS(ip_vs_esmib,
+						 FULLNAT_CONN_REUSED_CLOSEWAIT);
+				break;
+			case IP_VS_TCP_S_LAST_ACK:
+				IP_VS_INC_ESTATS(ip_vs_esmib,
+						 FULLNAT_CONN_REUSED_LASTACK);
+				break;
+			case IP_VS_TCP_S_ESTABLISHED:
+				IP_VS_INC_ESTATS(ip_vs_esmib,
+						 FULLNAT_CONN_REUSED_ESTAB);
+				break;
+			}
+		}
+	}
+}
+
+/* adjust tcp sequence, OUTside to INside */
+static void tcp_in_adjust_seq(struct ip_vs_conn *cp, struct tcphdr *tcph)
+{
+	/* adjust seq for FULLNAT */
+	tcph->seq = htonl(ntohl(tcph->seq) + cp->fnat_seq.delta);
+
+	/* adjust ack_seq for SYNPROXY, include tcp hdr and sack opt */
+	ip_vs_synproxy_dnat_handler(tcph, &cp->syn_proxy_seq);
+}
+
+/*
+ * add client address in tcp option
+ * alloc a new skb, and free the old skb
+ * return new skb
+ */
+static struct sk_buff *tcp_opt_add_toa(struct ip_vs_conn *cp,
+				       struct sk_buff *old_skb,
+				       struct tcphdr **tcph)
+{
+	__u32 mtu;
+	struct sk_buff *new_skb = NULL;
+	struct ip_vs_tcpo_addr *toa;
+	struct ip_vs_seq *fseq = &(cp->fnat_seq);
+	__u32 seq = ntohl((*tcph)->seq);
+	unsigned int tcphoff;
+	struct tcphdr *th;
+	__u8 *p, *q;
+
+	/* now only process IPV4 */
+	if (cp->af != AF_INET) {
+		IP_VS_INC_ESTATS(ip_vs_esmib, FULLNAT_ADD_TOA_FAIL_PROTO);
+		return old_skb;
+	}
+
+	/* stop insert tcp option address here */
+	if (after(seq, fseq->fdata_seq)) {
+		cp->flags |= IP_VS_CONN_F_CIP_INSERTED;
+		return old_skb;
+	}
+
+	/* skb length checking */
+	mtu = dst_mtu((struct dst_entry *)old_skb->_skb_dst);
+	if (old_skb->len > (mtu - sizeof(struct ip_vs_tcpo_addr))) {
+		IP_VS_INC_ESTATS(ip_vs_esmib, FULLNAT_ADD_TOA_FAIL_LEN);
+		return old_skb;
+	}
+
+	/* copy all skb, plus ttm space , new skb is linear */
+	new_skb = skb_copy_expand(old_skb,
+				  skb_headroom(old_skb),
+				  skb_tailroom(old_skb) +
+				  sizeof(struct ip_vs_tcpo_addr), GFP_ATOMIC);
+	if (new_skb == NULL) {
+		IP_VS_INC_ESTATS(ip_vs_esmib, FULLNAT_ADD_TOA_FAIL_MEM);
+		return old_skb;
+	}
+
+	/* free old skb */
+	kfree_skb(old_skb);
+
+	/*
+	 * add client ip
+	 */
+	tcphoff = ip_hdrlen(new_skb);
+	/* get new tcp header */
+	*tcph = th =
+	    (struct tcphdr *)((void *)skb_network_header(new_skb) + tcphoff);
+
+	/* ptr to old opts */
+	p = skb_tail_pointer(new_skb) - 1;
+	q = p + sizeof(struct ip_vs_tcpo_addr);
+
+	/* move data down, offset is sizeof(struct ip_vs_tcpo_addr) */
+	while (p >= ((__u8 *) th + sizeof(struct tcphdr))) {
+		*q = *p;
+		p--;
+		q--;
+	}
+
+	/* move tail to new postion */
+	new_skb->tail += sizeof(struct ip_vs_tcpo_addr);
+
+	/* put client ip opt , ptr point to opts */
+	toa = (struct ip_vs_tcpo_addr *)(th + 1);
+	toa->opcode = TCPOPT_ADDR;
+	toa->opsize = TCPOLEN_ADDR;
+	toa->port = cp->cport;
+	toa->addr = cp->caddr.ip;
+
+	/* reset tcp header length */
+	th->doff += sizeof(struct ip_vs_tcpo_addr) / 4;
+	/* reset ip header totoal length */
+	ip_hdr(new_skb)->tot_len =
+	    htons(ntohs(ip_hdr(new_skb)->tot_len) +
+		  sizeof(struct ip_vs_tcpo_addr));
+	/* reset skb length */
+	new_skb->len += sizeof(struct ip_vs_tcpo_addr);
+
+	/* re-calculate tcp csum in tcp_fnat_in_handler */
+	/* re-calculate ip csum */
+	ip_send_check(ip_hdr(new_skb));
+
+	IP_VS_INC_ESTATS(ip_vs_esmib, FULLNAT_ADD_TOA_OK);
+
+	return new_skb;
+}
 
 static int
 tcp_dnat_handler(struct sk_buff *skb,
@@ -246,7 +743,7 @@ tcp_dnat_handler(struct sk_buff *skb,
 	oldlen = skb->len - tcphoff;
 
 	/* csum_check requires unshared skb */
-	if (!skb_make_writable(skb, tcphoff+sizeof(*tcph)))
+	if (!skb_make_writable(skb, tcphoff + sizeof(*tcph)))
 		return 0;
 
 	if (unlikely(cp->app != NULL)) {
@@ -255,8 +752,8 @@ tcp_dnat_handler(struct sk_buff *skb,
 			return 0;
 
 		/*
-		 *	Attempt ip_vs_app call.
-		 *	It will fix ip_vs_conn and iph ack_seq stuff
+		 *      Attempt ip_vs_app call.
+		 *      It will fix ip_vs_conn and iph ack_seq stuff
 		 */
 		if (!ip_vs_app_pkt_in(cp, skb))
 			return 0;
@@ -266,7 +763,12 @@ tcp_dnat_handler(struct sk_buff *skb,
 	tcph->dest = cp->dport;
 
 	/*
-	 *	Adjust TCP checksums
+	 * Syn-proxy ack_seq change, include tcp hdr and sack opt.
+	 */
+	ip_vs_synproxy_dnat_handler(tcph, &cp->syn_proxy_seq);
+
+	/*
+	 *      Adjust TCP checksums
 	 */
 	if (skb->ip_summed == CHECKSUM_PARTIAL) {
 		tcp_partial_csum_update(cp->af, tcph, &cp->daddr, &cp->vaddr,
@@ -300,6 +802,329 @@ tcp_dnat_handler(struct sk_buff *skb,
 	return 1;
 }
 
+static int
+tcp_fnat_in_handler(struct sk_buff **skb_p,
+		    struct ip_vs_protocol *pp, struct ip_vs_conn *cp)
+{
+	struct tcphdr *tcph;
+	unsigned int tcphoff;
+	int oldlen;
+	struct sk_buff *skb = *skb_p;
+
+#ifdef CONFIG_IP_VS_IPV6
+	if (cp->af == AF_INET6)
+		tcphoff = sizeof(struct ipv6hdr);
+	else
+#endif
+		tcphoff = ip_hdrlen(skb);
+	oldlen = skb->len - tcphoff;
+
+	/* csum_check requires unshared skb */
+	if (!skb_make_writable(skb, tcphoff + sizeof(*tcph)))
+		return 0;
+
+	if (unlikely(cp->app != NULL)) {
+		/* Some checks before mangling */
+		if (pp->csum_check && !pp->csum_check(cp->af, skb, pp))
+			return 0;
+
+		/*
+		 *      Attempt ip_vs_app call.
+		 *      It will fix ip_vs_conn and iph ack_seq stuff
+		 */
+		if (!ip_vs_app_pkt_in(cp, skb))
+			return 0;
+	}
+
+	tcph = (void *)skb_network_header(skb) + tcphoff;
+	tcph->source = cp->lport;
+	tcph->dest = cp->dport;
+
+	/*
+	 * for syn packet
+	 * 1. remove tcp timestamp opt,
+	 *    because local address with diffrent client have the diffrent timestamp;
+	 * 2. recompute tcp sequence
+	 */
+	if (tcph->syn & !tcph->ack) {
+		tcp_opt_remove_timestamp(tcph);
+		tcp_in_init_seq(cp, skb, tcph);
+	}
+
+	/* TOA: add client ip */
+	if ((sysctl_ip_vs_toa_entry == 1)
+	    && !(cp->flags & IP_VS_CONN_F_CIP_INSERTED)
+	    && !tcph->rst && !tcph->fin) {
+		skb = *skb_p = tcp_opt_add_toa(cp, skb, &tcph);
+	}
+
+	/*
+	 * adjust tcp sequence, becase
+	 * 1. FULLNAT: local address with diffrent client have the diffrent sequence
+	 * 2. SYNPROXY: dont know rs->client synack sequence
+	 */
+	tcp_in_adjust_seq(cp, tcph);
+
+	/* full checksum calculation */
+	tcph->check = 0;
+	skb->csum = skb_checksum(skb, tcphoff, skb->len - tcphoff, 0);
+#ifdef CONFIG_IP_VS_IPV6
+	if (cp->af == AF_INET6)
+		tcph->check = csum_ipv6_magic(&cp->laddr.in6,
+					      &cp->daddr.in6,
+					      skb->len - tcphoff,
+					      cp->protocol, skb->csum);
+	else
+#endif
+		tcph->check = csum_tcpudp_magic(cp->laddr.ip,
+						cp->daddr.ip,
+						skb->len - tcphoff,
+						cp->protocol, skb->csum);
+	skb->ip_summed = CHECKSUM_UNNECESSARY;
+
+	return 1;
+}
+
+/* send reset packet to RS */
+static void tcp_send_rst_in(struct ip_vs_protocol *pp, struct ip_vs_conn *cp)
+{
+	struct sk_buff *skb = NULL;
+	struct sk_buff *tmp_skb = NULL;
+	struct tcphdr *th;
+	unsigned int tcphoff;
+
+	skb = alloc_skb(MAX_TCP_HEADER, GFP_ATOMIC);
+	if (unlikely(skb == NULL)) {
+		IP_VS_ERR_RL("alloc skb failed when send rs RST packet\n");
+		return;
+	}
+
+	skb_reserve(skb, MAX_TCP_HEADER);
+	th = (struct tcphdr *)skb_push(skb, sizeof(struct tcphdr));
+	skb_reset_transport_header(skb);
+	skb->csum = 0;
+
+	/* set tcp head */
+	memset(th, 0, sizeof(struct tcphdr));
+	th->source = cp->cport;
+	th->dest = cp->vport;
+
+	/* set the reset seq of tcp head */
+	if ((cp->state == IP_VS_TCP_S_SYN_SENT) &&
+			((tmp_skb = skb_dequeue(&cp->ack_skb)) != NULL)) {
+		struct tcphdr *tcph;
+#ifdef CONFIG_IP_VS_IPV6
+		if (cp->af == AF_INET6)
+			tcphoff = sizeof(struct ipv6hdr);
+		else
+#endif
+			tcphoff = ip_hdrlen(tmp_skb);
+		tcph = (void *)skb_network_header(tmp_skb) + tcphoff;
+
+		th->seq = tcph->seq;
+		/* put back. Just for sending reset packet to client */
+		skb_queue_head(&cp->ack_skb, tmp_skb);
+	} else if (cp->state == IP_VS_TCP_S_ESTABLISHED) {
+		th->seq = cp->rs_ack_seq;
+		/* Be careful! fullnat */
+		if (cp->flags & IP_VS_CONN_F_FULLNAT)
+			th->seq = htonl(ntohl(th->seq) - cp->fnat_seq.delta);
+	} else {
+		kfree_skb(skb);
+		IP_VS_DBG_RL("IPVS: Is SYN_SENT or ESTABLISHED ?");
+		return;
+	}
+
+	IP_VS_DBG_RL("IPVS: rst to rs seq: %u", htonl(th->seq));
+	th->ack_seq = 0;
+	th->doff = sizeof(struct tcphdr) >> 2;
+	th->rst = 1;
+
+	/*
+	 * Set ip hdr
+	 * Attention: set source and dest addr to ack skb's.
+	 * we rely on packet_xmit func to do NATs thing.
+	 */
+#ifdef CONFIG_IP_VS_IPV6
+	if (cp->af == AF_INET6) {
+		struct ipv6hdr *iph =
+		    (struct ipv6hdr *)skb_push(skb, sizeof(struct iphdr));
+
+		tcphoff = sizeof(struct ipv6hdr);
+		skb_reset_network_header(skb);
+		memcpy(&iph->saddr, &cp->caddr.in6, sizeof(struct in6_addr));
+		memcpy(&iph->daddr, &cp->vaddr.in6, sizeof(struct in6_addr));
+
+		iph->version = 6;
+		iph->nexthdr = NEXTHDR_TCP;
+		iph->hop_limit = IPV6_DEFAULT_HOPLIMIT;
+
+		th->check = 0;
+		skb->csum = skb_checksum(skb, tcphoff, skb->len - tcphoff, 0);
+		th->check = csum_ipv6_magic(&iph->saddr, &iph->daddr,
+					    skb->len - tcphoff,
+					    IPPROTO_TCP, skb->csum);
+	} else
+#endif
+	{
+		struct iphdr *iph =
+		    (struct iphdr *)skb_push(skb, sizeof(struct iphdr));
+
+		tcphoff = sizeof(struct iphdr);
+		skb_reset_network_header(skb);
+		iph->version = 4;
+		iph->ihl = 5;
+		iph->tot_len = htons(skb->len);
+		iph->frag_off = htons(IP_DF);
+		iph->ttl = IPDEFTTL;
+		iph->protocol = IPPROTO_TCP;
+		iph->saddr = cp->caddr.ip;
+		iph->daddr = cp->vaddr.ip;
+
+		ip_send_check(iph);
+
+		th->check = 0;
+		skb->csum = skb_checksum(skb, tcphoff, skb->len - tcphoff, 0);
+		th->check = csum_tcpudp_magic(iph->saddr, iph->daddr,
+					      skb->len - tcphoff,
+					      IPPROTO_TCP, skb->csum);
+	}
+
+	cp->packet_xmit(skb, cp, pp);
+}
+
+/* send reset packet to client */
+static void tcp_send_rst_out(struct ip_vs_protocol *pp, struct ip_vs_conn *cp)
+{
+	struct sk_buff *skb = NULL;
+	struct sk_buff *tmp_skb = NULL;
+	struct tcphdr *th;
+	unsigned int tcphoff;
+
+	skb = alloc_skb(MAX_TCP_HEADER, GFP_ATOMIC);
+	if (unlikely(skb == NULL)) {
+		IP_VS_ERR_RL("alloc skb failed when send client RST packet\n");
+		return;
+	}
+
+	skb_reserve(skb, MAX_TCP_HEADER);
+	th = (struct tcphdr *)skb_push(skb, sizeof(struct tcphdr));
+	skb_reset_transport_header(skb);
+	skb->csum = 0;
+
+	/* set tcp head */
+	memset(th, 0, sizeof(struct tcphdr));
+	th->source = cp->dport;
+	if (cp->flags & IP_VS_CONN_F_FULLNAT)
+		th->dest = cp->lport;
+	else
+		th->dest = cp->cport;
+
+	/* set the reset seq of tcp head*/
+	if ((cp->state == IP_VS_TCP_S_SYN_SENT) &&
+			((tmp_skb = skb_dequeue(&cp->ack_skb)) != NULL)) {
+		struct tcphdr *tcph;
+#ifdef CONFIG_IP_VS_IPV6
+		if (cp->af == AF_INET6)
+			tcphoff = sizeof(struct ipv6hdr);
+		else
+#endif
+			tcphoff = ip_hdrlen(tmp_skb);
+		tcph = (void *)skb_network_header(tmp_skb) + tcphoff;
+		/* Perhaps delta is 0 */
+		th->seq = htonl(ntohl(tcph->ack_seq) - cp->syn_proxy_seq.delta);
+		/* put back. Just for sending reset packet to RS */
+		skb_queue_head(&cp->ack_skb, tmp_skb);
+	} else if (cp->state == IP_VS_TCP_S_ESTABLISHED) {
+		th->seq = cp->rs_end_seq;
+	} else {
+		kfree_skb(skb);
+		IP_VS_DBG_RL("IPVS: Is in SYN_SENT or ESTABLISHED ?");
+		return;
+	}
+
+	IP_VS_DBG_RL("IPVS: rst to client seq: %u", htonl(th->seq));
+	th->ack_seq = 0;
+	th->doff = sizeof(struct tcphdr) >> 2;
+	th->rst = 1;
+
+	/*
+	 * Set ip hdr
+	 * Attention: set source and dest addr to ack skb's.
+	 * we rely on response_xmit func to do NATs thing.
+	 */
+#ifdef CONFIG_IP_VS_IPV6
+	if (cp->af == AF_INET6) {
+		struct ipv6hdr *iph =
+		    (struct ipv6hdr *)skb_push(skb, sizeof(struct iphdr));
+
+		tcphoff = sizeof(struct ipv6hdr);
+		skb_reset_network_header(skb);
+		memcpy(&iph->saddr, &cp->daddr.in6, sizeof(struct in6_addr));
+		memcpy(&iph->daddr, &cp->laddr.in6, sizeof(struct in6_addr));
+
+		iph->version = 6;
+		iph->nexthdr = NEXTHDR_TCP;
+		iph->hop_limit = IPV6_DEFAULT_HOPLIMIT;
+
+		th->check = 0;
+		skb->csum = skb_checksum(skb, tcphoff, skb->len - tcphoff, 0);
+		th->check = csum_ipv6_magic(&iph->saddr, &iph->daddr,
+					    skb->len - tcphoff,
+					    IPPROTO_TCP, skb->csum);
+
+		if (cp->flags & IP_VS_CONN_F_FULLNAT)
+			ip_vs_fnat_response_xmit_v6(skb, pp, cp,
+						    sizeof(struct ipv6hdr));
+		else
+			ip_vs_normal_response_xmit_v6(skb, pp, cp,
+						      sizeof(struct ipv6hdr));
+	} else
+#endif
+	{
+		struct iphdr *iph =
+		    (struct iphdr *)skb_push(skb, sizeof(struct iphdr));
+
+		tcphoff = sizeof(struct iphdr);
+		skb_reset_network_header(skb);
+		iph->version = 4;
+		iph->ihl = 5;
+		iph->tot_len = htons(skb->len);
+		iph->frag_off = htons(IP_DF);
+		iph->ttl = IPDEFTTL;
+		iph->protocol = IPPROTO_TCP;
+		iph->saddr = cp->daddr.ip;
+		iph->daddr = cp->laddr.ip;
+
+		ip_send_check(iph);
+
+		th->check = 0;
+		skb->csum = skb_checksum(skb, tcphoff, skb->len - tcphoff, 0);
+		th->check = csum_tcpudp_magic(iph->saddr, iph->daddr,
+					      skb->len - tcphoff,
+					      IPPROTO_TCP, skb->csum);
+
+		if (cp->flags & IP_VS_CONN_F_FULLNAT)
+			ip_vs_fnat_response_xmit(skb, pp, cp, iph->ihl << 2);
+		else
+			ip_vs_normal_response_xmit(skb, pp, cp, iph->ihl << 2);
+	}
+}
+
+static void
+tcp_conn_expire_handler(struct ip_vs_protocol *pp, struct ip_vs_conn *cp)
+{
+	/* support fullnat and nat */
+	if (sysctl_ip_vs_conn_expire_tcp_rst &&
+	    (cp->flags & (IP_VS_CONN_F_FULLNAT | IP_VS_CONN_F_MASQ))) {
+	    
+	    IP_VS_DBG(11, "tcp_conn_expire_handler cp->state[%d]\n",cp->state);
+		/* send reset packet to RS */
+		tcp_send_rst_in(pp, cp);
+		/* send reset packet to client */
+		tcp_send_rst_out(pp, cp);
+	}
+}
 
 static int
 tcp_csum_check(int af, struct sk_buff *skb, struct ip_vs_protocol *pp)
@@ -330,15 +1155,14 @@ tcp_csum_check(int af, struct sk_buff *s
 			}
 		} else
 #endif
-			if (csum_tcpudp_magic(ip_hdr(skb)->saddr,
+		if (csum_tcpudp_magic(ip_hdr(skb)->saddr,
 					      ip_hdr(skb)->daddr,
 					      skb->len - tcphoff,
 					      ip_hdr(skb)->protocol,
 					      skb->csum)) {
-				IP_VS_DBG_RL_PKT(0, pp, skb, 0,
-						 "Failed checksum for");
-				return 0;
-			}
+			IP_VS_DBG_RL_PKT(0, pp, skb, 0, "Failed checksum for");
+			return 0;
+		}
 		break;
 	default:
 		/* No need to checksum. */
@@ -348,48 +1172,47 @@ tcp_csum_check(int af, struct sk_buff *s
 	return 1;
 }
 
-
 #define TCP_DIR_INPUT		0
 #define TCP_DIR_OUTPUT		4
 #define TCP_DIR_INPUT_ONLY	8
 
 static const int tcp_state_off[IP_VS_DIR_LAST] = {
-	[IP_VS_DIR_INPUT]		=	TCP_DIR_INPUT,
-	[IP_VS_DIR_OUTPUT]		=	TCP_DIR_OUTPUT,
-	[IP_VS_DIR_INPUT_ONLY]		=	TCP_DIR_INPUT_ONLY,
+	[IP_VS_DIR_INPUT] = TCP_DIR_INPUT,
+	[IP_VS_DIR_OUTPUT] = TCP_DIR_OUTPUT,
+	[IP_VS_DIR_INPUT_ONLY] = TCP_DIR_INPUT_ONLY,
 };
 
 /*
  *	Timeout table[state]
  */
-static int tcp_timeouts[IP_VS_TCP_S_LAST+1] = {
-	[IP_VS_TCP_S_NONE]		=	2*HZ,
-	[IP_VS_TCP_S_ESTABLISHED]	=	15*60*HZ,
-	[IP_VS_TCP_S_SYN_SENT]		=	2*60*HZ,
-	[IP_VS_TCP_S_SYN_RECV]		=	1*60*HZ,
-	[IP_VS_TCP_S_FIN_WAIT]		=	2*60*HZ,
-	[IP_VS_TCP_S_TIME_WAIT]		=	2*60*HZ,
-	[IP_VS_TCP_S_CLOSE]		=	10*HZ,
-	[IP_VS_TCP_S_CLOSE_WAIT]	=	60*HZ,
-	[IP_VS_TCP_S_LAST_ACK]		=	30*HZ,
-	[IP_VS_TCP_S_LISTEN]		=	2*60*HZ,
-	[IP_VS_TCP_S_SYNACK]		=	120*HZ,
-	[IP_VS_TCP_S_LAST]		=	2*HZ,
+int sysctl_ip_vs_tcp_timeouts[IP_VS_TCP_S_LAST + 1] = {
+	[IP_VS_TCP_S_NONE] = 2 * HZ,
+	[IP_VS_TCP_S_ESTABLISHED] = 90 * HZ,
+	[IP_VS_TCP_S_SYN_SENT] = 3 * HZ,
+	[IP_VS_TCP_S_SYN_RECV] = 30 * HZ,
+	[IP_VS_TCP_S_FIN_WAIT] = 3 * HZ,
+	[IP_VS_TCP_S_TIME_WAIT] = 3 * HZ,
+	[IP_VS_TCP_S_CLOSE] = 3 * HZ,
+	[IP_VS_TCP_S_CLOSE_WAIT] = 3 * HZ,
+	[IP_VS_TCP_S_LAST_ACK] = 3 * HZ,
+	[IP_VS_TCP_S_LISTEN] = 2 * 60 * HZ,
+	[IP_VS_TCP_S_SYNACK] = 30 * HZ,
+	[IP_VS_TCP_S_LAST] = 2 * HZ,
 };
 
-static const char *const tcp_state_name_table[IP_VS_TCP_S_LAST+1] = {
-	[IP_VS_TCP_S_NONE]		=	"NONE",
-	[IP_VS_TCP_S_ESTABLISHED]	=	"ESTABLISHED",
-	[IP_VS_TCP_S_SYN_SENT]		=	"SYN_SENT",
-	[IP_VS_TCP_S_SYN_RECV]		=	"SYN_RECV",
-	[IP_VS_TCP_S_FIN_WAIT]		=	"FIN_WAIT",
-	[IP_VS_TCP_S_TIME_WAIT]		=	"TIME_WAIT",
-	[IP_VS_TCP_S_CLOSE]		=	"CLOSE",
-	[IP_VS_TCP_S_CLOSE_WAIT]	=	"CLOSE_WAIT",
-	[IP_VS_TCP_S_LAST_ACK]		=	"LAST_ACK",
-	[IP_VS_TCP_S_LISTEN]		=	"LISTEN",
-	[IP_VS_TCP_S_SYNACK]		=	"SYNACK",
-	[IP_VS_TCP_S_LAST]		=	"BUG!",
+static const char *const tcp_state_name_table[IP_VS_TCP_S_LAST + 1] = {
+	[IP_VS_TCP_S_NONE] = "NONE",
+	[IP_VS_TCP_S_ESTABLISHED] = "ESTABLISHED",
+	[IP_VS_TCP_S_SYN_SENT] = "SYN_SENT",
+	[IP_VS_TCP_S_SYN_RECV] = "SYN_RECV",
+	[IP_VS_TCP_S_FIN_WAIT] = "FIN_WAIT",
+	[IP_VS_TCP_S_TIME_WAIT] = "TIME_WAIT",
+	[IP_VS_TCP_S_CLOSE] = "CLOSE",
+	[IP_VS_TCP_S_CLOSE_WAIT] = "CLOSE_WAIT",
+	[IP_VS_TCP_S_LAST_ACK] = "LAST_ACK",
+	[IP_VS_TCP_S_LISTEN] = "LISTEN",
+	[IP_VS_TCP_S_SYNACK] = "SYNACK",
+	[IP_VS_TCP_S_LAST] = "BUG!",
 };
 
 #define sNO IP_VS_TCP_S_NONE
@@ -408,77 +1231,75 @@ struct tcp_states_t {
 	int next_state[IP_VS_TCP_S_LAST];
 };
 
-static const char * tcp_state_name(int state)
+static const char *tcp_state_name(int state)
 {
 	if (state >= IP_VS_TCP_S_LAST)
 		return "ERR!";
 	return tcp_state_name_table[state] ? tcp_state_name_table[state] : "?";
 }
 
-static struct tcp_states_t tcp_states [] = {
+static struct tcp_states_t tcp_states[] = {
 /*	INPUT */
 /*        sNO, sES, sSS, sSR, sFW, sTW, sCL, sCW, sLA, sLI, sSA	*/
-/*syn*/ {{sSR, sES, sES, sSR, sSR, sSR, sSR, sSR, sSR, sSR, sSR }},
-/*fin*/ {{sCL, sCW, sSS, sTW, sTW, sTW, sCL, sCW, sLA, sLI, sTW }},
-/*ack*/ {{sCL, sES, sSS, sES, sFW, sTW, sCL, sCW, sCL, sLI, sES }},
-/*rst*/ {{sCL, sCL, sCL, sSR, sCL, sCL, sCL, sCL, sLA, sLI, sSR }},
+/*syn*/ {{sSR, sES, sES, sSR, sSR, sSR, sSR, sSR, sSR, sSR, sSR}},
+/*fin*/ {{sCL, sCW, sSS, sTW, sTW, sTW, sCL, sCW, sLA, sLI, sTW}},
+/*ack*/ {{sCL, sES, sSS, sES, sFW, sTW, sCL, sCW, sCL, sLI, sES}},
+/*rst*/ {{sCL, sCL, sCL, sSR, sCL, sCL, sCL, sCL, sLA, sLI, sSR}},
 
 /*	OUTPUT */
 /*        sNO, sES, sSS, sSR, sFW, sTW, sCL, sCW, sLA, sLI, sSA	*/
-/*syn*/ {{sSS, sES, sSS, sSR, sSS, sSS, sSS, sSS, sSS, sLI, sSR }},
-/*fin*/ {{sTW, sFW, sSS, sTW, sFW, sTW, sCL, sTW, sLA, sLI, sTW }},
-/*ack*/ {{sES, sES, sSS, sES, sFW, sTW, sCL, sCW, sLA, sES, sES }},
-/*rst*/ {{sCL, sCL, sSS, sCL, sCL, sTW, sCL, sCL, sCL, sCL, sCL }},
+/*syn*/ {{sSS, sES, sSS, sSR, sSS, sSS, sSS, sSS, sSS, sLI, sSR}},
+/*fin*/ {{sTW, sFW, sSS, sTW, sFW, sTW, sCL, sTW, sLA, sLI, sTW}},
+/*ack*/ {{sES, sES, sES, sES, sFW, sTW, sCL, sCW, sLA, sES, sES}},
+/*rst*/ {{sCL, sCL, sSS, sCL, sCL, sTW, sCL, sCL, sCL, sCL, sCL}},
 
 /*	INPUT-ONLY */
 /*        sNO, sES, sSS, sSR, sFW, sTW, sCL, sCW, sLA, sLI, sSA	*/
-/*syn*/ {{sSR, sES, sES, sSR, sSR, sSR, sSR, sSR, sSR, sSR, sSR }},
-/*fin*/ {{sCL, sFW, sSS, sTW, sFW, sTW, sCL, sCW, sLA, sLI, sTW }},
-/*ack*/ {{sCL, sES, sSS, sES, sFW, sTW, sCL, sCW, sCL, sLI, sES }},
-/*rst*/ {{sCL, sCL, sCL, sSR, sCL, sCL, sCL, sCL, sLA, sLI, sCL }},
+/*syn*/ {{sSR, sES, sES, sSR, sSR, sSR, sSR, sSR, sSR, sSR, sSR}},
+/*fin*/ {{sCL, sFW, sSS, sTW, sFW, sTW, sCL, sCW, sLA, sLI, sTW}},
+/*ack*/ {{sCL, sES, sSS, sES, sFW, sTW, sCL, sCW, sCL, sLI, sES}},
+/*rst*/ {{sCL, sCL, sCL, sSR, sCL, sCL, sCL, sCL, sLA, sLI, sCL}},
 };
 
-static struct tcp_states_t tcp_states_dos [] = {
+static struct tcp_states_t tcp_states_dos[] = {
 /*	INPUT */
 /*        sNO, sES, sSS, sSR, sFW, sTW, sCL, sCW, sLA, sLI, sSA	*/
-/*syn*/ {{sSR, sES, sES, sSR, sSR, sSR, sSR, sSR, sSR, sSR, sSA }},
-/*fin*/ {{sCL, sCW, sSS, sTW, sTW, sTW, sCL, sCW, sLA, sLI, sSA }},
-/*ack*/ {{sCL, sES, sSS, sSR, sFW, sTW, sCL, sCW, sCL, sLI, sSA }},
-/*rst*/ {{sCL, sCL, sCL, sSR, sCL, sCL, sCL, sCL, sLA, sLI, sCL }},
+/*syn*/ {{sSR, sES, sES, sSR, sSR, sSR, sSR, sSR, sSR, sSR, sSA}},
+/*fin*/ {{sCL, sCW, sSS, sTW, sTW, sTW, sCL, sCW, sLA, sLI, sSA}},
+/*ack*/ {{sCL, sES, sSS, sSR, sFW, sTW, sCL, sCW, sCL, sLI, sSA}},
+/*rst*/ {{sCL, sCL, sCL, sSR, sCL, sCL, sCL, sCL, sLA, sLI, sCL}},
 
 /*	OUTPUT */
 /*        sNO, sES, sSS, sSR, sFW, sTW, sCL, sCW, sLA, sLI, sSA	*/
-/*syn*/ {{sSS, sES, sSS, sSA, sSS, sSS, sSS, sSS, sSS, sLI, sSA }},
-/*fin*/ {{sTW, sFW, sSS, sTW, sFW, sTW, sCL, sTW, sLA, sLI, sTW }},
-/*ack*/ {{sES, sES, sSS, sES, sFW, sTW, sCL, sCW, sLA, sES, sES }},
-/*rst*/ {{sCL, sCL, sSS, sCL, sCL, sTW, sCL, sCL, sCL, sCL, sCL }},
+/*syn*/ {{sSS, sES, sSS, sSA, sSS, sSS, sSS, sSS, sSS, sLI, sSA}},
+/*fin*/ {{sTW, sFW, sSS, sTW, sFW, sTW, sCL, sTW, sLA, sLI, sTW}},
+/*ack*/ {{sES, sES, sSS, sES, sFW, sTW, sCL, sCW, sLA, sES, sES}},
+/*rst*/ {{sCL, sCL, sSS, sCL, sCL, sTW, sCL, sCL, sCL, sCL, sCL}},
 
 /*	INPUT-ONLY */
 /*        sNO, sES, sSS, sSR, sFW, sTW, sCL, sCW, sLA, sLI, sSA	*/
-/*syn*/ {{sSA, sES, sES, sSR, sSA, sSA, sSA, sSA, sSA, sSA, sSA }},
-/*fin*/ {{sCL, sFW, sSS, sTW, sFW, sTW, sCL, sCW, sLA, sLI, sTW }},
-/*ack*/ {{sCL, sES, sSS, sES, sFW, sTW, sCL, sCW, sCL, sLI, sES }},
-/*rst*/ {{sCL, sCL, sCL, sSR, sCL, sCL, sCL, sCL, sLA, sLI, sCL }},
+/*syn*/ {{sSA, sES, sES, sSR, sSA, sSA, sSA, sSA, sSA, sSA, sSA}},
+/*fin*/ {{sCL, sFW, sSS, sTW, sFW, sTW, sCL, sCW, sLA, sLI, sTW}},
+/*ack*/ {{sCL, sES, sSS, sES, sFW, sTW, sCL, sCW, sCL, sLI, sES}},
+/*rst*/ {{sCL, sCL, sCL, sSR, sCL, sCL, sCL, sCL, sLA, sLI, sCL}},
 };
 
 static struct tcp_states_t *tcp_state_table = tcp_states;
 
-
 static void tcp_timeout_change(struct ip_vs_protocol *pp, int flags)
 {
-	int on = (flags & 1);		/* secure_tcp */
+	int on = (flags & 1);	/* secure_tcp */
 
 	/*
-	** FIXME: change secure_tcp to independent sysctl var
-	** or make it per-service or per-app because it is valid
-	** for most if not for all of the applications. Something
-	** like "capabilities" (flags) for each object.
-	*/
-	tcp_state_table = (on? tcp_states_dos : tcp_states);
+	 ** FIXME: change secure_tcp to independent sysctl var
+	 ** or make it per-service or per-app because it is valid
+	 ** for most if not for all of the applications. Something
+	 ** like "capabilities" (flags) for each object.
+	 */
+	tcp_state_table = (on ? tcp_states_dos : tcp_states);
 }
 
-static int
-tcp_set_state_timeout(struct ip_vs_protocol *pp, char *sname, int to)
+static int tcp_set_state_timeout(struct ip_vs_protocol *pp, char *sname, int to)
 {
 	return ip_vs_set_state_timeout(pp->timeout_table, IP_VS_TCP_S_LAST,
 				       tcp_state_name_table, sname, to);
@@ -521,9 +1342,10 @@ set_tcp_state(struct ip_vs_protocol *pp,
 		goto tcp_state_out;
 	}
 
-	new_state = tcp_state_table[state_off+state_idx].next_state[cp->state];
+	new_state =
+	    tcp_state_table[state_off + state_idx].next_state[cp->state];
 
-  tcp_state_out:
+      tcp_state_out:
 	if (new_state != cp->state) {
 		struct ip_vs_dest *dest = cp->dest;
 
@@ -559,17 +1381,16 @@ set_tcp_state(struct ip_vs_protocol *pp,
 		}
 	}
 
+	cp->old_state = cp->state;	// old_state called when connection reused
 	cp->timeout = pp->timeout_table[cp->state = new_state];
 }
 
-
 /*
  *	Handle state transitions
  */
 static int
 tcp_state_transition(struct ip_vs_conn *cp, int direction,
-		     const struct sk_buff *skb,
-		     struct ip_vs_protocol *pp)
+		     const struct sk_buff *skb, struct ip_vs_protocol *pp)
 {
 	struct tcphdr _tcph, *th;
 
@@ -590,7 +1411,6 @@ tcp_state_transition(struct ip_vs_conn *
 	return 1;
 }
 
-
 /*
  *	Hash table for TCP application incarnations
  */
@@ -603,11 +1423,10 @@ static DEFINE_SPINLOCK(tcp_app_lock);
 
 static inline __u16 tcp_app_hashkey(__be16 port)
 {
-	return (((__force u16)port >> TCP_APP_TAB_BITS) ^ (__force u16)port)
-		& TCP_APP_TAB_MASK;
+	return (((__force u16) port >> TCP_APP_TAB_BITS) ^ (__force u16) port)
+	    & TCP_APP_TAB_MASK;
 }
 
-
 static int tcp_register_app(struct ip_vs_app *inc)
 {
 	struct ip_vs_app *i;
@@ -627,14 +1446,12 @@ static int tcp_register_app(struct ip_vs
 	list_add(&inc->p_list, &tcp_apps[hash]);
 	atomic_inc(&ip_vs_protocol_tcp.appcnt);
 
-  out:
+      out:
 	spin_unlock_bh(&tcp_app_lock);
 	return ret;
 }
 
-
-static void
-tcp_unregister_app(struct ip_vs_app *inc)
+static void tcp_unregister_app(struct ip_vs_app *inc)
 {
 	spin_lock_bh(&tcp_app_lock);
 	atomic_dec(&ip_vs_protocol_tcp.appcnt);
@@ -642,9 +1459,7 @@ tcp_unregister_app(struct ip_vs_app *inc
 	spin_unlock_bh(&tcp_app_lock);
 }
 
-
-static int
-tcp_app_conn_bind(struct ip_vs_conn *cp)
+static int tcp_app_conn_bind(struct ip_vs_conn *cp)
 {
 	int hash;
 	struct ip_vs_app *inc;
@@ -681,11 +1496,10 @@ tcp_app_conn_bind(struct ip_vs_conn *cp)
 	}
 	spin_unlock(&tcp_app_lock);
 
-  out:
+      out:
 	return result;
 }
 
-
 /*
  *	Set LISTEN timeout. (ip_vs_conn_put will setup timer)
  */
@@ -697,39 +1511,39 @@ void ip_vs_tcp_conn_listen(struct ip_vs_
 	spin_unlock(&cp->lock);
 }
 
-
 static void ip_vs_tcp_init(struct ip_vs_protocol *pp)
 {
 	IP_VS_INIT_HASH_TABLE(tcp_apps);
-	pp->timeout_table = tcp_timeouts;
+	pp->timeout_table = sysctl_ip_vs_tcp_timeouts;
 }
 
-
 static void ip_vs_tcp_exit(struct ip_vs_protocol *pp)
 {
 }
 
-
 struct ip_vs_protocol ip_vs_protocol_tcp = {
-	.name =			"TCP",
-	.protocol =		IPPROTO_TCP,
-	.num_states =		IP_VS_TCP_S_LAST,
-	.dont_defrag =		0,
-	.appcnt =		ATOMIC_INIT(0),
-	.init =			ip_vs_tcp_init,
-	.exit =			ip_vs_tcp_exit,
-	.register_app =		tcp_register_app,
-	.unregister_app =	tcp_unregister_app,
-	.conn_schedule =	tcp_conn_schedule,
-	.conn_in_get =		tcp_conn_in_get,
-	.conn_out_get =		tcp_conn_out_get,
-	.snat_handler =		tcp_snat_handler,
-	.dnat_handler =		tcp_dnat_handler,
-	.csum_check =		tcp_csum_check,
-	.state_name =		tcp_state_name,
-	.state_transition =	tcp_state_transition,
-	.app_conn_bind =	tcp_app_conn_bind,
-	.debug_packet =		ip_vs_tcpudp_debug_packet,
-	.timeout_change =	tcp_timeout_change,
-	.set_state_timeout =	tcp_set_state_timeout,
+	.name = "TCP",
+	.protocol = IPPROTO_TCP,
+	.num_states = IP_VS_TCP_S_LAST,
+	.dont_defrag = 0,
+	.appcnt = ATOMIC_INIT(0),
+	.init = ip_vs_tcp_init,
+	.exit = ip_vs_tcp_exit,
+	.register_app = tcp_register_app,
+	.unregister_app = tcp_unregister_app,
+	.conn_schedule = tcp_conn_schedule,
+	.conn_in_get = tcp_conn_in_get,
+	.conn_out_get = tcp_conn_out_get,
+	.snat_handler = tcp_snat_handler,
+	.dnat_handler = tcp_dnat_handler,
+	.fnat_in_handler = tcp_fnat_in_handler,
+	.fnat_out_handler = tcp_fnat_out_handler,
+	.csum_check = tcp_csum_check,
+	.state_name = tcp_state_name,
+	.state_transition = tcp_state_transition,
+	.app_conn_bind = tcp_app_conn_bind,
+	.debug_packet = ip_vs_tcpudp_debug_packet,
+	.timeout_change = tcp_timeout_change,
+	.set_state_timeout = tcp_set_state_timeout,
+	.conn_expire_handler = tcp_conn_expire_handler,
 };
diff -uprN linux-2.6.32-220.23.1.el6.x86_64.bak/net/netfilter/ipvs/ip_vs_proto_udp.c linux-2.6.32-220.23.1.el6.x86_64/net/netfilter/ipvs/ip_vs_proto_udp.c
--- linux-2.6.32-220.23.1.el6.x86_64.bak/net/netfilter/ipvs/ip_vs_proto_udp.c	2009-12-03 11:51:21.000000000 +0800
+++ linux-2.6.32-220.23.1.el6.x86_64/net/netfilter/ipvs/ip_vs_proto_udp.c	2013-03-15 17:22:36.000000000 +0800
@@ -27,10 +27,11 @@
 #include <net/ip.h>
 #include <net/ip6_checksum.h>
 
-static struct ip_vs_conn *
-udp_conn_in_get(int af, const struct sk_buff *skb, struct ip_vs_protocol *pp,
-		const struct ip_vs_iphdr *iph, unsigned int proto_off,
-		int inverse)
+static struct ip_vs_conn *udp_conn_in_get(int af, const struct sk_buff *skb,
+					  struct ip_vs_protocol *pp,
+					  const struct ip_vs_iphdr *iph,
+					  unsigned int proto_off, int inverse,
+					  int *res_dir)
 {
 	struct ip_vs_conn *cp;
 	__be16 _ports[2], *pptr;
@@ -40,23 +41,23 @@ udp_conn_in_get(int af, const struct sk_
 		return NULL;
 
 	if (likely(!inverse)) {
-		cp = ip_vs_conn_in_get(af, iph->protocol,
-				       &iph->saddr, pptr[0],
-				       &iph->daddr, pptr[1]);
+		cp = ip_vs_conn_get(af, iph->protocol,
+				    &iph->saddr, pptr[0],
+				    &iph->daddr, pptr[1], res_dir);
 	} else {
-		cp = ip_vs_conn_in_get(af, iph->protocol,
-				       &iph->daddr, pptr[1],
-				       &iph->saddr, pptr[0]);
+		cp = ip_vs_conn_get(af, iph->protocol,
+				    &iph->daddr, pptr[1],
+				    &iph->saddr, pptr[0], res_dir);
 	}
 
 	return cp;
 }
 
-
-static struct ip_vs_conn *
-udp_conn_out_get(int af, const struct sk_buff *skb, struct ip_vs_protocol *pp,
-		 const struct ip_vs_iphdr *iph, unsigned int proto_off,
-		 int inverse)
+static struct ip_vs_conn *udp_conn_out_get(int af, const struct sk_buff *skb,
+					   struct ip_vs_protocol *pp,
+					   const struct ip_vs_iphdr *iph,
+					   unsigned int proto_off, int inverse,
+					   int *res_dir)
 {
 	struct ip_vs_conn *cp;
 	__be16 _ports[2], *pptr;
@@ -66,19 +67,18 @@ udp_conn_out_get(int af, const struct sk
 		return NULL;
 
 	if (likely(!inverse)) {
-		cp = ip_vs_conn_out_get(af, iph->protocol,
-					&iph->saddr, pptr[0],
-					&iph->daddr, pptr[1]);
+		cp = ip_vs_conn_get(af, iph->protocol,
+				    &iph->saddr, pptr[0],
+				    &iph->daddr, pptr[1], res_dir);
 	} else {
-		cp = ip_vs_conn_out_get(af, iph->protocol,
-					&iph->daddr, pptr[1],
-					&iph->saddr, pptr[0]);
+		cp = ip_vs_conn_get(af, iph->protocol,
+				    &iph->daddr, pptr[1],
+				    &iph->saddr, pptr[0], res_dir);
 	}
 
 	return cp;
 }
 
-
 static int
 udp_conn_schedule(int af, struct sk_buff *skb, struct ip_vs_protocol *pp,
 		  int *verdict, struct ip_vs_conn **cpp)
@@ -86,6 +86,7 @@ udp_conn_schedule(int af, struct sk_buff
 	struct ip_vs_service *svc;
 	struct udphdr _udph, *uh;
 	struct ip_vs_iphdr iph;
+	
 
 	ip_vs_fill_iphdr(af, skb_network_header(skb), &iph);
 
@@ -95,8 +96,10 @@ udp_conn_schedule(int af, struct sk_buff
 		return 0;
 	}
 
+
 	svc = ip_vs_service_get(af, skb->mark, iph.protocol,
-				&iph.daddr, uh->dest);
+			&iph.daddr, uh->dest);
+	
 	if (svc) {
 		if (ip_vs_todrop()) {
 			/*
@@ -112,7 +115,7 @@ udp_conn_schedule(int af, struct sk_buff
 		 * Let the virtual server select a real server for the
 		 * incoming connection, and create a connection entry.
 		 */
-		*cpp = ip_vs_schedule(svc, skb);
+		*cpp = ip_vs_schedule(svc, skb, 0);
 		if (!*cpp) {
 			*verdict = ip_vs_leave(svc, skb, pp);
 			return 0;
@@ -122,7 +125,6 @@ udp_conn_schedule(int af, struct sk_buff
 	return 1;
 }
 
-
 static inline void
 udp_fast_csum_update(int af, struct udphdr *uhdr,
 		     const union nf_inet_addr *oldip,
@@ -132,40 +134,51 @@ udp_fast_csum_update(int af, struct udph
 #ifdef CONFIG_IP_VS_IPV6
 	if (af == AF_INET6)
 		uhdr->check =
-			csum_fold(ip_vs_check_diff16(oldip->ip6, newip->ip6,
-					 ip_vs_check_diff2(oldport, newport,
-						~csum_unfold(uhdr->check))));
+		    csum_fold(ip_vs_check_diff16(oldip->ip6, newip->ip6,
+						 ip_vs_check_diff2(oldport,
+								   newport,
+								   ~csum_unfold
+								   (uhdr->
+								    check))));
 	else
 #endif
 		uhdr->check =
-			csum_fold(ip_vs_check_diff4(oldip->ip, newip->ip,
-					 ip_vs_check_diff2(oldport, newport,
-						~csum_unfold(uhdr->check))));
+		    csum_fold(ip_vs_check_diff4(oldip->ip, newip->ip,
+						ip_vs_check_diff2(oldport,
+								  newport,
+								  ~csum_unfold
+								  (uhdr->
+								   check))));
 	if (!uhdr->check)
 		uhdr->check = CSUM_MANGLED_0;
 }
 
 static inline void
 udp_partial_csum_update(int af, struct udphdr *uhdr,
-		     const union nf_inet_addr *oldip,
-		     const union nf_inet_addr *newip,
-		     __be16 oldlen, __be16 newlen)
+			const union nf_inet_addr *oldip,
+			const union nf_inet_addr *newip,
+			__be16 oldlen, __be16 newlen)
 {
 #ifdef CONFIG_IP_VS_IPV6
 	if (af == AF_INET6)
 		uhdr->check =
-			csum_fold(ip_vs_check_diff16(oldip->ip6, newip->ip6,
-					 ip_vs_check_diff2(oldlen, newlen,
-						~csum_unfold(uhdr->check))));
+		    csum_fold(ip_vs_check_diff16(oldip->ip6, newip->ip6,
+						 ip_vs_check_diff2(oldlen,
+								   newlen,
+								   ~csum_unfold
+								   (uhdr->
+								    check))));
 	else
 #endif
-	uhdr->check =
-		csum_fold(ip_vs_check_diff4(oldip->ip, newip->ip,
-				ip_vs_check_diff2(oldlen, newlen,
-						~csum_unfold(uhdr->check))));
+		uhdr->check =
+		    csum_fold(ip_vs_check_diff4(oldip->ip, newip->ip,
+						ip_vs_check_diff2(oldlen,
+								  newlen,
+								  ~csum_unfold
+								  (uhdr->
+								   check))));
 }
 
-
 static int
 udp_snat_handler(struct sk_buff *skb,
 		 struct ip_vs_protocol *pp, struct ip_vs_conn *cp)
@@ -183,7 +196,7 @@ udp_snat_handler(struct sk_buff *skb,
 	oldlen = skb->len - udphoff;
 
 	/* csum_check requires unshared skb */
-	if (!skb_make_writable(skb, udphoff+sizeof(*udph)))
+	if (!skb_make_writable(skb, udphoff + sizeof(*udph)))
 		return 0;
 
 	if (unlikely(cp->app != NULL)) {
@@ -192,7 +205,7 @@ udp_snat_handler(struct sk_buff *skb,
 			return 0;
 
 		/*
-		 *	Call application helper if needed
+		 *      Call application helper if needed
 		 */
 		if (!ip_vs_app_pkt_out(cp, skb))
 			return 0;
@@ -200,18 +213,24 @@ udp_snat_handler(struct sk_buff *skb,
 
 	udph = (void *)skb_network_header(skb) + udphoff;
 	udph->source = cp->vport;
+	udph->dest = cp->cport;
 
 	/*
-	 *	Adjust UDP checksums
+	 *      Adjust UDP checksums
 	 */
 	if (skb->ip_summed == CHECKSUM_PARTIAL) {
 		udp_partial_csum_update(cp->af, udph, &cp->daddr, &cp->vaddr,
 					htons(oldlen),
 					htons(skb->len - udphoff));
+		udp_partial_csum_update(cp->af, udph, &cp->laddr, &cp->caddr,
+					htons(oldlen),
+					htons(skb->len - udphoff));
 	} else if (!cp->app && (udph->check != 0)) {
 		/* Only port and addr are changed, do fast csum update */
 		udp_fast_csum_update(cp->af, udph, &cp->daddr, &cp->vaddr,
 				     cp->dport, cp->vport);
+		udp_fast_csum_update(cp->af, udph, &cp->laddr, &cp->caddr,
+				     cp->lport, cp->cport);
 		if (skb->ip_summed == CHECKSUM_COMPLETE)
 			skb->ip_summed = CHECKSUM_NONE;
 	} else {
@@ -235,12 +254,11 @@ udp_snat_handler(struct sk_buff *skb,
 			udph->check = CSUM_MANGLED_0;
 		IP_VS_DBG(11, "O-pkt: %s O-csum=%d (+%zd)\n",
 			  pp->name, udph->check,
-			  (char*)&(udph->check) - (char*)udph);
+			  (char *)&(udph->check) - (char *)udph);
 	}
 	return 1;
 }
 
-
 static int
 udp_dnat_handler(struct sk_buff *skb,
 		 struct ip_vs_protocol *pp, struct ip_vs_conn *cp)
@@ -258,7 +276,7 @@ udp_dnat_handler(struct sk_buff *skb,
 	oldlen = skb->len - udphoff;
 
 	/* csum_check requires unshared skb */
-	if (!skb_make_writable(skb, udphoff+sizeof(*udph)))
+	if (!skb_make_writable(skb, udphoff + sizeof(*udph)))
 		return 0;
 
 	if (unlikely(cp->app != NULL)) {
@@ -267,27 +285,33 @@ udp_dnat_handler(struct sk_buff *skb,
 			return 0;
 
 		/*
-		 *	Attempt ip_vs_app call.
-		 *	It will fix ip_vs_conn
+		 *      Attempt ip_vs_app call.
+		 *      It will fix ip_vs_conn
 		 */
 		if (!ip_vs_app_pkt_in(cp, skb))
 			return 0;
 	}
 
 	udph = (void *)skb_network_header(skb) + udphoff;
+	udph->source = cp->lport;
 	udph->dest = cp->dport;
 
 	/*
-	 *	Adjust UDP checksums
+	 *      Adjust UDP checksums
 	 */
 	if (skb->ip_summed == CHECKSUM_PARTIAL) {
-		udp_partial_csum_update(cp->af, udph, &cp->daddr, &cp->vaddr,
+		udp_partial_csum_update(cp->af, udph, &cp->vaddr, &cp->daddr,
+					htons(oldlen),
+					htons(skb->len - udphoff));
+		udp_partial_csum_update(cp->af, udph, &cp->caddr, &cp->laddr,
 					htons(oldlen),
 					htons(skb->len - udphoff));
 	} else if (!cp->app && (udph->check != 0)) {
 		/* Only port and addr are changed, do fast csum update */
 		udp_fast_csum_update(cp->af, udph, &cp->vaddr, &cp->daddr,
 				     cp->vport, cp->dport);
+		udp_fast_csum_update(cp->af, udph, &cp->caddr, &cp->laddr,
+				     cp->cport, cp->lport);
 		if (skb->ip_summed == CHECKSUM_COMPLETE)
 			skb->ip_summed = CHECKSUM_NONE;
 	} else {
@@ -314,7 +338,6 @@ udp_dnat_handler(struct sk_buff *skb,
 	return 1;
 }
 
-
 static int
 udp_csum_check(int af, struct sk_buff *skb, struct ip_vs_protocol *pp)
 {
@@ -351,15 +374,17 @@ udp_csum_check(int af, struct sk_buff *s
 				}
 			} else
 #endif
-				if (csum_tcpudp_magic(ip_hdr(skb)->saddr,
-						      ip_hdr(skb)->daddr,
-						      skb->len - udphoff,
-						      ip_hdr(skb)->protocol,
-						      skb->csum)) {
-					IP_VS_DBG_RL_PKT(0, pp, skb, 0,
-							 "Failed checksum for");
-					return 0;
-				}
+			if (csum_tcpudp_magic(ip_hdr(skb)->saddr,
+						      ip_hdr(skb)->
+						      daddr,
+						      skb->len -
+						      udphoff,
+						      ip_hdr(skb)->
+						      protocol, skb->csum)) {
+				IP_VS_DBG_RL_PKT(0, pp, skb, 0,
+						 "Failed checksum for");
+				return 0;
+			}
 			break;
 		default:
 			/* No need to checksum. */
@@ -369,7 +394,6 @@ udp_csum_check(int af, struct sk_buff *s
 	return 1;
 }
 
-
 /*
  *	Note: the caller guarantees that only one of register_app,
  *	unregister_app or app_conn_bind is called each time.
@@ -384,11 +408,10 @@ static DEFINE_SPINLOCK(udp_app_lock);
 
 static inline __u16 udp_app_hashkey(__be16 port)
 {
-	return (((__force u16)port >> UDP_APP_TAB_BITS) ^ (__force u16)port)
-		& UDP_APP_TAB_MASK;
+	return (((__force u16) port >> UDP_APP_TAB_BITS) ^ (__force u16) port)
+	    & UDP_APP_TAB_MASK;
 }
 
-
 static int udp_register_app(struct ip_vs_app *inc)
 {
 	struct ip_vs_app *i;
@@ -398,7 +421,6 @@ static int udp_register_app(struct ip_vs
 
 	hash = udp_app_hashkey(port);
 
-
 	spin_lock_bh(&udp_app_lock);
 	list_for_each_entry(i, &udp_apps[hash], p_list) {
 		if (i->port == port) {
@@ -409,14 +431,12 @@ static int udp_register_app(struct ip_vs
 	list_add(&inc->p_list, &udp_apps[hash]);
 	atomic_inc(&ip_vs_protocol_udp.appcnt);
 
-  out:
+      out:
 	spin_unlock_bh(&udp_app_lock);
 	return ret;
 }
 
-
-static void
-udp_unregister_app(struct ip_vs_app *inc)
+static void udp_unregister_app(struct ip_vs_app *inc)
 {
 	spin_lock_bh(&udp_app_lock);
 	atomic_dec(&ip_vs_protocol_udp.appcnt);
@@ -424,7 +444,6 @@ udp_unregister_app(struct ip_vs_app *inc
 	spin_unlock_bh(&udp_app_lock);
 }
 
-
 static int udp_app_conn_bind(struct ip_vs_conn *cp)
 {
 	int hash;
@@ -462,30 +481,27 @@ static int udp_app_conn_bind(struct ip_v
 	}
 	spin_unlock(&udp_app_lock);
 
-  out:
+      out:
 	return result;
 }
 
-
-static int udp_timeouts[IP_VS_UDP_S_LAST+1] = {
-	[IP_VS_UDP_S_NORMAL]		=	5*60*HZ,
-	[IP_VS_UDP_S_LAST]		=	2*HZ,
+static int udp_timeouts[IP_VS_UDP_S_LAST + 1] = {
+	[IP_VS_UDP_S_NORMAL] = 5 * 60 * HZ,
+	[IP_VS_UDP_S_LAST] = 2 * HZ,
 };
 
-static const char *const udp_state_name_table[IP_VS_UDP_S_LAST+1] = {
-	[IP_VS_UDP_S_NORMAL]		=	"UDP",
-	[IP_VS_UDP_S_LAST]		=	"BUG!",
+static const char *const udp_state_name_table[IP_VS_UDP_S_LAST + 1] = {
+	[IP_VS_UDP_S_NORMAL] = "UDP",
+	[IP_VS_UDP_S_LAST] = "BUG!",
 };
 
-
-static int
-udp_set_state_timeout(struct ip_vs_protocol *pp, char *sname, int to)
+static int udp_set_state_timeout(struct ip_vs_protocol *pp, char *sname, int to)
 {
 	return ip_vs_set_state_timeout(pp->timeout_table, IP_VS_UDP_S_LAST,
 				       udp_state_name_table, sname, to);
 }
 
-static const char * udp_state_name(int state)
+static const char *udp_state_name(int state)
 {
 	if (state >= IP_VS_UDP_S_LAST)
 		return "ERR!";
@@ -494,8 +510,7 @@ static const char * udp_state_name(int s
 
 static int
 udp_state_transition(struct ip_vs_conn *cp, int direction,
-		     const struct sk_buff *skb,
-		     struct ip_vs_protocol *pp)
+		     const struct sk_buff *skb, struct ip_vs_protocol *pp)
 {
 	cp->timeout = pp->timeout_table[IP_VS_UDP_S_NORMAL];
 	return 1;
@@ -511,26 +526,25 @@ static void udp_exit(struct ip_vs_protoc
 {
 }
 
-
 struct ip_vs_protocol ip_vs_protocol_udp = {
-	.name =			"UDP",
-	.protocol =		IPPROTO_UDP,
-	.num_states =		IP_VS_UDP_S_LAST,
-	.dont_defrag =		0,
-	.init =			udp_init,
-	.exit =			udp_exit,
-	.conn_schedule =	udp_conn_schedule,
-	.conn_in_get =		udp_conn_in_get,
-	.conn_out_get =		udp_conn_out_get,
-	.snat_handler =		udp_snat_handler,
-	.dnat_handler =		udp_dnat_handler,
-	.csum_check =		udp_csum_check,
-	.state_transition =	udp_state_transition,
-	.state_name =		udp_state_name,
-	.register_app =		udp_register_app,
-	.unregister_app =	udp_unregister_app,
-	.app_conn_bind =	udp_app_conn_bind,
-	.debug_packet =		ip_vs_tcpudp_debug_packet,
-	.timeout_change =	NULL,
-	.set_state_timeout =	udp_set_state_timeout,
+	.name = "UDP",
+	.protocol = IPPROTO_UDP,
+	.num_states = IP_VS_UDP_S_LAST,
+	.dont_defrag = 0,
+	.init = udp_init,
+	.exit = udp_exit,
+	.conn_schedule = udp_conn_schedule,
+	.conn_in_get = udp_conn_in_get,
+	.conn_out_get = udp_conn_out_get,
+	.snat_handler = udp_snat_handler,
+	.dnat_handler = udp_dnat_handler,
+	.csum_check = udp_csum_check,
+	.state_transition = udp_state_transition,
+	.state_name = udp_state_name,
+	.register_app = udp_register_app,
+	.unregister_app = udp_unregister_app,
+	.app_conn_bind = udp_app_conn_bind,
+	.debug_packet = ip_vs_tcpudp_debug_packet,
+	.timeout_change = NULL,
+	.set_state_timeout = udp_set_state_timeout,
 };
diff -uprN linux-2.6.32-220.23.1.el6.x86_64.bak/net/netfilter/ipvs/ip_vs_rr.c linux-2.6.32-220.23.1.el6.x86_64/net/netfilter/ipvs/ip_vs_rr.c
--- linux-2.6.32-220.23.1.el6.x86_64.bak/net/netfilter/ipvs/ip_vs_rr.c	2009-12-03 11:51:21.000000000 +0800
+++ linux-2.6.32-220.23.1.el6.x86_64/net/netfilter/ipvs/ip_vs_rr.c	2012-10-11 15:01:03.000000000 +0800
@@ -27,26 +27,23 @@
 
 #include <net/ip_vs.h>
 
-
 static int ip_vs_rr_init_svc(struct ip_vs_service *svc)
 {
 	svc->sched_data = &svc->destinations;
 	return 0;
 }
 
-
 static int ip_vs_rr_update_svc(struct ip_vs_service *svc)
 {
 	svc->sched_data = &svc->destinations;
 	return 0;
 }
 
-
 /*
  * Round-Robin Scheduling
  */
-static struct ip_vs_dest *
-ip_vs_rr_schedule(struct ip_vs_service *svc, const struct sk_buff *skb)
+static struct ip_vs_dest *ip_vs_rr_schedule(struct ip_vs_service *svc,
+					    const struct sk_buff *skb)
 {
 	struct list_head *p, *q;
 	struct ip_vs_dest *dest;
@@ -75,7 +72,7 @@ ip_vs_rr_schedule(struct ip_vs_service *
 	IP_VS_ERR_RL("RR: no destination available\n");
 	return NULL;
 
-  out:
+      out:
 	svc->sched_data = q;
 	write_unlock(&svc->sched_lock);
 	IP_VS_DBG_BUF(6, "RR: server %s:%u "
@@ -87,15 +84,14 @@ ip_vs_rr_schedule(struct ip_vs_service *
 	return dest;
 }
 
-
 static struct ip_vs_scheduler ip_vs_rr_scheduler = {
-	.name =			"rr",			/* name */
-	.refcnt =		ATOMIC_INIT(0),
-	.module =		THIS_MODULE,
-	.n_list =		LIST_HEAD_INIT(ip_vs_rr_scheduler.n_list),
-	.init_service =		ip_vs_rr_init_svc,
-	.update_service =	ip_vs_rr_update_svc,
-	.schedule =		ip_vs_rr_schedule,
+	.name = "rr",		/* name */
+	.refcnt = ATOMIC_INIT(0),
+	.module = THIS_MODULE,
+	.n_list = LIST_HEAD_INIT(ip_vs_rr_scheduler.n_list),
+	.init_service = ip_vs_rr_init_svc,
+	.update_service = ip_vs_rr_update_svc,
+	.schedule = ip_vs_rr_schedule,
 };
 
 static int __init ip_vs_rr_init(void)
diff -uprN linux-2.6.32-220.23.1.el6.x86_64.bak/net/netfilter/ipvs/ip_vs_sched.c linux-2.6.32-220.23.1.el6.x86_64/net/netfilter/ipvs/ip_vs_sched.c
--- linux-2.6.32-220.23.1.el6.x86_64.bak/net/netfilter/ipvs/ip_vs_sched.c	2009-12-03 11:51:21.000000000 +0800
+++ linux-2.6.32-220.23.1.el6.x86_64/net/netfilter/ipvs/ip_vs_sched.c	2012-10-11 15:01:03.000000000 +0800
@@ -37,7 +37,6 @@ static LIST_HEAD(ip_vs_schedulers);
 /* lock for service table */
 static DEFINE_RWLOCK(__ip_vs_sched_lock);
 
-
 /*
  *  Bind a service with a scheduler
  */
@@ -68,7 +67,6 @@ int ip_vs_bind_scheduler(struct ip_vs_se
 	return 0;
 }
 
-
 /*
  *  Unbind a service with its scheduler
  */
@@ -98,7 +96,6 @@ int ip_vs_unbind_scheduler(struct ip_vs_
 	return 0;
 }
 
-
 /*
  *  Get scheduler in the scheduler list by name
  */
@@ -120,7 +117,7 @@ static struct ip_vs_scheduler *ip_vs_sch
 			 */
 			continue;
 		}
-		if (strcmp(sched_name, sched->name)==0) {
+		if (strcmp(sched_name, sched->name) == 0) {
 			/* HIT */
 			read_unlock_bh(&__ip_vs_sched_lock);
 			return sched;
@@ -133,7 +130,6 @@ static struct ip_vs_scheduler *ip_vs_sch
 	return NULL;
 }
 
-
 /*
  *  Lookup scheduler and try to load it if it doesn't exist
  */
@@ -163,7 +159,6 @@ void ip_vs_scheduler_put(struct ip_vs_sc
 		module_put(scheduler->module);
 }
 
-
 /*
  *  Register a scheduler in the scheduler list
  */
@@ -208,7 +203,7 @@ int register_ip_vs_scheduler(struct ip_v
 		}
 	}
 	/*
-	 *	Add it into the d-linked scheduler list
+	 *      Add it into the d-linked scheduler list
 	 */
 	list_add(&scheduler->n_list, &ip_vs_schedulers);
 	write_unlock_bh(&__ip_vs_sched_lock);
@@ -218,7 +213,6 @@ int register_ip_vs_scheduler(struct ip_v
 	return 0;
 }
 
-
 /*
  *  Unregister a scheduler from the scheduler list
  */
@@ -238,7 +232,7 @@ int unregister_ip_vs_scheduler(struct ip
 	}
 
 	/*
-	 *	Remove it from the d-linked scheduler list
+	 *      Remove it from the d-linked scheduler list
 	 */
 	list_del(&scheduler->n_list);
 	write_unlock_bh(&__ip_vs_sched_lock);
diff -uprN linux-2.6.32-220.23.1.el6.x86_64.bak/net/netfilter/ipvs/ip_vs_sed.c linux-2.6.32-220.23.1.el6.x86_64/net/netfilter/ipvs/ip_vs_sed.c
--- linux-2.6.32-220.23.1.el6.x86_64.bak/net/netfilter/ipvs/ip_vs_sed.c	2009-12-03 11:51:21.000000000 +0800
+++ linux-2.6.32-220.23.1.el6.x86_64/net/netfilter/ipvs/ip_vs_sed.c	2012-10-11 15:01:03.000000000 +0800
@@ -43,9 +43,7 @@
 
 #include <net/ip_vs.h>
 
-
-static inline unsigned int
-ip_vs_sed_dest_overhead(struct ip_vs_dest *dest)
+static inline unsigned int ip_vs_sed_dest_overhead(struct ip_vs_dest *dest)
 {
 	/*
 	 * We only use the active connection number in the cost
@@ -54,12 +52,11 @@ ip_vs_sed_dest_overhead(struct ip_vs_des
 	return atomic_read(&dest->activeconns) + 1;
 }
 
-
 /*
  *	Weighted Least Connection scheduling
  */
-static struct ip_vs_dest *
-ip_vs_sed_schedule(struct ip_vs_service *svc, const struct sk_buff *skb)
+static struct ip_vs_dest *ip_vs_sed_schedule(struct ip_vs_service *svc,
+					     const struct sk_buff *skb)
 {
 	struct ip_vs_dest *dest, *least;
 	unsigned int loh, doh;
@@ -68,11 +65,11 @@ ip_vs_sed_schedule(struct ip_vs_service 
 
 	/*
 	 * We calculate the load of each dest server as follows:
-	 *	(server expected overhead) / dest->weight
+	 *      (server expected overhead) / dest->weight
 	 *
 	 * Remember -- no floats in kernel mode!!!
 	 * The comparison of h1*w2 > h2*w1 is equivalent to that of
-	 *		  h1/w1 > h2/w2
+	 *                h1/w1 > h2/w2
 	 * if every weight is larger than zero.
 	 *
 	 * The server with weight=0 is quiesced and will not receive any
@@ -93,7 +90,7 @@ ip_vs_sed_schedule(struct ip_vs_service 
 	/*
 	 *    Find the destination with the least load.
 	 */
-  nextstage:
+      nextstage:
 	list_for_each_entry_continue(dest, &svc->destinations, n_list) {
 		if (dest->flags & IP_VS_DEST_F_OVERLOAD)
 			continue;
@@ -115,17 +112,14 @@ ip_vs_sed_schedule(struct ip_vs_service 
 	return least;
 }
 
-
-static struct ip_vs_scheduler ip_vs_sed_scheduler =
-{
-	.name =			"sed",
-	.refcnt =		ATOMIC_INIT(0),
-	.module =		THIS_MODULE,
-	.n_list =		LIST_HEAD_INIT(ip_vs_sed_scheduler.n_list),
-	.schedule =		ip_vs_sed_schedule,
+static struct ip_vs_scheduler ip_vs_sed_scheduler = {
+	.name = "sed",
+	.refcnt = ATOMIC_INIT(0),
+	.module = THIS_MODULE,
+	.n_list = LIST_HEAD_INIT(ip_vs_sed_scheduler.n_list),
+	.schedule = ip_vs_sed_schedule,
 };
 
-
 static int __init ip_vs_sed_init(void)
 {
 	return register_ip_vs_scheduler(&ip_vs_sed_scheduler);
diff -uprN linux-2.6.32-220.23.1.el6.x86_64.bak/net/netfilter/ipvs/ip_vs_sh.c linux-2.6.32-220.23.1.el6.x86_64/net/netfilter/ipvs/ip_vs_sh.c
--- linux-2.6.32-220.23.1.el6.x86_64.bak/net/netfilter/ipvs/ip_vs_sh.c	2009-12-03 11:51:21.000000000 +0800
+++ linux-2.6.32-220.23.1.el6.x86_64/net/netfilter/ipvs/ip_vs_sh.c	2012-10-11 15:01:03.000000000 +0800
@@ -42,12 +42,11 @@
 
 #include <net/ip_vs.h>
 
-
 /*
  *      IPVS SH bucket
  */
 struct ip_vs_sh_bucket {
-	struct ip_vs_dest       *dest;          /* real server (cache) */
+	struct ip_vs_dest *dest;	/* real server (cache) */
 };
 
 /*
@@ -60,7 +59,6 @@ struct ip_vs_sh_bucket {
 #define IP_VS_SH_TAB_SIZE               (1 << IP_VS_SH_TAB_BITS)
 #define IP_VS_SH_TAB_MASK               (IP_VS_SH_TAB_SIZE - 1)
 
-
 /*
  *	Returns hash value for IPVS SH entry
  */
@@ -70,24 +68,22 @@ static inline unsigned ip_vs_sh_hashkey(
 
 #ifdef CONFIG_IP_VS_IPV6
 	if (af == AF_INET6)
-		addr_fold = addr->ip6[0]^addr->ip6[1]^
-			    addr->ip6[2]^addr->ip6[3];
+		addr_fold = addr->ip6[0] ^ addr->ip6[1] ^
+		    addr->ip6[2] ^ addr->ip6[3];
 #endif
-	return (ntohl(addr_fold)*2654435761UL) & IP_VS_SH_TAB_MASK;
+	return (ntohl(addr_fold) * 2654435761UL) & IP_VS_SH_TAB_MASK;
 }
 
-
 /*
  *      Get ip_vs_dest associated with supplied parameters.
  */
-static inline struct ip_vs_dest *
-ip_vs_sh_get(int af, struct ip_vs_sh_bucket *tbl,
-	     const union nf_inet_addr *addr)
+static inline struct ip_vs_dest *ip_vs_sh_get(int af,
+					      struct ip_vs_sh_bucket *tbl,
+					      const union nf_inet_addr *addr)
 {
 	return (tbl[ip_vs_sh_hashkey(af, addr)]).dest;
 }
 
-
 /*
  *      Assign all the hash buckets of the specified table with the service.
  */
@@ -101,7 +97,7 @@ ip_vs_sh_assign(struct ip_vs_sh_bucket *
 
 	b = tbl;
 	p = &svc->destinations;
-	for (i=0; i<IP_VS_SH_TAB_SIZE; i++) {
+	for (i = 0; i < IP_VS_SH_TAB_SIZE; i++) {
 		if (list_empty(p)) {
 			b->dest = NULL;
 		} else {
@@ -119,7 +115,6 @@ ip_vs_sh_assign(struct ip_vs_sh_bucket *
 	return 0;
 }
 
-
 /*
  *      Flush all the hash buckets of the specified table.
  */
@@ -129,7 +124,7 @@ static void ip_vs_sh_flush(struct ip_vs_
 	struct ip_vs_sh_bucket *b;
 
 	b = tbl;
-	for (i=0; i<IP_VS_SH_TAB_SIZE; i++) {
+	for (i = 0; i < IP_VS_SH_TAB_SIZE; i++) {
 		if (b->dest) {
 			atomic_dec(&b->dest->refcnt);
 			b->dest = NULL;
@@ -138,13 +133,12 @@ static void ip_vs_sh_flush(struct ip_vs_
 	}
 }
 
-
 static int ip_vs_sh_init_svc(struct ip_vs_service *svc)
 {
 	struct ip_vs_sh_bucket *tbl;
 
 	/* allocate the SH table for this service */
-	tbl = kmalloc(sizeof(struct ip_vs_sh_bucket)*IP_VS_SH_TAB_SIZE,
+	tbl = kmalloc(sizeof(struct ip_vs_sh_bucket) * IP_VS_SH_TAB_SIZE,
 		      GFP_ATOMIC);
 	if (tbl == NULL) {
 		pr_err("%s(): no memory\n", __func__);
@@ -153,7 +147,7 @@ static int ip_vs_sh_init_svc(struct ip_v
 	svc->sched_data = tbl;
 	IP_VS_DBG(6, "SH hash table (memory=%Zdbytes) allocated for "
 		  "current service\n",
-		  sizeof(struct ip_vs_sh_bucket)*IP_VS_SH_TAB_SIZE);
+		  sizeof(struct ip_vs_sh_bucket) * IP_VS_SH_TAB_SIZE);
 
 	/* assign the hash buckets with the updated service */
 	ip_vs_sh_assign(tbl, svc);
@@ -161,7 +155,6 @@ static int ip_vs_sh_init_svc(struct ip_v
 	return 0;
 }
 
-
 static int ip_vs_sh_done_svc(struct ip_vs_service *svc)
 {
 	struct ip_vs_sh_bucket *tbl = svc->sched_data;
@@ -172,12 +165,11 @@ static int ip_vs_sh_done_svc(struct ip_v
 	/* release the table itself */
 	kfree(svc->sched_data);
 	IP_VS_DBG(6, "SH hash table (memory=%Zdbytes) released\n",
-		  sizeof(struct ip_vs_sh_bucket)*IP_VS_SH_TAB_SIZE);
+		  sizeof(struct ip_vs_sh_bucket) * IP_VS_SH_TAB_SIZE);
 
 	return 0;
 }
 
-
 static int ip_vs_sh_update_svc(struct ip_vs_service *svc)
 {
 	struct ip_vs_sh_bucket *tbl = svc->sched_data;
@@ -191,7 +183,6 @@ static int ip_vs_sh_update_svc(struct ip
 	return 0;
 }
 
-
 /*
  *      If the dest flags is set with IP_VS_DEST_F_OVERLOAD,
  *      consider that the server is overloaded here.
@@ -201,12 +192,11 @@ static inline int is_overloaded(struct i
 	return dest->flags & IP_VS_DEST_F_OVERLOAD;
 }
 
-
 /*
  *      Source Hashing scheduling
  */
-static struct ip_vs_dest *
-ip_vs_sh_schedule(struct ip_vs_service *svc, const struct sk_buff *skb)
+static struct ip_vs_dest *ip_vs_sh_schedule(struct ip_vs_service *svc,
+					    const struct sk_buff *skb)
 {
 	struct ip_vs_dest *dest;
 	struct ip_vs_sh_bucket *tbl;
@@ -218,51 +208,43 @@ ip_vs_sh_schedule(struct ip_vs_service *
 
 	tbl = (struct ip_vs_sh_bucket *)svc->sched_data;
 	dest = ip_vs_sh_get(svc->af, tbl, &iph.saddr);
-	if (!dest
-	    || !(dest->flags & IP_VS_DEST_F_AVAILABLE)
-	    || atomic_read(&dest->weight) <= 0
-	    || is_overloaded(dest)) {
+	if (!dest || !(dest->flags & IP_VS_DEST_F_AVAILABLE)
+	    || atomic_read(&dest->weight) <= 0 || is_overloaded(dest)) {
 		IP_VS_ERR_RL("SH: no destination available\n");
 		return NULL;
 	}
 
 	IP_VS_DBG_BUF(6, "SH: source IP address %s --> server %s:%d\n",
 		      IP_VS_DBG_ADDR(svc->af, &iph.saddr),
-		      IP_VS_DBG_ADDR(svc->af, &dest->addr),
-		      ntohs(dest->port));
+		      IP_VS_DBG_ADDR(svc->af, &dest->addr), ntohs(dest->port));
 
 	return dest;
 }
 
-
 /*
  *      IPVS SH Scheduler structure
  */
-static struct ip_vs_scheduler ip_vs_sh_scheduler =
-{
-	.name =			"sh",
-	.refcnt =		ATOMIC_INIT(0),
-	.module =		THIS_MODULE,
-	.n_list	 =		LIST_HEAD_INIT(ip_vs_sh_scheduler.n_list),
-	.init_service =		ip_vs_sh_init_svc,
-	.done_service =		ip_vs_sh_done_svc,
-	.update_service =	ip_vs_sh_update_svc,
-	.schedule =		ip_vs_sh_schedule,
+static struct ip_vs_scheduler ip_vs_sh_scheduler = {
+	.name = "sh",
+	.refcnt = ATOMIC_INIT(0),
+	.module = THIS_MODULE,
+	.n_list = LIST_HEAD_INIT(ip_vs_sh_scheduler.n_list),
+	.init_service = ip_vs_sh_init_svc,
+	.done_service = ip_vs_sh_done_svc,
+	.update_service = ip_vs_sh_update_svc,
+	.schedule = ip_vs_sh_schedule,
 };
 
-
 static int __init ip_vs_sh_init(void)
 {
 	return register_ip_vs_scheduler(&ip_vs_sh_scheduler);
 }
 
-
 static void __exit ip_vs_sh_cleanup(void)
 {
 	unregister_ip_vs_scheduler(&ip_vs_sh_scheduler);
 }
 
-
 module_init(ip_vs_sh_init);
 module_exit(ip_vs_sh_cleanup);
 MODULE_LICENSE("GPL");
diff -uprN linux-2.6.32-220.23.1.el6.x86_64.bak/net/netfilter/ipvs/ip_vs_sync.c linux-2.6.32-220.23.1.el6.x86_64/net/netfilter/ipvs/ip_vs_sync.c
--- linux-2.6.32-220.23.1.el6.x86_64.bak/net/netfilter/ipvs/ip_vs_sync.c	2009-12-03 11:51:21.000000000 +0800
+++ linux-2.6.32-220.23.1.el6.x86_64/net/netfilter/ipvs/ip_vs_sync.c	2012-10-11 15:01:03.000000000 +0800
@@ -28,7 +28,7 @@
 #include <linux/delay.h>
 #include <linux/skbuff.h>
 #include <linux/in.h>
-#include <linux/igmp.h>                 /* for ip_mc_join_group */
+#include <linux/igmp.h>		/* for ip_mc_join_group */
 #include <linux/udp.h>
 #include <linux/err.h>
 #include <linux/kthread.h>
@@ -40,35 +40,34 @@
 
 #include <net/ip_vs.h>
 
-#define IP_VS_SYNC_GROUP 0xe0000051    /* multicast addr - 224.0.0.81 */
-#define IP_VS_SYNC_PORT  8848          /* multicast port */
-
+#define IP_VS_SYNC_GROUP 0xe0000051	/* multicast addr - 224.0.0.81 */
+#define IP_VS_SYNC_PORT  8848	/* multicast port */
 
 /*
  *	IPVS sync connection entry
  */
 struct ip_vs_sync_conn {
-	__u8			reserved;
+	__u8 reserved;
 
 	/* Protocol, addresses and port numbers */
-	__u8			protocol;       /* Which protocol (TCP/UDP) */
-	__be16			cport;
-	__be16                  vport;
-	__be16                  dport;
-	__be32                  caddr;          /* client address */
-	__be32                  vaddr;          /* virtual address */
-	__be32                  daddr;          /* destination address */
+	__u8 protocol;		/* Which protocol (TCP/UDP) */
+	__be16 cport;
+	__be16 vport;
+	__be16 dport;
+	__be32 caddr;		/* client address */
+	__be32 vaddr;		/* virtual address */
+	__be32 daddr;		/* destination address */
 
 	/* Flags and state transition */
-	__be16                  flags;          /* status flags */
-	__be16                  state;          /* state info */
+	__be16 flags;		/* status flags */
+	__be16 state;		/* state info */
 
 	/* The sequence options start here */
 };
 
 struct ip_vs_sync_conn_options {
-	struct ip_vs_seq        in_seq;         /* incoming seq. struct */
-	struct ip_vs_seq        out_seq;        /* outgoing seq. struct */
+	struct ip_vs_seq in_seq;	/* incoming seq. struct */
+	struct ip_vs_seq out_seq;	/* outgoing seq. struct */
 };
 
 struct ip_vs_sync_thread_data {
@@ -80,7 +79,6 @@ struct ip_vs_sync_thread_data {
 #define FULL_CONN_SIZE  \
 (sizeof(struct ip_vs_sync_conn) + sizeof(struct ip_vs_sync_conn_options))
 
-
 /*
   The master mulitcasts messages to the backup load balancers in the
   following format.
@@ -103,12 +101,12 @@ struct ip_vs_sync_thread_data {
 */
 
 #define SYNC_MESG_HEADER_LEN	4
-#define MAX_CONNS_PER_SYNCBUFF	255 /* nr_conns in ip_vs_sync_mesg is 8 bit */
+#define MAX_CONNS_PER_SYNCBUFF	255	/* nr_conns in ip_vs_sync_mesg is 8 bit */
 
 struct ip_vs_sync_mesg {
-	__u8                    nr_conns;
-	__u8                    syncid;
-	__u16                   size;
+	__u8 nr_conns;
+	__u8 syncid;
+	__u16 size;
 
 	/* ip_vs_sync_conn entries start here */
 };
@@ -118,22 +116,21 @@ static int sync_send_mesg_maxlen;
 static int sync_recv_mesg_maxlen;
 
 struct ip_vs_sync_buff {
-	struct list_head        list;
-	unsigned long           firstuse;
+	struct list_head list;
+	unsigned long firstuse;
 
 	/* pointers for the message data */
-	struct ip_vs_sync_mesg  *mesg;
-	unsigned char           *head;
-	unsigned char           *end;
+	struct ip_vs_sync_mesg *mesg;
+	unsigned char *head;
+	unsigned char *end;
 };
 
-
 /* the sync_buff list head and the lock */
 static LIST_HEAD(ip_vs_sync_queue);
 static DEFINE_SPINLOCK(ip_vs_sync_lock);
 
 /* current sync_buff for accepting new conn entries */
-static struct ip_vs_sync_buff   *curr_sb = NULL;
+static struct ip_vs_sync_buff *curr_sb = NULL;
 static DEFINE_SPINLOCK(curr_sb_lock);
 
 /* ipvs sync daemon state */
@@ -151,12 +148,11 @@ static struct task_struct *sync_backup_t
 
 /* multicast addr */
 static struct sockaddr_in mcast_addr = {
-	.sin_family		= AF_INET,
-	.sin_port		= cpu_to_be16(IP_VS_SYNC_PORT),
-	.sin_addr.s_addr	= cpu_to_be32(IP_VS_SYNC_GROUP),
+	.sin_family = AF_INET,
+	.sin_port = cpu_to_be16(IP_VS_SYNC_PORT),
+	.sin_addr.s_addr = cpu_to_be32(IP_VS_SYNC_GROUP),
 };
 
-
 static inline struct ip_vs_sync_buff *sb_dequeue(void)
 {
 	struct ip_vs_sync_buff *sb;
@@ -166,8 +162,7 @@ static inline struct ip_vs_sync_buff *sb
 		sb = NULL;
 	} else {
 		sb = list_entry(ip_vs_sync_queue.next,
-				struct ip_vs_sync_buff,
-				list);
+				struct ip_vs_sync_buff, list);
 		list_del(&sb->list);
 	}
 	spin_unlock_bh(&ip_vs_sync_lock);
@@ -175,14 +170,14 @@ static inline struct ip_vs_sync_buff *sb
 	return sb;
 }
 
-static inline struct ip_vs_sync_buff * ip_vs_sync_buff_create(void)
+static inline struct ip_vs_sync_buff *ip_vs_sync_buff_create(void)
 {
 	struct ip_vs_sync_buff *sb;
 
-	if (!(sb=kmalloc(sizeof(struct ip_vs_sync_buff), GFP_ATOMIC)))
+	if (!(sb = kmalloc(sizeof(struct ip_vs_sync_buff), GFP_ATOMIC)))
 		return NULL;
 
-	if (!(sb->mesg=kmalloc(sync_send_mesg_maxlen, GFP_ATOMIC))) {
+	if (!(sb->mesg = kmalloc(sync_send_mesg_maxlen, GFP_ATOMIC))) {
 		kfree(sb);
 		return NULL;
 	}
@@ -215,8 +210,7 @@ static inline void sb_queue_tail(struct 
  *	Get the current sync buffer if it has been created for more
  *	than the specified time or the specified time is zero.
  */
-static inline struct ip_vs_sync_buff *
-get_curr_sync_buff(unsigned long time)
+static inline struct ip_vs_sync_buff *get_curr_sync_buff(unsigned long time)
 {
 	struct ip_vs_sync_buff *sb;
 
@@ -231,7 +225,6 @@ get_curr_sync_buff(unsigned long time)
 	return sb;
 }
 
-
 /*
  *      Add an ip_vs_conn information into the current sync_buff.
  *      Called by ip_vs_in.
@@ -244,7 +237,7 @@ void ip_vs_sync_conn(struct ip_vs_conn *
 
 	spin_lock(&curr_sb_lock);
 	if (!curr_sb) {
-		if (!(curr_sb=ip_vs_sync_buff_create())) {
+		if (!(curr_sb = ip_vs_sync_buff_create())) {
 			spin_unlock(&curr_sb_lock);
 			pr_err("ip_vs_sync_buff_create failed.\n");
 			return;
@@ -252,7 +245,7 @@ void ip_vs_sync_conn(struct ip_vs_conn *
 	}
 
 	len = (cp->flags & IP_VS_CONN_F_SEQ_MASK) ? FULL_CONN_SIZE :
-		SIMPLE_CONN_SIZE;
+	    SIMPLE_CONN_SIZE;
 	m = curr_sb->mesg;
 	s = (struct ip_vs_sync_conn *)curr_sb->head;
 
@@ -268,7 +261,7 @@ void ip_vs_sync_conn(struct ip_vs_conn *
 	s->state = htons(cp->state);
 	if (cp->flags & IP_VS_CONN_F_SEQ_MASK) {
 		struct ip_vs_sync_conn_options *opt =
-			(struct ip_vs_sync_conn_options *)&s[1];
+		    (struct ip_vs_sync_conn_options *)&s[1];
 		memcpy(opt, &cp->in_seq, sizeof(*opt));
 	}
 
@@ -277,7 +270,7 @@ void ip_vs_sync_conn(struct ip_vs_conn *
 	curr_sb->head += len;
 
 	/* check if there is a space for next one */
-	if (curr_sb->head+FULL_CONN_SIZE > curr_sb->end) {
+	if (curr_sb->head + FULL_CONN_SIZE > curr_sb->end) {
 		sb_queue_tail(curr_sb);
 		curr_sb = NULL;
 	}
@@ -288,7 +281,6 @@ void ip_vs_sync_conn(struct ip_vs_conn *
 		ip_vs_sync_conn(cp->control);
 }
 
-
 /*
  *      Process received multicast message and create the corresponding
  *      ip_vs_conn entries.
@@ -303,6 +295,7 @@ static void ip_vs_process_message(const 
 	struct ip_vs_dest *dest;
 	char *p;
 	int i;
+	int res_dir;
 
 	if (buflen < sizeof(struct ip_vs_sync_mesg)) {
 		IP_VS_ERR_RL("sync message header too short\n");
@@ -325,21 +318,22 @@ static void ip_vs_process_message(const 
 	}
 
 	p = (char *)buffer + sizeof(struct ip_vs_sync_mesg);
-	for (i=0; i<m->nr_conns; i++) {
+	for (i = 0; i < m->nr_conns; i++) {
 		unsigned flags, state;
 
-		if (p + SIMPLE_CONN_SIZE > buffer+buflen) {
+		if (p + SIMPLE_CONN_SIZE > buffer + buflen) {
 			IP_VS_ERR_RL("bogus conn in sync message\n");
 			return;
 		}
-		s = (struct ip_vs_sync_conn *) p;
+		s = (struct ip_vs_sync_conn *)p;
 		flags = ntohs(s->flags) | IP_VS_CONN_F_SYNC;
 		flags &= ~IP_VS_CONN_F_HASHED;
 		if (flags & IP_VS_CONN_F_SEQ_MASK) {
 			opt = (struct ip_vs_sync_conn_options *)&s[1];
 			p += FULL_CONN_SIZE;
-			if (p > buffer+buflen) {
-				IP_VS_ERR_RL("bogus conn options in sync message\n");
+			if (p > buffer + buflen) {
+				IP_VS_ERR_RL
+				    ("bogus conn options in sync message\n");
 				return;
 			}
 		} else {
@@ -351,31 +345,34 @@ static void ip_vs_process_message(const 
 		if (!(flags & IP_VS_CONN_F_TEMPLATE)) {
 			pp = ip_vs_proto_get(s->protocol);
 			if (!pp) {
-				IP_VS_ERR_RL("Unsupported protocol %u in sync msg\n",
-					s->protocol);
+				IP_VS_ERR_RL
+				    ("Unsupported protocol %u in sync msg\n",
+				     s->protocol);
 				continue;
 			}
 			if (state >= pp->num_states) {
-				IP_VS_DBG(2, "Invalid %s state %u in sync msg\n",
-					pp->name, state);
+				IP_VS_DBG(2,
+					  "Invalid %s state %u in sync msg\n",
+					  pp->name, state);
 				continue;
 			}
 		} else {
 			/* protocol in templates is not used for state/timeout */
 			pp = NULL;
 			if (state > 0) {
-				IP_VS_DBG(2, "Invalid template state %u in sync msg\n",
-					state);
+				IP_VS_DBG(2,
+					  "Invalid template state %u in sync msg\n",
+					  state);
 				state = 0;
 			}
 		}
 
 		if (!(flags & IP_VS_CONN_F_TEMPLATE))
-			cp = ip_vs_conn_in_get(AF_INET, s->protocol,
-					       (union nf_inet_addr *)&s->caddr,
-					       s->cport,
-					       (union nf_inet_addr *)&s->vaddr,
-					       s->vport);
+			cp = ip_vs_conn_get(AF_INET, s->protocol,
+					    (union nf_inet_addr *)&s->caddr,
+					    s->cport,
+					    (union nf_inet_addr *)&s->vaddr,
+					    s->vport, &res_dir);
 		else
 			cp = ip_vs_ct_in_get(AF_INET, s->protocol,
 					     (union nf_inet_addr *)&s->caddr,
@@ -392,8 +389,7 @@ static void ip_vs_process_message(const 
 					       (union nf_inet_addr *)&s->daddr,
 					       s->dport,
 					       (union nf_inet_addr *)&s->vaddr,
-					       s->vport,
-					       s->protocol);
+					       s->vport, s->protocol);
 			/*  Set the approprite ativity flag */
 			if (s->protocol == IPPROTO_TCP) {
 				if (state != IP_VS_TCP_S_ESTABLISHED)
@@ -407,8 +403,7 @@ static void ip_vs_process_message(const 
 					    (union nf_inet_addr *)&s->vaddr,
 					    s->vport,
 					    (union nf_inet_addr *)&s->daddr,
-					    s->dport,
-					    flags, dest);
+					    s->dport, flags, dest, NULL, 0);
 			if (dest)
 				atomic_dec(&dest->refcnt);
 			if (!cp) {
@@ -424,12 +419,12 @@ static void ip_vs_process_message(const 
 			/* update active/inactive flag for the connection */
 			dest = cp->dest;
 			if (!(cp->flags & IP_VS_CONN_F_INACTIVE) &&
-				(state != IP_VS_TCP_S_ESTABLISHED)) {
+			    (state != IP_VS_TCP_S_ESTABLISHED)) {
 				atomic_dec(&dest->activeconns);
 				atomic_inc(&dest->inactconns);
 				cp->flags |= IP_VS_CONN_F_INACTIVE;
 			} else if ((cp->flags & IP_VS_CONN_F_INACTIVE) &&
-				(state == IP_VS_TCP_S_ESTABLISHED)) {
+				   (state == IP_VS_TCP_S_ESTABLISHED)) {
 				atomic_inc(&dest->activeconns);
 				atomic_dec(&dest->inactconns);
 				cp->flags &= ~IP_VS_CONN_F_INACTIVE;
@@ -450,12 +445,11 @@ static void ip_vs_process_message(const 
 		if (!(flags & IP_VS_CONN_F_TEMPLATE) && pp->timeout_table)
 			cp->timeout = pp->timeout_table[state];
 		else
-			cp->timeout = (3*60*HZ);
+			cp->timeout = (3 * 60 * HZ);
 		ip_vs_conn_put(cp);
 	}
 }
 
-
 /*
  *      Setup loopback of outgoing multicasts on a sending socket
  */
@@ -504,7 +498,6 @@ static int set_mcast_if(struct sock *sk,
 	return 0;
 }
 
-
 /*
  *	Set the maximum length of sync message according to the
  *	specified interface's MTU.
@@ -515,22 +508,26 @@ static int set_sync_mesg_maxlen(int sync
 	int num;
 
 	if (sync_state == IP_VS_STATE_MASTER) {
-		if ((dev = __dev_get_by_name(&init_net, ip_vs_master_mcast_ifn)) == NULL)
+		if ((dev =
+		     __dev_get_by_name(&init_net,
+				       ip_vs_master_mcast_ifn)) == NULL)
 			return -ENODEV;
 
 		num = (dev->mtu - sizeof(struct iphdr) -
 		       sizeof(struct udphdr) -
 		       SYNC_MESG_HEADER_LEN - 20) / SIMPLE_CONN_SIZE;
 		sync_send_mesg_maxlen = SYNC_MESG_HEADER_LEN +
-			SIMPLE_CONN_SIZE * min(num, MAX_CONNS_PER_SYNCBUFF);
+		    SIMPLE_CONN_SIZE * min(num, MAX_CONNS_PER_SYNCBUFF);
 		IP_VS_DBG(7, "setting the maximum length of sync sending "
 			  "message %d.\n", sync_send_mesg_maxlen);
 	} else if (sync_state == IP_VS_STATE_BACKUP) {
-		if ((dev = __dev_get_by_name(&init_net, ip_vs_backup_mcast_ifn)) == NULL)
+		if ((dev =
+		     __dev_get_by_name(&init_net,
+				       ip_vs_backup_mcast_ifn)) == NULL)
 			return -ENODEV;
 
 		sync_recv_mesg_maxlen = dev->mtu -
-			sizeof(struct iphdr) - sizeof(struct udphdr);
+		    sizeof(struct iphdr) - sizeof(struct udphdr);
 		IP_VS_DBG(7, "setting the maximum length of sync receiving "
 			  "message %d.\n", sync_recv_mesg_maxlen);
 	}
@@ -538,14 +535,12 @@ static int set_sync_mesg_maxlen(int sync
 	return 0;
 }
 
-
 /*
  *      Join a multicast group.
  *      the group is specified by a class D multicast address 224.0.0.0/8
  *      in the in_addr structure passed in as a parameter.
  */
-static int
-join_mcast_group(struct sock *sk, struct in_addr *addr, char *ifname)
+static int join_mcast_group(struct sock *sk, struct in_addr *addr, char *ifname)
 {
 	struct ip_mreqn mreq;
 	struct net_device *dev;
@@ -568,7 +563,6 @@ join_mcast_group(struct sock *sk, struct
 	return ret;
 }
 
-
 static int bind_mcastif_addr(struct socket *sock, char *ifname)
 {
 	struct net_device *dev;
@@ -583,21 +577,20 @@ static int bind_mcastif_addr(struct sock
 		pr_err("You probably need to specify IP address on "
 		       "multicast interface.\n");
 
-	IP_VS_DBG(7, "binding socket with (%s) %pI4\n",
-		  ifname, &addr);
+	IP_VS_DBG(7, "binding socket with (%s) %pI4\n", ifname, &addr);
 
 	/* Now bind the socket with the address of multicast interface */
-	sin.sin_family	     = AF_INET;
-	sin.sin_addr.s_addr  = addr;
-	sin.sin_port         = 0;
+	sin.sin_family = AF_INET;
+	sin.sin_addr.s_addr = addr;
+	sin.sin_port = 0;
 
-	return sock->ops->bind(sock, (struct sockaddr*)&sin, sizeof(sin));
+	return sock->ops->bind(sock, (struct sockaddr *)&sin, sizeof(sin));
 }
 
 /*
  *      Set up sending multicast socket over UDP
  */
-static struct socket * make_send_sock(void)
+static struct socket *make_send_sock(void)
 {
 	struct socket *sock;
 	int result;
@@ -624,8 +617,8 @@ static struct socket * make_send_sock(vo
 		goto error;
 	}
 
-	result = sock->ops->connect(sock, (struct sockaddr *) &mcast_addr,
-			sizeof(struct sockaddr), 0);
+	result = sock->ops->connect(sock, (struct sockaddr *)&mcast_addr,
+				    sizeof(struct sockaddr), 0);
 	if (result < 0) {
 		pr_err("Error connecting to the multicast addr\n");
 		goto error;
@@ -633,16 +626,15 @@ static struct socket * make_send_sock(vo
 
 	return sock;
 
-  error:
+      error:
 	sock_release(sock);
 	return ERR_PTR(result);
 }
 
-
 /*
  *      Set up receiving multicast socket over UDP
  */
-static struct socket * make_receive_sock(void)
+static struct socket *make_receive_sock(void)
 {
 	struct socket *sock;
 	int result;
@@ -657,8 +649,8 @@ static struct socket * make_receive_sock
 	/* it is equivalent to the REUSEADDR option in user-space */
 	sock->sk->sk_reuse = 1;
 
-	result = sock->ops->bind(sock, (struct sockaddr *) &mcast_addr,
-			sizeof(struct sockaddr));
+	result = sock->ops->bind(sock, (struct sockaddr *)&mcast_addr,
+				 sizeof(struct sockaddr));
 	if (result < 0) {
 		pr_err("Error binding to the multicast addr\n");
 		goto error;
@@ -666,8 +658,8 @@ static struct socket * make_receive_sock
 
 	/* join the multicast group */
 	result = join_mcast_group(sock->sk,
-			(struct in_addr *) &mcast_addr.sin_addr,
-			ip_vs_backup_mcast_ifn);
+				  (struct in_addr *)&mcast_addr.sin_addr,
+				  ip_vs_backup_mcast_ifn);
 	if (result < 0) {
 		pr_err("Error joining to the multicast group\n");
 		goto error;
@@ -675,24 +667,23 @@ static struct socket * make_receive_sock
 
 	return sock;
 
-  error:
+      error:
 	sock_release(sock);
 	return ERR_PTR(result);
 }
 
-
 static int
 ip_vs_send_async(struct socket *sock, const char *buffer, const size_t length)
 {
-	struct msghdr	msg = {.msg_flags = MSG_DONTWAIT|MSG_NOSIGNAL};
-	struct kvec	iov;
-	int		len;
+	struct msghdr msg = {.msg_flags = MSG_DONTWAIT | MSG_NOSIGNAL };
+	struct kvec iov;
+	int len;
 
 	EnterFunction(7);
-	iov.iov_base     = (void *)buffer;
-	iov.iov_len      = length;
+	iov.iov_base = (void *)buffer;
+	iov.iov_len = length;
 
-	len = kernel_sendmsg(sock, &msg, &iov, 1, (size_t)(length));
+	len = kernel_sendmsg(sock, &msg, &iov, 1, (size_t) (length));
 
 	LeaveFunction(7);
 	return len;
@@ -712,18 +703,17 @@ ip_vs_send_sync_msg(struct socket *sock,
 		pr_err("ip_vs_send_async error\n");
 }
 
-static int
-ip_vs_receive(struct socket *sock, char *buffer, const size_t buflen)
+static int ip_vs_receive(struct socket *sock, char *buffer, const size_t buflen)
 {
-	struct msghdr		msg = {NULL,};
-	struct kvec		iov;
-	int			len;
+	struct msghdr msg = { NULL, };
+	struct kvec iov;
+	int len;
 
 	EnterFunction(7);
 
 	/* Receive a packet */
-	iov.iov_base     = buffer;
-	iov.iov_len      = (size_t)buflen;
+	iov.iov_base = buffer;
+	iov.iov_len = (size_t) buflen;
 
 	len = kernel_recvmsg(sock, &msg, &iov, 1, buflen, 0);
 
@@ -734,15 +724,13 @@ ip_vs_receive(struct socket *sock, char 
 	return len;
 }
 
-
 static int sync_thread_master(void *data)
 {
 	struct ip_vs_sync_thread_data *tinfo = data;
 	struct ip_vs_sync_buff *sb;
 
 	pr_info("sync thread started: state = MASTER, mcast_ifn = %s, "
-		"syncid = %d\n",
-		ip_vs_master_mcast_ifn, ip_vs_master_syncid);
+		"syncid = %d\n", ip_vs_master_mcast_ifn, ip_vs_master_syncid);
 
 	while (!kthread_should_stop()) {
 		while ((sb = sb_dequeue())) {
@@ -761,7 +749,7 @@ static int sync_thread_master(void *data
 	}
 
 	/* clean up the sync_buff queue */
-	while ((sb=sb_dequeue())) {
+	while ((sb = sb_dequeue())) {
 		ip_vs_sync_buff_release(sb);
 	}
 
@@ -777,25 +765,24 @@ static int sync_thread_master(void *data
 	return 0;
 }
 
-
 static int sync_thread_backup(void *data)
 {
 	struct ip_vs_sync_thread_data *tinfo = data;
 	int len;
 
 	pr_info("sync thread started: state = BACKUP, mcast_ifn = %s, "
-		"syncid = %d\n",
-		ip_vs_backup_mcast_ifn, ip_vs_backup_syncid);
+		"syncid = %d\n", ip_vs_backup_mcast_ifn, ip_vs_backup_syncid);
 
 	while (!kthread_should_stop()) {
 		wait_event_interruptible(*tinfo->sock->sk->sk_sleep,
-			 !skb_queue_empty(&tinfo->sock->sk->sk_receive_queue)
-			 || kthread_should_stop());
+					 !skb_queue_empty(&tinfo->sock->sk->
+							  sk_receive_queue)
+					 || kthread_should_stop());
 
 		/* do we have data now? */
 		while (!skb_queue_empty(&(tinfo->sock->sk->sk_receive_queue))) {
 			len = ip_vs_receive(tinfo->sock, tinfo->buf,
-					sync_recv_mesg_maxlen);
+					    sync_recv_mesg_maxlen);
 			if (len <= 0) {
 				pr_err("receiving message error\n");
 				break;
@@ -817,14 +804,13 @@ static int sync_thread_backup(void *data
 	return 0;
 }
 
-
 int start_sync_thread(int state, char *mcast_ifn, __u8 syncid)
 {
 	struct ip_vs_sync_thread_data *tinfo;
 	struct task_struct **realtask, *task;
 	struct socket *sock;
 	char *name, *buf = NULL;
-	int (*threadfn)(void *data);
+	int (*threadfn) (void *data);
 	int result = -ENOMEM;
 
 	IP_VS_DBG(7, "%s(): pid %d\n", __func__, task_pid_nr(current));
@@ -891,17 +877,16 @@ int start_sync_thread(int state, char *m
 
 	return 0;
 
-outtinfo:
+      outtinfo:
 	kfree(tinfo);
-outbuf:
+      outbuf:
 	kfree(buf);
-outsocket:
+      outsocket:
 	sock_release(sock);
-out:
+      out:
 	return result;
 }
 
-
 int stop_sync_thread(int state)
 {
 	IP_VS_DBG(7, "%s(): pid %d\n", __func__, task_pid_nr(current));
diff -uprN linux-2.6.32-220.23.1.el6.x86_64.bak/net/netfilter/ipvs/ip_vs_synproxy.c linux-2.6.32-220.23.1.el6.x86_64/net/netfilter/ipvs/ip_vs_synproxy.c
--- linux-2.6.32-220.23.1.el6.x86_64.bak/net/netfilter/ipvs/ip_vs_synproxy.c	1970-01-01 08:00:00.000000000 +0800
+++ linux-2.6.32-220.23.1.el6.x86_64/net/netfilter/ipvs/ip_vs_synproxy.c	2013-03-18 19:24:01.000000000 +0800
@@ -0,0 +1,1108 @@
+/*
+ * ip_vs_synproxy.c:   SYNPROXY for defence synflood attack, based on tcp syncookies
+ *
+ * Authors:     
+ * 		Jian Chen  <jian.chen1225@gmail.com>
+ * 		Yan Tian   <tianyan.7c00@gmail.com>
+ * 		Wen Li     <steel.mental@gmail.com>
+ * 		Jiaming Wu <pukong.wjm@taobao.com>
+ *
+ *              This program is free software; you can redistribute it and/or
+ *              modify it under the terms of the GNU General Public License
+ *              as published by the Free Software Foundation; either version
+ *              2 of the License, or (at your option) any later version.
+ *
+ * Changes:
+ */
+
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/tcp.h>
+#include <linux/if_arp.h>
+
+#include <net/ip.h>
+#include <net/tcp.h>
+#include <net/udp.h>
+#include <net/icmp.h>		/* for icmp_send */
+#include <net/route.h>
+
+#include <linux/netfilter.h>
+#include <linux/netfilter_ipv4.h>
+
+#ifdef CONFIG_IP_VS_IPV6
+#include <net/ipv6.h>
+#include <linux/netfilter_ipv6.h>
+#endif
+
+#include <net/ip_vs.h>
+#include <net/ip_vs_synproxy.h>
+
+static inline void
+syn_proxy_seq_csum_update(struct tcphdr *tcph, __u32 old_seq, __u32 new_seq)
+{
+	tcph->check = csum_fold(ip_vs_check_diff4(old_seq, new_seq,
+						  ~csum_unfold(tcph->check)));
+}
+
+/*
+ * Replace tcp options in tcp header, called by syn_proxy_reuse_skb()
+ *
+ */
+static void
+syn_proxy_parse_set_opts(struct sk_buff *skb, struct tcphdr *th,
+			 struct ip_vs_synproxy_opt *opt)
+{
+	/* mss in received packet */
+	__u16 in_mss;
+	__u32 *tmp;
+	unsigned char *ptr;
+	int length = (th->doff * 4) - sizeof(struct tcphdr);
+	/*tcp_sk(sk)->user_mss. set from proc */
+	__u16 user_mss = sysctl_ip_vs_synproxy_init_mss;
+
+	memset(opt, '\0', sizeof(struct ip_vs_synproxy_opt));
+	opt->mss_clamp = 536;
+	ptr = (unsigned char *)(th + 1);
+
+	while (length > 0) {
+		unsigned char *tmp_opcode = ptr;
+		int opcode = *ptr++;
+		int opsize;
+
+		switch (opcode) {
+		case TCPOPT_EOL:
+			return;
+		case TCPOPT_NOP:
+			length--;
+			continue;
+		default:
+			opsize = *ptr++;
+			if (opsize < 2)	/* "silly options" */
+				return;
+			if (opsize > length)
+				return;	/* don't parse partial options */
+			switch (opcode) {
+			case TCPOPT_MSS:
+				if (opsize == TCPOLEN_MSS) {
+					in_mss = ntohs(*(__u16 *) ptr);
+					if (in_mss) {
+						if (user_mss < in_mss) {
+							in_mss = user_mss;
+						}
+						opt->mss_clamp = in_mss;
+					}
+					*(__u16 *) ptr = htons(opt->mss_clamp);
+				}
+				break;
+			case TCPOPT_WINDOW:
+				if (opsize == TCPOLEN_WINDOW) {
+					if (sysctl_ip_vs_synproxy_wscale) {
+						opt->wscale_ok = 1;
+						opt->snd_wscale = *(__u8 *) ptr;
+						if (opt->snd_wscale >
+						    IP_VS_SYNPROXY_WSCALE_MAX) {
+							IP_VS_DBG(6,
+								  "tcp_parse_options: Illegal window "
+								  "scaling value %d > %d received.",
+								  opt->
+								  snd_wscale,
+								  IP_VS_SYNPROXY_WSCALE_MAX);
+							opt->snd_wscale =
+							    IP_VS_SYNPROXY_WSCALE_MAX;
+						}
+						*(__u8 *) ptr = (__u8)
+						    sysctl_ip_vs_synproxy_wscale;
+					} else {
+						memset(tmp_opcode, TCPOPT_NOP,
+						       TCPOLEN_WINDOW);
+					}
+				}
+				break;
+			case TCPOPT_TIMESTAMP:
+				if (opsize == TCPOLEN_TIMESTAMP) {
+					if (sysctl_ip_vs_synproxy_timestamp) {
+						opt->tstamp_ok = 1;
+						tmp = (__u32 *) ptr;
+						*(tmp + 1) = *tmp;
+						*tmp = htonl(tcp_time_stamp);
+					} else {
+						memset(tmp_opcode, TCPOPT_NOP,
+						       TCPOLEN_TIMESTAMP);
+					}
+				}
+				break;
+			case TCPOPT_SACK_PERM:
+				if (opsize == TCPOLEN_SACK_PERM) {
+					if (sysctl_ip_vs_synproxy_sack) {
+						opt->sack_ok = 1;
+					} else {
+						memset(tmp_opcode, TCPOPT_NOP,
+						       TCPOLEN_SACK_PERM);
+					}
+				}
+				break;
+			}
+			ptr += opsize - 2;
+			length -= opsize;
+		}
+	}
+}
+
+/*
+ * Reuse skb for syn proxy, called by syn_proxy_syn_rcv().
+ * do following things:
+ * 1) set tcp options;
+ * 2) compute seq with cookie func.
+ * 3) set tcp seq and ack_seq;
+ * 4) exchange ip addr and tcp port;
+ * 5) compute iphdr and tcp check.
+ *
+ */
+static void
+syn_proxy_reuse_skb(int af, struct sk_buff *skb, struct ip_vs_synproxy_opt *opt)
+{
+	__u32 isn;
+	unsigned short tmpport;
+	unsigned int tcphoff;
+	struct tcphdr *th;
+	af &= ~IP_VS_CONN_F_DSNAT;
+
+#ifdef CONFIG_IP_VS_IPV6
+	if (af == AF_INET6)
+		tcphoff = sizeof(struct ipv6hdr);
+	else
+#endif
+		tcphoff = ip_hdrlen(skb);
+
+	th = (void *)skb_network_header(skb) + tcphoff;
+
+	/* deal with tcp options */
+	syn_proxy_parse_set_opts(skb, th, opt);
+
+	/* get cookie */
+	skb_set_transport_header(skb, tcphoff);
+	isn = ip_vs_synproxy_cookie_v4_init_sequence(skb, opt);
+
+	/* Set syn-ack flag
+	 * the tcp opt in syn/ack packet : 00010010 = 0x12
+	 */
+	((u_int8_t *) th)[13] = 0x12;
+
+	/* Exchange ports */
+	tmpport = th->dest;
+	th->dest = th->source;
+	th->source = tmpport;
+
+	/* Set seq(cookie) and ack_seq */
+	th->ack_seq = htonl(ntohl(th->seq) + 1);
+	th->seq = htonl(isn);
+
+	/* Exchange addresses and compute checksums */
+#ifdef CONFIG_IP_VS_IPV6
+	if (af == AF_INET6) {
+		struct ipv6hdr *iph = ipv6_hdr(skb);
+		struct in6_addr tmpAddr;
+
+		memcpy(&tmpAddr, &iph->saddr, sizeof(struct in6_addr));
+		memcpy(&iph->saddr, &iph->daddr, sizeof(struct in6_addr));
+		memcpy(&iph->daddr, &tmpAddr, sizeof(struct in6_addr));
+
+		iph->hop_limit = sysctl_ip_vs_synproxy_synack_ttl;
+
+		th->check = 0;
+		skb->csum = skb_checksum(skb, tcphoff, skb->len - tcphoff, 0);
+		th->check = csum_ipv6_magic(&iph->saddr, &iph->daddr,
+					    skb->len - tcphoff,
+					    IPPROTO_TCP, skb->csum);
+	} else
+#endif
+	{
+		struct iphdr *iph = ip_hdr(skb);
+		__be32 tmpAddr;
+
+		tmpAddr = iph->saddr;
+		iph->saddr = iph->daddr;
+		iph->daddr = tmpAddr;
+
+		iph->ttl = sysctl_ip_vs_synproxy_synack_ttl;
+		iph->tos = 0;
+
+		ip_send_check(iph);
+
+		th->check = 0;
+		skb->csum = skb_checksum(skb, tcphoff, skb->len - tcphoff, 0);
+		th->check = csum_tcpudp_magic(iph->saddr, iph->daddr,
+					      skb->len - tcphoff,
+					      IPPROTO_TCP, skb->csum);
+	}
+}
+
+/*
+ *  syn-proxy step 1 logic:
+ *  Check if synproxy is enabled for this skb, and
+ *  send Syn/Ack back.
+ *
+ *  Synproxy is enabled when:
+ *  1) skb is a Syn packet.
+ *  2) And the service is synproxy-enable.
+ *  3) And ip_vs_todrop return false.
+ *
+ *  @return 0 means the caller should return at once and use
+ *   verdict as return value, return 1 for nothing.
+ */
+int
+ip_vs_synproxy_syn_rcv(int af, struct sk_buff *skb,
+		       struct ip_vs_iphdr *iph, int *verdict)
+{
+	struct ip_vs_service *svc = NULL;
+	struct tcphdr _tcph, *th;
+	struct ip_vs_synproxy_opt tcp_opt;
+
+	
+
+	th = skb_header_pointer(skb, iph->len, sizeof(_tcph), &_tcph);
+	if (unlikely(th == NULL)) {
+		goto syn_rcv_out;
+	}
+
+	if (th->syn && !th->ack && !th->rst && !th->fin &&
+	    (svc =
+	     ip_vs_service_get(af, skb->mark, iph->protocol, &iph->daddr,
+			       th->dest))
+	    && (svc->flags & IP_VS_CONN_F_SYNPROXY)) {
+		// release service here, because don't use it any all.
+		ip_vs_service_put(svc);
+
+		if (ip_vs_todrop()) {
+			/*
+			 * It seems that we are very loaded.
+			 * We have to drop this packet :(
+			 */
+			goto syn_rcv_out;
+		}
+	} else {
+		/*
+		 * release service.
+		 */
+		if (svc != NULL) {
+			ip_vs_service_put(svc);
+		}
+		return 1;
+	}
+
+	EnterFunction(11);
+	
+	/* update statistics */
+	IP_VS_INC_ESTATS(ip_vs_esmib, SYNPROXY_SYN_CNT);
+
+	/* Try to reuse skb if possible */
+	if (unlikely(skb_shared(skb) || skb_cloned(skb))) {
+		struct sk_buff *new_skb = skb_copy(skb, GFP_ATOMIC);
+		if (unlikely(new_skb == NULL)) {
+			goto syn_rcv_out;
+		}
+		/* Drop old skb */
+		kfree_skb(skb);
+		skb = new_skb;
+	}
+
+	/* reuse skb here: deal with tcp options, exchage ip, port. */
+	syn_proxy_reuse_skb(af, skb, &tcp_opt);
+
+	skb->pkt_type = PACKET_OUTGOING;
+
+	/* Send the packet out */
+	if (likely(skb->dev->type == ARPHRD_ETHER)) {
+		unsigned char t_hwaddr[ETH_ALEN];
+
+		/* Move the data pointer to point to the link layer header */
+		struct ethhdr *eth = (struct ethhdr *)skb_mac_header(skb);
+		skb->data = (unsigned char *)skb_mac_header(skb);
+		skb->len += ETH_HLEN;	//sizeof(skb->mac.ethernet);
+
+		memcpy(t_hwaddr, (eth->h_dest), ETH_ALEN);
+		memcpy((eth->h_dest), (eth->h_source), ETH_ALEN);
+		memcpy((eth->h_source), t_hwaddr, ETH_ALEN);
+	}
+
+	dev_queue_xmit(skb);
+	*verdict = NF_STOLEN;
+	return 0;
+      syn_rcv_out:
+	/* Drop the packet when all things are right also,
+	 * then we needn't to kfree_skb() */
+	*verdict = NF_DROP;
+	return 0;
+}
+
+/*
+ * Check if skb has user data.
+ * Attention: decrease iph len also.
+ */
+static inline int
+syn_proxy_ack_has_data(struct sk_buff *skb, struct ip_vs_iphdr *iph,
+		       struct tcphdr *th)
+{
+	IP_VS_DBG(6, "tot_len = %u, iph_len = %u, tcph_len = %u\n",
+		  skb->len, iph->len, th->doff * 4);
+	return (skb->len - iph->len - th->doff * 4) != 0;
+}
+
+static inline void
+syn_proxy_syn_build_options(__be32 * ptr, struct ip_vs_synproxy_opt *opt)
+{
+	*ptr++ =
+	    htonl((TCPOPT_MSS << 24) | (TCPOLEN_MSS << 16) | opt->mss_clamp);
+	if (opt->tstamp_ok) {
+		if (opt->sack_ok)
+			*ptr++ = htonl((TCPOPT_SACK_PERM << 24) |
+				       (TCPOLEN_SACK_PERM << 16) |
+				       (TCPOPT_TIMESTAMP << 8) |
+				       TCPOLEN_TIMESTAMP);
+		else
+			*ptr++ = htonl((TCPOPT_NOP << 24) |
+				       (TCPOPT_NOP << 16) |
+				       (TCPOPT_TIMESTAMP << 8) |
+				       TCPOLEN_TIMESTAMP);
+		*ptr++ = htonl(tcp_time_stamp);	/* TSVAL */
+		*ptr++ = 0;	/* TSECR */
+	} else if (opt->sack_ok)
+		*ptr++ = htonl((TCPOPT_NOP << 24) |
+			       (TCPOPT_NOP << 16) |
+			       (TCPOPT_SACK_PERM << 8) | TCPOLEN_SACK_PERM);
+	if (opt->wscale_ok)
+		*ptr++ = htonl((TCPOPT_NOP << 24) |
+			       (TCPOPT_WINDOW << 16) |
+			       (TCPOLEN_WINDOW << 8) | (opt->snd_wscale));
+}
+
+/*
+ * Create syn packet and send it to rs.
+ * ATTENTION: we also store syn skb in cp if syn retransimition
+ * is tured on.
+ */
+static int
+syn_proxy_send_rs_syn(int af, const struct tcphdr *th,
+		      struct ip_vs_conn *cp, struct sk_buff *skb,
+		      struct ip_vs_protocol *pp, struct ip_vs_synproxy_opt *opt)
+{
+	struct sk_buff *syn_skb;
+	int tcp_hdr_size;
+	__u8 tcp_flags = TCPCB_FLAG_SYN;
+	unsigned int tcphoff;
+	struct tcphdr *new_th;
+
+	if (!cp->packet_xmit) {
+		IP_VS_ERR_RL("warning: packet_xmit is null");
+		return 0;
+	}
+
+	syn_skb = alloc_skb(MAX_TCP_HEADER + 15, GFP_ATOMIC);
+	if (unlikely(syn_skb == NULL)) {
+		IP_VS_ERR_RL("alloc skb failed when send rs syn packet\n");
+		return 0;
+	}
+
+	/* Reserve space for headers */
+	skb_reserve(syn_skb, MAX_TCP_HEADER);
+	tcp_hdr_size = (sizeof(struct tcphdr) + TCPOLEN_MSS +
+			(opt->tstamp_ok ? TCPOLEN_TSTAMP_ALIGNED : 0) +
+			(opt->wscale_ok ? TCPOLEN_WSCALE_ALIGNED : 0) +
+			/* SACK_PERM is in the place of NOP NOP of TS */
+			((opt->sack_ok
+			  && !opt->tstamp_ok) ? TCPOLEN_SACKPERM_ALIGNED : 0));
+
+	new_th = (struct tcphdr *)skb_push(syn_skb, tcp_hdr_size);
+	/* Compose tcp header */
+	skb_reset_transport_header(syn_skb);
+	syn_skb->csum = 0;
+
+	/* Set tcp hdr */
+	new_th->source = th->source;
+	new_th->dest = cp->dest->port;
+	new_th->seq = htonl(ntohl(th->seq) - 1);
+	new_th->ack_seq = 0;
+	*(((__u16 *) new_th) + 6) =
+	    htons(((tcp_hdr_size >> 2) << 12) | tcp_flags);
+	/* FIX_ME: what window should we use */
+	new_th->window = htons(5000);
+	new_th->check = 0;
+	new_th->urg_ptr = 0;
+	new_th->urg = 0;
+	new_th->ece = 0;
+	new_th->cwr = 0;
+
+	syn_proxy_syn_build_options((__be32 *) (new_th + 1), opt);
+
+	/*
+	 * Set ip hdr
+	 * Attention: set source and dest addr to ack skb's.
+	 * we rely on packet_xmit func to do NATs thing.
+	 */
+#ifdef CONFIG_IP_VS_IPV6
+	if (af == AF_INET6) {
+		struct ipv6hdr *ack_iph = ipv6_hdr(skb);
+		struct ipv6hdr *iph =
+		    (struct ipv6hdr *)skb_push(syn_skb, sizeof(struct iphdr));
+
+		tcphoff = sizeof(struct ipv6hdr);
+		skb_reset_network_header(syn_skb);
+		memcpy(&iph->saddr, &ack_iph->saddr, sizeof(struct in6_addr));
+		memcpy(&iph->daddr, &ack_iph->daddr, sizeof(struct in6_addr));
+
+		iph->hop_limit = IPV6_DEFAULT_HOPLIMIT;
+
+		new_th->check = 0;
+		syn_skb->csum =
+		    skb_checksum(syn_skb, tcphoff, syn_skb->len - tcphoff, 0);
+		new_th->check =
+		    csum_ipv6_magic(&iph->saddr, &iph->daddr,
+				    syn_skb->len - tcphoff, IPPROTO_TCP,
+				    syn_skb->csum);
+	} else
+#endif
+	{
+		struct iphdr *ack_iph = ip_hdr(skb);
+		u32 rtos = RT_TOS(ack_iph->tos);
+		struct iphdr *iph =
+		    (struct iphdr *)skb_push(syn_skb, sizeof(struct iphdr));
+
+		tcphoff = sizeof(struct iphdr);
+		skb_reset_network_header(syn_skb);
+		*((__u16 *) iph) = htons((4 << 12) | (5 << 8) | (rtos & 0xff));
+		iph->tot_len = htons(syn_skb->len);
+		iph->frag_off = htons(IP_DF);
+		/* FIX_ME: what ttl shoule we use */
+		iph->ttl = IPDEFTTL;
+		iph->protocol = IPPROTO_TCP;
+		iph->saddr = ack_iph->saddr;
+		iph->daddr = ack_iph->daddr;
+
+		ip_send_check(iph);
+
+		new_th->check = 0;
+		syn_skb->csum =
+		    skb_checksum(syn_skb, tcphoff, syn_skb->len - tcphoff, 0);
+		new_th->check =
+		    csum_tcpudp_magic(iph->saddr, iph->daddr,
+				      syn_skb->len - tcphoff, IPPROTO_TCP,
+				      syn_skb->csum);
+	}
+
+	/* Save syn_skb if syn retransmission is on  */
+	if (sysctl_ip_vs_synproxy_syn_retry > 0) {
+		cp->syn_skb = skb_copy(syn_skb, GFP_ATOMIC);
+		atomic_set(&cp->syn_retry_max, sysctl_ip_vs_synproxy_syn_retry);
+	}
+
+	/* If xmit failed, syn_skb will be freed correctly. */
+	cp->packet_xmit(syn_skb, cp, pp);
+
+	return 1;
+}
+
+/*
+ * Syn-proxy step 2 logic
+ * Receive client's 3-handshakes  Ack packet, do cookie check
+ * and then send syn to rs after creating a session.
+ *
+ */
+int
+ip_vs_synproxy_ack_rcv(int af, struct sk_buff *skb, struct tcphdr *th,
+		       struct ip_vs_protocol *pp, struct ip_vs_conn **cpp,
+		       struct ip_vs_iphdr *iph, int *verdict)
+{
+	struct ip_vs_synproxy_opt opt;
+	struct ip_vs_service *svc;
+	int res_cookie_check;
+	int dsnat;
+	dsnat = af & IP_VS_CONN_F_DSNAT;
+	af &= ~IP_VS_CONN_F_DSNAT;
+
+	
+	/*
+	 * Don't check svc syn-proxy flag, as it may
+	 * be changed after syn-proxy step 1.
+	 */
+	if (!th->syn && th->ack && !th->rst && !th->fin &&
+	    (svc =
+	     ip_vs_service_get(af|dsnat, skb->mark, iph->protocol, &iph->daddr,
+			       th->dest))) {
+		if (ip_vs_todrop()) {
+			/*
+			 * It seems that we are very loaded.
+			 * We have to drop this packet :(
+			 */
+			ip_vs_service_put(svc);
+			*verdict = NF_DROP;
+			return 0;
+		}
+		
+		EnterFunction(11);
+		
+		if (sysctl_ip_vs_synproxy_defer &&
+		    !syn_proxy_ack_has_data(skb, iph, th)) {
+			/* update statistics */
+			IP_VS_INC_ESTATS(ip_vs_esmib, SYNPROXY_NULL_ACK);
+			/*
+			 * When expecting ack packet with payload,
+			 * we get a pure ack, so have to drop it.
+			 */
+			ip_vs_service_put(svc);
+			*verdict = NF_DROP;
+			return 0;
+		}
+
+		/*
+		 * Import: set tcp hdr before cookie check, as it
+		 * will be used in cookie_check funcs.
+		 */
+		skb_set_transport_header(skb, iph->len);
+#ifdef CONFIG_IP_VS_IPV6
+		if (af == AF_INET6) {
+			res_cookie_check = ip_vs_synproxy_v6_cookie_check(skb,
+									  ntohl
+									  (th->
+									   ack_seq)
+									  - 1,
+									  &opt);
+		} else
+#endif
+		{
+			res_cookie_check = ip_vs_synproxy_v4_cookie_check(skb,
+									  ntohl
+									  (th->
+									   ack_seq)
+									  - 1,
+									  &opt);
+		}
+
+		if (!res_cookie_check) {
+			/* update statistics */
+			IP_VS_INC_ESTATS(ip_vs_esmib, SYNPROXY_BAD_ACK);
+			/*
+			 * Cookie check fail, drop it.
+			 */
+			IP_VS_DBG(6, "syn_cookie check failed seq=%u\n",
+				  ntohl(th->ack_seq) - 1);
+			ip_vs_service_put(svc);
+			*verdict = NF_DROP;
+			return 0;
+		}
+
+		/* update statistics */
+		IP_VS_INC_ESTATS(ip_vs_esmib, SYNPROXY_OK_ACK);
+
+		/*
+		 * Let the virtual server select a real server for the
+		 * incoming connection, and create a connection entry.
+		 */
+		*cpp = ip_vs_schedule(svc, skb, 1);
+		if (!*cpp) {
+			IP_VS_DBG(6, "ip_vs_schedule failed\n");
+			*verdict = ip_vs_leave(svc, skb, pp);
+			return 0;
+		}
+
+		/*
+		 * Release service, we don't need it any more.
+		 */
+		ip_vs_service_put(svc);
+
+		/*
+		 * Do anything but print a error msg when fail.
+		 * Because session will be correctly freed in ip_vs_conn_expire.
+		 */
+		if (!syn_proxy_send_rs_syn(af, th, *cpp, skb, pp, &opt)) {
+			IP_VS_ERR_RL("syn_proxy_send_rs_syn failed!\n");
+		}
+
+		/*
+		 * Active sesion timer, and dec refcnt.
+		 * Also stole the skb, and let caller return immediately.
+		 */
+		ip_vs_conn_put(*cpp);
+		*verdict = NF_STOLEN;
+		return 0;
+	}
+
+	return 1;
+}
+
+/*
+ * Update out-in sack seqs, and also correct th->check
+ */
+static inline void
+syn_proxy_filter_opt_outin(struct tcphdr *th, struct ip_vs_seq *sp_seq)
+{
+	unsigned char *ptr;
+	int length = (th->doff * 4) - sizeof(struct tcphdr);
+	__be32 *tmp;
+	__u32 old_ack_seq;
+
+	if (!length)
+		return;
+
+	ptr = (unsigned char *)(th + 1);
+
+	/* Fast path for timestamp-only option */
+	if (length == TCPOLEN_TSTAMP_ALIGNED * 4
+	    && *(__be32 *) ptr == __constant_htonl((TCPOPT_NOP << 24)
+						   | (TCPOPT_NOP << 16)
+						   | (TCPOPT_TIMESTAMP << 8) |
+						   TCPOLEN_TIMESTAMP))
+		return;
+
+	while (length > 0) {
+		int opcode = *ptr++;
+		int opsize, i;
+
+		switch (opcode) {
+		case TCPOPT_EOL:
+			return;
+		case TCPOPT_NOP:	/* Ref: RFC 793 section 3.1 */
+			length--;
+			continue;
+		default:
+			opsize = *ptr++;
+			if (opsize < 2)	/* "silly options" */
+				return;
+			if (opsize > length)
+				break;	/* don't parse partial options */
+
+			if (opcode == TCPOPT_SACK
+			    && opsize >= (TCPOLEN_SACK_BASE
+					  + TCPOLEN_SACK_PERBLOCK)
+			    && !((opsize - TCPOLEN_SACK_BASE) %
+				 TCPOLEN_SACK_PERBLOCK)) {
+				for (i = 0; i < (opsize - TCPOLEN_SACK_BASE);
+				     i += TCPOLEN_SACK_PERBLOCK) {
+					tmp = (__be32 *) (ptr + i);
+					old_ack_seq = ntohl(*tmp);
+					*tmp = htonl((__u32)
+						     (old_ack_seq -
+						      sp_seq->delta));
+					syn_proxy_seq_csum_update(th,
+								  htonl
+								  (old_ack_seq),
+								  *tmp);
+					IP_VS_DBG(6,
+						  "syn_proxy_filter_opt_outin: sack_left_seq %u => %u, delta = %u \n",
+						  old_ack_seq, ntohl(*tmp),
+						  sp_seq->delta);
+					tmp++;
+					old_ack_seq = ntohl(*tmp);
+					*tmp = htonl((__u32)
+						     (old_ack_seq -
+						      sp_seq->delta));
+					syn_proxy_seq_csum_update(th,
+								  htonl
+								  (old_ack_seq),
+								  *tmp);
+					IP_VS_DBG(6,
+						  "syn_proxy_filter_opt_outin: sack_right_seq %u => %u, delta = %u \n",
+						  old_ack_seq, ntohl(*tmp),
+						  sp_seq->delta);
+				}
+				return;
+			}
+			ptr += opsize - 2;
+			length -= opsize;
+		}
+	}
+}
+
+/*
+ * Update out-in ack_seqs: include th->ack_seq, sack opt
+ * and also correct tcph->check.
+ */
+void ip_vs_synproxy_dnat_handler(struct tcphdr *tcph, struct ip_vs_seq *sp_seq)
+{
+	__u32 old_ack_seq;
+
+	if (sp_seq->delta != 0) {
+		old_ack_seq = ntohl(tcph->ack_seq);
+		tcph->ack_seq = htonl((__u32) (old_ack_seq - sp_seq->delta));
+		syn_proxy_seq_csum_update(tcph, htonl(old_ack_seq),
+					  tcph->ack_seq);
+		syn_proxy_filter_opt_outin(tcph, sp_seq);
+		IP_VS_DBG(6,
+			  "tcp_dnat_handler: tcph->ack_seq %u => %u, delta = %u \n",
+			  old_ack_seq, htonl(tcph->ack_seq), sp_seq->delta);
+	}
+}
+
+/*
+ * Syn-proxy step 3 logic: receive syn-ack from rs
+ * Update syn_proxy_seq.delta and send stored ack skbs
+ * to rs.
+ */
+int
+ip_vs_synproxy_synack_rcv(struct sk_buff *skb, struct ip_vs_conn *cp,
+			  struct ip_vs_protocol *pp, int ihl, int *verdict)
+{
+	struct tcphdr _tcph, *th;
+	struct sk_buff_head save_skb;
+	struct sk_buff *tmp_skb = NULL;
+	struct ip_vs_dest *dest = cp->dest;
+
+	EnterFunction(11);
+	
+	th = skb_header_pointer(skb, ihl, sizeof(_tcph), &_tcph);
+	if (th == NULL) {
+		*verdict = NF_DROP;
+		return 0;
+	}
+
+	IP_VS_DBG(6, "in syn_proxy_synack_rcv, "
+		  "seq = %u ack_seq = %u %c%c%c cp->is_synproxy = %u cp->state = %u\n",
+		  ntohl(th->seq),
+		  ntohl(th->ack_seq),
+		  (th->syn) ? 'S' : '-',
+		  (th->ack) ? 'A' : '-',
+		  (th->rst) ? 'R' : '-',
+		  cp->flags & IP_VS_CONN_F_SYNPROXY, cp->state);
+
+	skb_queue_head_init(&save_skb);
+	spin_lock(&cp->lock);
+	if ((th->syn) && (th->ack) && (!th->rst) &&
+	    (cp->flags & IP_VS_CONN_F_SYNPROXY) &&
+	    cp->state == IP_VS_TCP_S_SYN_SENT) {
+		cp->syn_proxy_seq.delta =
+		    htonl(cp->syn_proxy_seq.init_seq) - htonl(th->seq);
+		cp->timeout = pp->timeout_table[cp->state =
+						IP_VS_TCP_S_ESTABLISHED];
+		if (dest) {
+			atomic_inc(&dest->activeconns);
+			atomic_dec(&dest->inactconns);
+			cp->flags &= ~IP_VS_CONN_F_INACTIVE;
+		}
+
+		/* save tcp sequense for fullnat/nat, INside to OUTside */
+		if (sysctl_ip_vs_conn_expire_tcp_rst == 1) {
+			cp->rs_end_seq = htonl(ntohl(th->seq) + 1);
+			cp->rs_ack_seq = th->ack_seq;
+			IP_VS_DBG_RL("packet from RS, seq:%u ack_seq:%u.",
+				     ntohl(th->seq), ntohl(th->ack_seq));
+			IP_VS_DBG_RL("port:%u->%u", ntohs(th->source),
+				     ntohs(th->dest));
+		}
+
+		/* First: free stored syn skb */
+		if ((tmp_skb = xchg(&cp->syn_skb, NULL)) != NULL) {
+			kfree_skb(tmp_skb);
+			tmp_skb = NULL;
+		}
+
+		if (skb_queue_len(&cp->ack_skb) <= 0) {
+			/*
+			 * FIXME: maybe a bug here, print err msg and go.
+			 * Attention: cp->state has been changed and we
+			 * should still DROP the Syn/Ack skb.
+			 */
+			IP_VS_ERR_RL
+			    ("Got ack_skb NULL pointer in syn_proxy_synack_rcv\n");
+			spin_unlock(&cp->lock);
+			*verdict = NF_DROP;
+			return 0;
+		}
+
+		while ((tmp_skb = skb_dequeue(&cp->ack_skb)) != NULL) {
+			skb_queue_tail(&save_skb, tmp_skb);
+		}
+
+		/*
+		 * Release the lock, because we don't
+		 * touch session any more.
+		 */
+		spin_unlock(&cp->lock);
+
+		while ((tmp_skb = skb_dequeue(&save_skb)) != NULL) {
+			/* If xmit failed, syn_skb will be freed correctly. */
+			cp->packet_xmit(tmp_skb, cp, pp);
+		}
+
+		*verdict = NF_DROP;
+		return 0;
+	} else if ((th->rst) &&
+		   (cp->flags & IP_VS_CONN_F_SYNPROXY) &&
+		   cp->state == IP_VS_TCP_S_SYN_SENT) {
+		__u32 temp_seq;
+		temp_seq = ntohl(th->seq);
+		IP_VS_DBG(6, "get rst from rs, seq = %u ack_seq= %u\n",
+			  ntohl(th->seq), ntohl(th->ack_seq));
+		/* coute the delta of seq */
+		cp->syn_proxy_seq.delta =
+		    ntohl(cp->syn_proxy_seq.init_seq) - ntohl(th->seq);
+		cp->timeout = pp->timeout_table[cp->state = IP_VS_TCP_S_CLOSE];
+		spin_unlock(&cp->lock);
+		th->seq = htonl(ntohl(th->seq) + 1);
+		syn_proxy_seq_csum_update(th, htonl(temp_seq), th->seq);
+
+		return 1;
+	}
+	spin_unlock(&cp->lock);
+
+	return 1;
+}
+
+static inline void
+__syn_proxy_reuse_conn(struct ip_vs_conn *cp,
+		       struct sk_buff *ack_skb,
+		       struct tcphdr *th, struct ip_vs_protocol *pp)
+{
+	struct sk_buff *tmp_skb = NULL;
+
+	/* Free stored ack packet */
+	while ((tmp_skb = skb_dequeue(&cp->ack_skb)) != NULL) {
+		kfree_skb(tmp_skb);
+		tmp_skb = NULL;
+	}
+
+	/* Free stored syn skb */
+	if ((tmp_skb = xchg(&cp->syn_skb, NULL)) != NULL) {
+		kfree_skb(tmp_skb);
+		tmp_skb = NULL;
+	}
+
+	/* Store new ack_skb */
+	skb_queue_head_init(&cp->ack_skb);
+	skb_queue_tail(&cp->ack_skb, ack_skb);
+
+	/* Save ack_seq - 1 */
+	cp->syn_proxy_seq.init_seq = htonl((__u32) ((htonl(th->ack_seq) - 1)));
+	/* don't change delta here, so original flow can still be valid */
+
+	/* Clean dup ack cnt */
+	atomic_set(&cp->dup_ack_cnt, 0);
+
+	/* Set timeout value */
+	cp->timeout = pp->timeout_table[cp->state = IP_VS_TCP_S_SYN_SENT];
+}
+
+/*
+ * Syn-proxy session reuse function.
+ * Update syn_proxy_seq struct and clean syn-proxy related
+ * members.
+ */
+int
+ip_vs_synproxy_reuse_conn(int af, struct sk_buff *skb,
+			  struct ip_vs_conn *cp,
+			  struct ip_vs_protocol *pp,
+			  struct ip_vs_iphdr *iph, int *verdict)
+{
+	struct tcphdr _tcph, *th = NULL;
+	struct ip_vs_synproxy_opt opt;
+	int res_cookie_check;
+	u32 tcp_conn_reuse_states = 0;
+	af &= ~IP_VS_CONN_F_DSNAT;
+
+	th = skb_header_pointer(skb, iph->len, sizeof(_tcph), &_tcph);
+	if (unlikely(NULL == th)) {
+		IP_VS_ERR_RL("skb has a invalid tcp header\n");
+		*verdict = NF_DROP;
+		return 0;
+	}
+
+	tcp_conn_reuse_states =
+	    ((sysctl_ip_vs_synproxy_conn_reuse_cl << IP_VS_TCP_S_CLOSE) |
+	     (sysctl_ip_vs_synproxy_conn_reuse_tw << IP_VS_TCP_S_TIME_WAIT) |
+	     (sysctl_ip_vs_synproxy_conn_reuse_fw << IP_VS_TCP_S_FIN_WAIT) |
+	     (sysctl_ip_vs_synproxy_conn_reuse_cw << IP_VS_TCP_S_CLOSE_WAIT) |
+	     (sysctl_ip_vs_synproxy_conn_reuse_la << IP_VS_TCP_S_LAST_ACK));
+
+	if (((1 << (cp->state)) & tcp_conn_reuse_states) &&
+	    (cp->flags & IP_VS_CONN_F_SYNPROXY) &&
+	    (!th->syn && th->ack && !th->rst && !th->fin) &&
+	    (cp->syn_proxy_seq.init_seq !=
+	     htonl((__u32) ((ntohl(th->ack_seq) - 1))))) {
+		/*
+		 * Import: set tcp hdr before cookie check, as it
+		 * will be used in cookie_check funcs.
+		 */
+		skb_set_transport_header(skb, iph->len);
+#ifdef CONFIG_IP_VS_IPV6
+		if (af == AF_INET6) {
+			res_cookie_check = ip_vs_synproxy_v6_cookie_check(skb,
+									  ntohl
+									  (th->
+									   ack_seq)
+									  - 1,
+									  &opt);
+		} else
+#endif
+		{
+			res_cookie_check = ip_vs_synproxy_v4_cookie_check(skb,
+									  ntohl
+									  (th->
+									   ack_seq)
+									  - 1,
+									  &opt);
+		}
+
+		if (!res_cookie_check) {
+			/* update statistics */
+			IP_VS_INC_ESTATS(ip_vs_esmib, SYNPROXY_BAD_ACK);
+			/*
+			 * Cookie check fail, let it go.
+			 */
+			return 1;
+		}
+
+		/* update statistics */
+		IP_VS_INC_ESTATS(ip_vs_esmib, SYNPROXY_OK_ACK);
+		IP_VS_INC_ESTATS(ip_vs_esmib, SYNPROXY_CONN_REUSED);
+		switch (cp->old_state) {
+		case IP_VS_TCP_S_CLOSE:
+			IP_VS_INC_ESTATS(ip_vs_esmib,
+					 SYNPROXY_CONN_REUSED_CLOSE);
+			break;
+		case IP_VS_TCP_S_TIME_WAIT:
+			IP_VS_INC_ESTATS(ip_vs_esmib,
+					 SYNPROXY_CONN_REUSED_TIMEWAIT);
+			break;
+		case IP_VS_TCP_S_FIN_WAIT:
+			IP_VS_INC_ESTATS(ip_vs_esmib,
+					 SYNPROXY_CONN_REUSED_FINWAIT);
+			break;
+		case IP_VS_TCP_S_CLOSE_WAIT:
+			IP_VS_INC_ESTATS(ip_vs_esmib,
+					 SYNPROXY_CONN_REUSED_CLOSEWAIT);
+			break;
+		case IP_VS_TCP_S_LAST_ACK:
+			IP_VS_INC_ESTATS(ip_vs_esmib,
+					 SYNPROXY_CONN_REUSED_LASTACK);
+			break;
+		}
+
+		spin_lock(&cp->lock);
+		__syn_proxy_reuse_conn(cp, skb, th, pp);
+		spin_unlock(&cp->lock);
+
+		if (unlikely(!syn_proxy_send_rs_syn(af, th, cp, skb, pp, &opt))) {
+			IP_VS_ERR_RL
+			    ("syn_proxy_send_rs_syn failed when reuse conn!\n");
+			/* release conn immediately */
+			spin_lock(&cp->lock);
+			cp->timeout = 0;
+			spin_unlock(&cp->lock);
+		}
+
+		*verdict = NF_STOLEN;
+		return 0;
+	}
+
+	return 1;
+}
+
+/*
+ * Check and stop ack storm.
+ * Return 0 if ack storm is found.
+ */
+static int syn_proxy_is_ack_storm(struct tcphdr *tcph, struct ip_vs_conn *cp)
+{
+	/* only for syn-proxy sessions */
+	if (!(cp->flags & IP_VS_CONN_F_SYNPROXY) || !tcph->ack)
+		return 1;
+
+	if (unlikely(sysctl_ip_vs_synproxy_dup_ack_thresh == 0))
+		return 1;
+
+	if (unlikely(tcph->seq == cp->last_seq &&
+		     tcph->ack_seq == cp->last_ack_seq)) {
+		atomic_inc(&cp->dup_ack_cnt);
+		if (atomic_read(&cp->dup_ack_cnt) >=
+		    sysctl_ip_vs_synproxy_dup_ack_thresh) {
+			atomic_set(&cp->dup_ack_cnt,
+				   sysctl_ip_vs_synproxy_dup_ack_thresh);
+			/* update statistics */
+			IP_VS_INC_ESTATS(ip_vs_esmib, SYNPROXY_ACK_STORM);
+			return 0;
+		}
+
+		return 1;
+	}
+
+	cp->last_seq = tcph->seq;
+	cp->last_ack_seq = tcph->ack_seq;
+	atomic_set(&cp->dup_ack_cnt, 0);
+
+	return 1;
+}
+
+/*
+ * Syn-proxy snat handler:
+ * 1) check and stop ack storm.
+ * 2)Update in-out seqs: include th->seq
+ * and also correct tcph->check.
+ *
+ * Return 0 if ack storm is found and stoped.
+ */
+int ip_vs_synproxy_snat_handler(struct tcphdr *tcph, struct ip_vs_conn *cp)
+{
+	__u32 old_seq;
+
+	if (syn_proxy_is_ack_storm(tcph, cp) == 0) {
+		return 0;
+	}
+
+	if (cp->syn_proxy_seq.delta != 0) {
+		old_seq = ntohl(tcph->seq);
+		tcph->seq = htonl((__u32) (old_seq + cp->syn_proxy_seq.delta));
+		syn_proxy_seq_csum_update(tcph, htonl(old_seq), tcph->seq);
+		IP_VS_DBG(6,
+			  "tcp_snat_handler: tcph->seq %u => %u, delta = %u \n",
+			  old_seq, htonl(tcph->seq), cp->syn_proxy_seq.delta);
+	}
+
+	return 1;
+}
+
+int
+ip_vs_synproxy_filter_ack(struct sk_buff *skb, struct ip_vs_conn *cp,
+			  struct ip_vs_protocol *pp,
+			  struct ip_vs_iphdr *iph, int *verdict)
+{
+	struct tcphdr _tcph, *th;
+
+	th = skb_header_pointer(skb, iph->len, sizeof(_tcph), &_tcph);
+
+	if (unlikely(NULL == th)) {
+		IP_VS_ERR_RL("skb has a invalid tcp header\n");
+		*verdict = NF_DROP;
+		return 0;
+	}
+
+	spin_lock(&cp->lock);
+	if ((cp->flags & IP_VS_CONN_F_SYNPROXY) &&
+	    cp->state == IP_VS_TCP_S_SYN_SENT) {
+		/*
+		 * Not a ack packet, drop it.
+		 */
+		if (!th->ack) {
+			spin_unlock(&cp->lock);
+			*verdict = NF_DROP;
+			return 0;
+		}
+
+		if (sysctl_ip_vs_synproxy_skb_store_thresh <
+		    skb_queue_len(&cp->ack_skb)) {
+			spin_unlock(&cp->lock);
+			/* update statistics */
+			IP_VS_INC_ESTATS(ip_vs_esmib, SYNPROXY_SYNSEND_QLEN);
+			*verdict = NF_DROP;
+			return 0;
+		}
+
+		/*
+		 * Still some space left, store it.
+		 */
+		skb_queue_tail(&cp->ack_skb, skb);
+		spin_unlock(&cp->lock);
+		*verdict = NF_STOLEN;
+		return 0;
+	}
+
+	spin_unlock(&cp->lock);
+	return 1;
+}
diff -uprN linux-2.6.32-220.23.1.el6.x86_64.bak/net/netfilter/ipvs/ip_vs_wlc.c linux-2.6.32-220.23.1.el6.x86_64/net/netfilter/ipvs/ip_vs_wlc.c
--- linux-2.6.32-220.23.1.el6.x86_64.bak/net/netfilter/ipvs/ip_vs_wlc.c	2009-12-03 11:51:21.000000000 +0800
+++ linux-2.6.32-220.23.1.el6.x86_64/net/netfilter/ipvs/ip_vs_wlc.c	2012-10-11 15:01:03.000000000 +0800
@@ -27,27 +27,24 @@
 
 #include <net/ip_vs.h>
 
-
-static inline unsigned int
-ip_vs_wlc_dest_overhead(struct ip_vs_dest *dest)
+static inline unsigned int ip_vs_wlc_dest_overhead(struct ip_vs_dest *dest)
 {
 	/*
 	 * We think the overhead of processing active connections is 256
 	 * times higher than that of inactive connections in average. (This
 	 * 256 times might not be accurate, we will change it later) We
 	 * use the following formula to estimate the overhead now:
-	 *		  dest->activeconns*256 + dest->inactconns
+	 *                dest->activeconns*256 + dest->inactconns
 	 */
 	return (atomic_read(&dest->activeconns) << 8) +
-		atomic_read(&dest->inactconns);
+	    atomic_read(&dest->inactconns);
 }
 
-
 /*
  *	Weighted Least Connection scheduling
  */
-static struct ip_vs_dest *
-ip_vs_wlc_schedule(struct ip_vs_service *svc, const struct sk_buff *skb)
+static struct ip_vs_dest *ip_vs_wlc_schedule(struct ip_vs_service *svc,
+					     const struct sk_buff *skb)
 {
 	struct ip_vs_dest *dest, *least;
 	unsigned int loh, doh;
@@ -56,11 +53,11 @@ ip_vs_wlc_schedule(struct ip_vs_service 
 
 	/*
 	 * We calculate the load of each dest server as follows:
-	 *		  (dest overhead) / dest->weight
+	 *                (dest overhead) / dest->weight
 	 *
 	 * Remember -- no floats in kernel mode!!!
 	 * The comparison of h1*w2 > h2*w1 is equivalent to that of
-	 *		  h1/w1 > h2/w2
+	 *                h1/w1 > h2/w2
 	 * if every weight is larger than zero.
 	 *
 	 * The server with weight=0 is quiesced and will not receive any
@@ -81,7 +78,7 @@ ip_vs_wlc_schedule(struct ip_vs_service 
 	/*
 	 *    Find the destination with the least load.
 	 */
-  nextstage:
+      nextstage:
 	list_for_each_entry_continue(dest, &svc->destinations, n_list) {
 		if (dest->flags & IP_VS_DEST_F_OVERLOAD)
 			continue;
@@ -103,17 +100,14 @@ ip_vs_wlc_schedule(struct ip_vs_service 
 	return least;
 }
 
-
-static struct ip_vs_scheduler ip_vs_wlc_scheduler =
-{
-	.name =			"wlc",
-	.refcnt =		ATOMIC_INIT(0),
-	.module =		THIS_MODULE,
-	.n_list =		LIST_HEAD_INIT(ip_vs_wlc_scheduler.n_list),
-	.schedule =		ip_vs_wlc_schedule,
+static struct ip_vs_scheduler ip_vs_wlc_scheduler = {
+	.name = "wlc",
+	.refcnt = ATOMIC_INIT(0),
+	.module = THIS_MODULE,
+	.n_list = LIST_HEAD_INIT(ip_vs_wlc_scheduler.n_list),
+	.schedule = ip_vs_wlc_schedule,
 };
 
-
 static int __init ip_vs_wlc_init(void)
 {
 	return register_ip_vs_scheduler(&ip_vs_wlc_scheduler);
diff -uprN linux-2.6.32-220.23.1.el6.x86_64.bak/net/netfilter/ipvs/ip_vs_wrr.c linux-2.6.32-220.23.1.el6.x86_64/net/netfilter/ipvs/ip_vs_wrr.c
--- linux-2.6.32-220.23.1.el6.x86_64.bak/net/netfilter/ipvs/ip_vs_wrr.c	2009-12-03 11:51:21.000000000 +0800
+++ linux-2.6.32-220.23.1.el6.x86_64/net/netfilter/ipvs/ip_vs_wrr.c	2012-10-11 15:01:03.000000000 +0800
@@ -37,7 +37,6 @@ struct ip_vs_wrr_mark {
 	int di;			/* decreasing interval */
 };
 
-
 /*
  *    Get the gcd of server weights
  */
@@ -70,7 +69,6 @@ static int ip_vs_wrr_gcd_weight(struct i
 	return g ? g : 1;
 }
 
-
 /*
  *    Get the maximum weight of the service destinations.
  */
@@ -88,7 +86,6 @@ static int ip_vs_wrr_max_weight(struct i
 	return weight;
 }
 
-
 static int ip_vs_wrr_init_svc(struct ip_vs_service *svc)
 {
 	struct ip_vs_wrr_mark *mark;
@@ -110,7 +107,6 @@ static int ip_vs_wrr_init_svc(struct ip_
 	return 0;
 }
 
-
 static int ip_vs_wrr_done_svc(struct ip_vs_service *svc)
 {
 	/*
@@ -121,7 +117,6 @@ static int ip_vs_wrr_done_svc(struct ip_
 	return 0;
 }
 
-
 static int ip_vs_wrr_update_svc(struct ip_vs_service *svc)
 {
 	struct ip_vs_wrr_mark *mark = svc->sched_data;
@@ -134,12 +129,11 @@ static int ip_vs_wrr_update_svc(struct i
 	return 0;
 }
 
-
 /*
  *    Weighted Round-Robin Scheduling
  */
-static struct ip_vs_dest *
-ip_vs_wrr_schedule(struct ip_vs_service *svc, const struct sk_buff *skb)
+static struct ip_vs_dest *ip_vs_wrr_schedule(struct ip_vs_service *svc,
+					     const struct sk_buff *skb)
 {
 	struct ip_vs_dest *dest;
 	struct ip_vs_wrr_mark *mark = svc->sched_data;
@@ -207,29 +201,27 @@ ip_vs_wrr_schedule(struct ip_vs_service 
 		      "activeconns %d refcnt %d weight %d\n",
 		      IP_VS_DBG_ADDR(svc->af, &dest->addr), ntohs(dest->port),
 		      atomic_read(&dest->activeconns),
-		      atomic_read(&dest->refcnt),
-		      atomic_read(&dest->weight));
+		      atomic_read(&dest->refcnt), atomic_read(&dest->weight));
 
-  out:
+      out:
 	write_unlock(&svc->sched_lock);
 	return dest;
 }
 
-
 static struct ip_vs_scheduler ip_vs_wrr_scheduler = {
-	.name =			"wrr",
-	.refcnt =		ATOMIC_INIT(0),
-	.module =		THIS_MODULE,
-	.n_list =		LIST_HEAD_INIT(ip_vs_wrr_scheduler.n_list),
-	.init_service =		ip_vs_wrr_init_svc,
-	.done_service =		ip_vs_wrr_done_svc,
-	.update_service =	ip_vs_wrr_update_svc,
-	.schedule =		ip_vs_wrr_schedule,
+	.name = "wrr",
+	.refcnt = ATOMIC_INIT(0),
+	.module = THIS_MODULE,
+	.n_list = LIST_HEAD_INIT(ip_vs_wrr_scheduler.n_list),
+	.init_service = ip_vs_wrr_init_svc,
+	.done_service = ip_vs_wrr_done_svc,
+	.update_service = ip_vs_wrr_update_svc,
+	.schedule = ip_vs_wrr_schedule,
 };
 
 static int __init ip_vs_wrr_init(void)
 {
-	return register_ip_vs_scheduler(&ip_vs_wrr_scheduler) ;
+	return register_ip_vs_scheduler(&ip_vs_wrr_scheduler);
 }
 
 static void __exit ip_vs_wrr_cleanup(void)
diff -uprN linux-2.6.32-220.23.1.el6.x86_64.bak/net/netfilter/ipvs/ip_vs_xmit.c linux-2.6.32-220.23.1.el6.x86_64/net/netfilter/ipvs/ip_vs_xmit.c
--- linux-2.6.32-220.23.1.el6.x86_64.bak/net/netfilter/ipvs/ip_vs_xmit.c	2009-12-03 11:51:21.000000000 +0800
+++ linux-2.6.32-220.23.1.el6.x86_64/net/netfilter/ipvs/ip_vs_xmit.c	2013-03-07 17:32:24.000000000 +0800
@@ -10,6 +10,11 @@
  *              2 of the License, or (at your option) any later version.
  *
  * Changes:
+ *	Yi Yang      <specific@gmail.com>
+ *	Shunmin Zhu  <jianghe.zsm@taobao.com>
+ *	Jiaming Wu   <pukong.wjm@taobao.com>  support FULLNAT
+ *
+ *	Yu Bo        <yubo@xiaomi.com>
  *
  */
 
@@ -17,21 +22,21 @@
 #define pr_fmt(fmt) KMSG_COMPONENT ": " fmt
 
 #include <linux/kernel.h>
-#include <linux/tcp.h>                  /* for tcphdr */
+#include <linux/tcp.h>		/* for tcphdr */
 #include <net/ip.h>
-#include <net/tcp.h>                    /* for csum_tcpudp_magic */
+#include <net/tcp.h>		/* for csum_tcpudp_magic */
 #include <net/udp.h>
-#include <net/icmp.h>                   /* for icmp_send */
-#include <net/route.h>                  /* for ip_route_output */
+#include <net/icmp.h>		/* for icmp_send */
+#include <net/route.h>		/* for ip_route_output */
 #include <net/ipv6.h>
 #include <net/ip6_route.h>
 #include <linux/icmpv6.h>
 #include <linux/netfilter.h>
 #include <linux/netfilter_ipv4.h>
+#include <linux/netfilter_ipv6.h>
 
 #include <net/ip_vs.h>
 
-
 /*
  *      Destination cache to speed up outgoing route lookup
  */
@@ -46,8 +51,8 @@ __ip_vs_dst_set(struct ip_vs_dest *dest,
 	dst_release(old_dst);
 }
 
-static inline struct dst_entry *
-__ip_vs_dst_check(struct ip_vs_dest *dest, u32 rtos, u32 cookie)
+static inline struct dst_entry *__ip_vs_dst_check(struct ip_vs_dest *dest,
+						  u32 rtos, u32 cookie)
 {
 	struct dst_entry *dst = dest->dst_cache;
 
@@ -64,29 +69,29 @@ __ip_vs_dst_check(struct ip_vs_dest *des
 	return dst;
 }
 
-static struct rtable *
-__ip_vs_get_out_rt(struct ip_vs_conn *cp, u32 rtos)
+static struct rtable *__ip_vs_get_out_rt(struct ip_vs_conn *cp, u32 rtos)
 {
-	struct rtable *rt;			/* Route to the other host */
+	struct rtable *rt;	/* Route to the other host */
 	struct ip_vs_dest *dest = cp->dest;
 
-	if (dest) {
+	if (dest && dest->addr.ip != IP_VS_DSNAT_RS_ADDR) {
 		spin_lock(&dest->dst_lock);
 		if (!(rt = (struct rtable *)
 		      __ip_vs_dst_check(dest, rtos, 0))) {
 			struct flowi fl = {
 				.oif = 0,
 				.nl_u = {
-					.ip4_u = {
-						.daddr = dest->addr.ip,
-						.saddr = 0,
-						.tos = rtos, } },
+					 .ip4_u = {
+						   .daddr = dest->addr.ip,
+						   .saddr = 0,
+						   .tos = rtos,}},
 			};
 
 			if (ip_route_output_key(&init_net, &rt, &fl)) {
 				spin_unlock(&dest->dst_lock);
-				IP_VS_DBG_RL("ip_route_output error, dest: %pI4\n",
-					     &dest->addr.ip);
+				IP_VS_DBG_RL
+				    ("ip_route_output error, dest: %pI4\n",
+				     &dest->addr.ip);
 				return NULL;
 			}
 			__ip_vs_dst_set(dest, rtos, dst_clone(&rt->u.dst));
@@ -99,10 +104,10 @@ __ip_vs_get_out_rt(struct ip_vs_conn *cp
 		struct flowi fl = {
 			.oif = 0,
 			.nl_u = {
-				.ip4_u = {
-					.daddr = cp->daddr.ip,
-					.saddr = 0,
-					.tos = rtos, } },
+				 .ip4_u = {
+					   .daddr = cp->daddr.ip,
+					   .saddr = 0,
+					   .tos = rtos,}},
 		};
 
 		if (ip_route_output_key(&init_net, &rt, &fl)) {
@@ -115,11 +120,31 @@ __ip_vs_get_out_rt(struct ip_vs_conn *cp
 	return rt;
 }
 
+struct rtable *ip_vs_get_rt(union nf_inet_addr *addr, u32 rtos)
+{
+	struct rtable *rt;	/* Route to the other host */
+
+	struct flowi fl = {
+		.oif = 0,
+		.nl_u = {
+			 .ip4_u = {
+				   .daddr = addr->ip,
+				   .saddr = 0,
+				   .tos = rtos,}},
+	};
+
+	if (ip_route_output_key(&init_net, &rt, &fl)) {
+		IP_VS_DBG_RL("ip_route_output error, dest: %pI4\n", &addr->ip);
+		return NULL;
+	}
+
+	return rt;
+}
+
 #ifdef CONFIG_IP_VS_IPV6
-static struct rt6_info *
-__ip_vs_get_out_rt_v6(struct ip_vs_conn *cp)
+static struct rt6_info *__ip_vs_get_out_rt_v6(struct ip_vs_conn *cp)
 {
-	struct rt6_info *rt;			/* Route to the other host */
+	struct rt6_info *rt;	/* Route to the other host */
 	struct ip_vs_dest *dest = cp->dest;
 
 	if (dest) {
@@ -129,22 +154,23 @@ __ip_vs_get_out_rt_v6(struct ip_vs_conn 
 			struct flowi fl = {
 				.oif = 0,
 				.nl_u = {
-					.ip6_u = {
-						.daddr = dest->addr.in6,
-						.saddr = {
-							.s6_addr32 =
-								{ 0, 0, 0, 0 },
-						},
-					},
-				},
+					 .ip6_u = {
+						   .daddr = dest->addr.in6,
+						   .saddr = {
+							     .s6_addr32 =
+							     {0, 0, 0, 0},
+							     },
+						   },
+					 },
 			};
 
 			rt = (struct rt6_info *)ip6_route_output(&init_net,
 								 NULL, &fl);
 			if (!rt) {
 				spin_unlock(&dest->dst_lock);
-				IP_VS_DBG_RL("ip6_route_output error, dest: %pI6\n",
-					     &dest->addr.in6);
+				IP_VS_DBG_RL
+				    ("ip6_route_output error, dest: %pI6\n",
+				     &dest->addr.in6);
 				return NULL;
 			}
 			__ip_vs_dst_set(dest, 0, dst_clone(&rt->u.dst));
@@ -157,13 +183,13 @@ __ip_vs_get_out_rt_v6(struct ip_vs_conn 
 		struct flowi fl = {
 			.oif = 0,
 			.nl_u = {
-				.ip6_u = {
-					.daddr = cp->daddr.in6,
-					.saddr = {
-						.s6_addr32 = { 0, 0, 0, 0 },
-					},
-				},
-			},
+				 .ip6_u = {
+					   .daddr = cp->daddr.in6,
+					   .saddr = {
+						     .s6_addr32 = {0, 0, 0, 0},
+						     },
+					   },
+				 },
 		};
 
 		rt = (struct rt6_info *)ip6_route_output(&init_net, NULL, &fl);
@@ -176,14 +202,38 @@ __ip_vs_get_out_rt_v6(struct ip_vs_conn 
 
 	return rt;
 }
-#endif
 
+struct rt6_info *ip_vs_get_rt_v6(union nf_inet_addr *addr)
+{
+	struct rt6_info *rt;	/* Route to the other host */
+
+	struct flowi fl = {
+		.oif = 0,
+		.nl_u = {
+			 .ip6_u = {
+				   .daddr = addr->in6,
+				   .saddr = {
+					     .s6_addr32 = {0, 0, 0, 0},
+					     },
+				   },
+			 },
+	};
+
+	rt = (struct rt6_info *)ip6_route_output(&init_net, NULL, &fl);
+	if (!rt) {
+		IP_VS_DBG_RL("ip6_route_output error, dest: %pI6\n",
+			     &addr->in6);
+		return NULL;
+	}
+
+	return rt;
+}
+#endif
 
 /*
  *	Release dest->dst_cache before a dest is removed
  */
-void
-ip_vs_dst_reset(struct ip_vs_dest *dest)
+void ip_vs_dst_reset(struct ip_vs_dest *dest)
 {
 	struct dst_entry *old_dst;
 
@@ -200,6 +250,477 @@ do {							\
 		(rt)->u.dst.dev, dst_output);		\
 } while (0)
 
+/*
+ * Packet has been made sufficiently writable in caller
+ * - inout: 1=in->out, 0=out->in
+ */
+static void ip_vs_nat_icmp(struct sk_buff *skb, struct ip_vs_protocol *pp,
+			   struct ip_vs_conn *cp, int inout)
+{
+	struct iphdr *iph = ip_hdr(skb);
+	unsigned int icmp_offset = iph->ihl * 4;
+	struct icmphdr *icmph = (struct icmphdr *)(skb_network_header(skb) +
+						   icmp_offset);
+	struct iphdr *ciph = (struct iphdr *)(icmph + 1);
+	__u32 fullnat = (IP_VS_FWD_METHOD(cp) == IP_VS_CONN_F_FULLNAT);
+	__u32 dsnat = (IP_VS_FWD_METHOD(cp) == IP_VS_CONN_F_DSNAT);
+
+	if (fullnat | dsnat) {
+		if (inout) {
+			iph->daddr = cp->caddr.ip;
+			ciph->saddr = cp->caddr.ip;
+		} else {
+			iph->saddr = cp->laddr.ip;
+			ciph->daddr = cp->laddr.ip;
+		}
+	}
+
+	if (inout) {
+		iph->saddr = cp->vaddr.ip;
+		ip_send_check(iph);
+		ciph->daddr = cp->vaddr.ip;
+		ip_send_check(ciph);
+	} else {
+		iph->daddr = cp->daddr.ip;
+		ip_send_check(iph);
+		ciph->saddr = cp->daddr.ip;
+		ip_send_check(ciph);
+	}
+
+	/* the TCP/UDP port */
+	if (IPPROTO_TCP == ciph->protocol || IPPROTO_UDP == ciph->protocol) {
+		__be16 *ports = (void *)ciph + ciph->ihl * 4;
+
+		if (fullnat | dsnat) {
+			if (inout)
+				ports[0] = cp->cport;
+			else
+				ports[1] = cp->lport;
+		}
+
+		if (inout)
+			ports[1] = cp->vport;
+		else
+			ports[0] = cp->dport;
+	}
+
+	/* And finally the ICMP checksum */
+	icmph->checksum = 0;
+	icmph->checksum = ip_vs_checksum_complete(skb, icmp_offset);
+	skb->ip_summed = CHECKSUM_UNNECESSARY;
+
+	if (inout)
+		IP_VS_DBG_PKT(11, pp, skb, (void *)ciph - (void *)iph,
+			      "Forwarding altered outgoing ICMP");
+	else
+		IP_VS_DBG_PKT(11, pp, skb, (void *)ciph - (void *)iph,
+			      "Forwarding altered incoming ICMP");
+}
+
+#ifdef CONFIG_IP_VS_IPV6
+static void ip_vs_nat_icmp_v6(struct sk_buff *skb, struct ip_vs_protocol *pp,
+			      struct ip_vs_conn *cp, int inout)
+{
+	struct ipv6hdr *iph = ipv6_hdr(skb);
+	unsigned int icmp_offset = sizeof(struct ipv6hdr);
+	struct icmp6hdr *icmph = (struct icmp6hdr *)(skb_network_header(skb) +
+						     icmp_offset);
+	struct ipv6hdr *ciph = (struct ipv6hdr *)(icmph + 1);
+	__u32 fullnat = (IP_VS_FWD_METHOD(cp) == IP_VS_CONN_F_FULLNAT);
+
+	if (fullnat) {
+		if (inout) {
+			iph->daddr = cp->caddr.in6;
+			ciph->saddr = cp->caddr.in6;
+		} else {
+			iph->saddr = cp->laddr.in6;
+			ciph->daddr = cp->laddr.in6;
+		}
+	}
+
+	if (inout) {
+		iph->saddr = cp->vaddr.in6;
+		ciph->daddr = cp->vaddr.in6;
+	} else {
+		iph->daddr = cp->daddr.in6;
+		ciph->saddr = cp->daddr.in6;
+	}
+
+	/* the TCP/UDP port */
+	if (IPPROTO_TCP == ciph->nexthdr || IPPROTO_UDP == ciph->nexthdr) {
+		__be16 *ports = (void *)ciph + sizeof(struct ipv6hdr);
+
+		if (fullnat) {
+			if (inout)
+				ports[0] = cp->cport;
+			else
+				ports[1] = cp->lport;
+		}
+
+		if (inout)
+			ports[1] = cp->vport;
+		else
+			ports[0] = cp->dport;
+	}
+
+	/* And finally the ICMP checksum */
+	icmph->icmp6_cksum = 0;
+	/* TODO IPv6: is this correct for ICMPv6? */
+	ip_vs_checksum_complete(skb, icmp_offset);
+	skb->ip_summed = CHECKSUM_UNNECESSARY;
+
+	if (inout)
+		IP_VS_DBG_PKT(11, pp, skb, (void *)ciph - (void *)iph,
+			      "Forwarding altered outgoing ICMPv6");
+	else
+		IP_VS_DBG_PKT(11, pp, skb, (void *)ciph - (void *)iph,
+			      "Forwarding altered incoming ICMPv6");
+}
+#endif
+
+/* Response transmit icmp to client
+ * Used for NAT/LOCAL.
+ */
+int
+ip_vs_normal_response_icmp_xmit(struct sk_buff *skb, struct ip_vs_protocol *pp,
+				struct ip_vs_conn *cp, int offset)
+{
+	unsigned int verdict = NF_DROP;
+
+	if (!skb_make_writable(skb, offset))
+		goto out;
+
+	ip_vs_nat_icmp(skb, pp, cp, 1);
+
+	skb->ipvs_property = 1;
+	verdict = NF_ACCEPT;
+
+      out:
+	return verdict;
+}
+
+#ifdef CONFIG_IP_VS_IPV6
+
+int
+ip_vs_normal_response_icmp_xmit_v6(struct sk_buff *skb,
+				   struct ip_vs_protocol *pp,
+				   struct ip_vs_conn *cp, int offset)
+{
+	unsigned int verdict = NF_DROP;
+
+	if (!skb_make_writable(skb, offset))
+		goto out;
+
+	ip_vs_nat_icmp_v6(skb, pp, cp, 1);
+
+	skb->ipvs_property = 1;
+	verdict = NF_ACCEPT;
+
+      out:
+	return verdict;
+}
+
+#endif
+
+/* Response transmit icmp to client
+ * Used for NAT / local client / FULLNAT.
+ */
+int
+ip_vs_fnat_response_icmp_xmit(struct sk_buff *skb, struct ip_vs_protocol *pp,
+			      struct ip_vs_conn *cp, int offset)
+{
+	struct rtable *rt;	/* Route to the other host */
+	int mtu;
+	struct iphdr *iph = ip_hdr(skb);
+
+	/* lookup route table */
+	if (!(rt = ip_vs_get_rt(&cp->caddr, RT_TOS(iph->tos))))
+		goto tx_error_icmp;
+
+	/* MTU checking */
+	mtu = dst_mtu(&rt->u.dst);
+	if ((skb->len > mtu) && (iph->frag_off & htons(IP_DF))) {
+		ip_rt_put(rt);
+		IP_VS_DBG_RL_PKT(0, pp, skb, 0,
+				 "fnat_response_icmp(): frag needed for");
+		goto tx_error;
+	}
+
+	/* copy-on-write the packet before mangling it */
+	if (!skb_make_writable(skb, offset))
+		goto tx_error_put;
+
+	if (skb_cow(skb, rt->u.dst.dev->hard_header_len))
+		goto tx_error_put;
+
+	/* drop old route */
+	skb_dst_drop(skb);
+	skb_dst_set(skb, &rt->u.dst);
+
+	ip_vs_nat_icmp(skb, pp, cp, 1);
+
+	/* Another hack: avoid icmp_send in ip_fragment */
+	skb->local_df = 1;
+
+	IP_VS_XMIT(PF_INET, skb, rt);
+
+	return NF_STOLEN;
+
+      tx_error_icmp:
+	dst_link_failure(skb);
+      tx_error:
+	kfree_skb(skb);
+	return NF_STOLEN;
+      tx_error_put:
+	ip_rt_put(rt);
+	goto tx_error;
+}
+
+#ifdef CONFIG_IP_VS_IPV6
+
+int
+ip_vs_fnat_response_icmp_xmit_v6(struct sk_buff *skb, struct ip_vs_protocol *pp,
+				 struct ip_vs_conn *cp, int offset)
+{
+	struct rt6_info *rt;	/* Route to the other host */
+	int mtu;
+
+	/* lookup route table */
+	if (!(rt = ip_vs_get_rt_v6(&cp->caddr)))
+		goto tx_error_icmp;
+
+	/* MTU checking */
+	mtu = dst_mtu(&rt->u.dst);
+	if (skb->len > mtu) {
+		dst_release(&rt->u.dst);
+		IP_VS_DBG_RL("%s(): frag needed\n", __func__);
+		goto tx_error;
+	}
+
+	/* copy-on-write the packet before mangling it */
+	if (!skb_make_writable(skb, offset))
+		goto tx_error_put;
+
+	if (skb_cow(skb, rt->u.dst.dev->hard_header_len))
+		goto tx_error_put;
+
+	/* drop old route */
+	skb_dst_drop(skb);
+	skb_dst_set(skb, &rt->u.dst);
+
+	ip_vs_nat_icmp_v6(skb, pp, cp, 1);
+
+	/* Another hack: avoid icmp_send in ip_fragment */
+	skb->local_df = 1;
+
+	IP_VS_XMIT(PF_INET6, skb, rt);
+
+	return NF_STOLEN;
+
+      tx_error_icmp:
+	dst_link_failure(skb);
+      tx_error:
+	kfree_skb(skb);
+	return NF_STOLEN;
+      tx_error_put:
+	dst_release(&rt->u.dst);
+	goto tx_error;
+}
+
+#endif
+
+/* Response transmit to client
+ * Used for NAT/Local.
+ */
+int
+ip_vs_normal_response_xmit(struct sk_buff *skb, struct ip_vs_protocol *pp,
+			   struct ip_vs_conn *cp, int ihl)
+{
+	/* copy-on-write the packet before mangling it */
+	if (!skb_make_writable(skb, ihl))
+		goto drop;
+
+	/* mangle the packet */
+	if (pp->snat_handler && !pp->snat_handler(skb, pp, cp))
+		goto drop;
+
+	ip_hdr(skb)->saddr = cp->vaddr.ip;
+	ip_send_check(ip_hdr(skb));
+
+	/* For policy routing, packets originating from this
+	 * machine itself may be routed differently to packets
+	 * passing through.  We want this packet to be routed as
+	 * if it came from this machine itself.  So re-compute
+	 * the routing information.
+	 */
+	if (ip_route_me_harder(skb, RTN_LOCAL) != 0)
+		goto drop;
+
+	skb->ipvs_property = 1;
+
+	return NF_ACCEPT;
+
+      drop:
+	kfree_skb(skb);
+	return NF_STOLEN;
+}
+
+#ifdef CONFIG_IP_VS_IPV6
+
+int
+ip_vs_normal_response_xmit_v6(struct sk_buff *skb, struct ip_vs_protocol *pp,
+			      struct ip_vs_conn *cp, int ihl)
+{
+	/* copy-on-write the packet before mangling it */
+	if (!skb_make_writable(skb, ihl))
+		goto drop;
+
+	/* mangle the packet */
+	if (pp->snat_handler && !pp->snat_handler(skb, pp, cp))
+		goto drop;
+
+	ipv6_hdr(skb)->saddr = cp->vaddr.in6;
+
+	/* For policy routing, packets originating from this
+	 * machine itself may be routed differently to packets
+	 * passing through.  We want this packet to be routed as
+	 * if it came from this machine itself.  So re-compute
+	 * the routing information.
+	 */
+	if (ip6_route_me_harder(skb) != 0)
+		goto drop;
+
+	skb->ipvs_property = 1;
+
+	return NF_ACCEPT;
+
+      drop:
+	kfree_skb(skb);
+	return NF_STOLEN;
+}
+
+#endif
+
+/* Response transmit to client
+ * Used for FULLNAT.
+ */
+int
+ip_vs_fnat_response_xmit(struct sk_buff *skb, struct ip_vs_protocol *pp,
+			 struct ip_vs_conn *cp, int ihl)
+{
+	struct rtable *rt;	/* Route to the other host */
+	int mtu;
+	struct iphdr *iph = ip_hdr(skb);
+
+	/* lookup route table */
+	if (!(rt = ip_vs_get_rt(&cp->caddr, RT_TOS(iph->tos))))
+		goto tx_error_icmp;
+
+	/* MTU checking */
+	mtu = dst_mtu(&rt->u.dst);
+	if ((skb->len > mtu) && (iph->frag_off & htons(IP_DF))) {
+		ip_rt_put(rt);
+		icmp_send(skb, ICMP_DEST_UNREACH, ICMP_FRAG_NEEDED, htonl(mtu));
+		IP_VS_DBG_RL_PKT(0, pp, skb, 0,
+				 "handle_fnat_response(): frag needed for");
+		goto tx_error;
+	}
+
+	/* copy-on-write the packet before mangling it */
+	if (!skb_make_writable(skb, ihl))
+		goto tx_error_put;
+
+	if (skb_cow(skb, rt->u.dst.dev->hard_header_len))
+		goto tx_error_put;
+
+	/* drop old route */
+	skb_dst_drop(skb);
+	skb_dst_set(skb, &rt->u.dst);
+
+	/* mangle the packet */
+	if (pp->fnat_out_handler && !pp->fnat_out_handler(skb, pp, cp))
+		goto tx_error;
+
+	ip_hdr(skb)->saddr = cp->vaddr.ip;
+	ip_hdr(skb)->daddr = cp->caddr.ip;
+	ip_send_check(ip_hdr(skb));
+
+	/* Another hack: avoid icmp_send in ip_fragment */
+	skb->local_df = 1;
+
+	IP_VS_XMIT(PF_INET, skb, rt);
+
+	return NF_STOLEN;
+
+      tx_error_icmp:
+	dst_link_failure(skb);
+      tx_error:
+	kfree_skb(skb);
+	return NF_STOLEN;
+      tx_error_put:
+	ip_rt_put(rt);
+	goto tx_error;
+}
+
+#ifdef CONFIG_IP_VS_IPV6
+
+int
+ip_vs_fnat_response_xmit_v6(struct sk_buff *skb, struct ip_vs_protocol *pp,
+			    struct ip_vs_conn *cp, int ihl)
+{
+	struct rt6_info *rt;	/* Route to the other host */
+	int mtu;
+
+	/* lookup route table */
+	if (!(rt = ip_vs_get_rt_v6(&cp->caddr)))
+		goto tx_error_icmp;
+
+	/* MTU checking */
+	mtu = dst_mtu(&rt->u.dst);
+	if (skb->len > mtu) {
+		dst_release(&rt->u.dst);
+		icmpv6_send(skb, ICMPV6_PKT_TOOBIG, 0, mtu, skb->dev);
+		IP_VS_DBG_RL_PKT(0, pp, skb, 0,
+				 "handle_fnat_response_v6(): frag needed for");
+		goto tx_error;
+	}
+
+	/* copy-on-write the packet before mangling it */
+	if (!skb_make_writable(skb, ihl))
+		goto tx_error_put;
+
+	if (skb_cow(skb, rt->u.dst.dev->hard_header_len))
+		goto tx_error_put;
+
+	/* drop old route */
+	skb_dst_drop(skb);
+	skb_dst_set(skb, &rt->u.dst);
+
+	/* mangle the packet */
+	if (pp->fnat_out_handler && !pp->fnat_out_handler(skb, pp, cp))
+		goto tx_error;
+
+	ipv6_hdr(skb)->saddr = cp->vaddr.in6;
+	ipv6_hdr(skb)->daddr = cp->caddr.in6;
+
+	/* Another hack: avoid icmp_send in ip_fragment */
+	skb->local_df = 1;
+
+	IP_VS_XMIT(PF_INET6, skb, rt);
+
+	return NF_STOLEN;
+
+      tx_error_icmp:
+	dst_link_failure(skb);
+      tx_error:
+	kfree_skb(skb);
+	return NF_STOLEN;
+      tx_error_put:
+	dst_release(&rt->u.dst);
+	goto tx_error;
+}
+
+#endif
 
 /*
  *      NULL transmitter (do nothing except return NF_ACCEPT)
@@ -212,7 +733,6 @@ ip_vs_null_xmit(struct sk_buff *skb, str
 	return NF_ACCEPT;
 }
 
-
 /*
  *      Bypass transmitter
  *      Let packets bypass the destination when the destination is not
@@ -222,17 +742,17 @@ int
 ip_vs_bypass_xmit(struct sk_buff *skb, struct ip_vs_conn *cp,
 		  struct ip_vs_protocol *pp)
 {
-	struct rtable *rt;			/* Route to the other host */
-	struct iphdr  *iph = ip_hdr(skb);
-	u8     tos = iph->tos;
-	int    mtu;
+	struct rtable *rt;	/* Route to the other host */
+	struct iphdr *iph = ip_hdr(skb);
+	u8 tos = iph->tos;
+	int mtu;
 	struct flowi fl = {
 		.oif = 0,
 		.nl_u = {
-			.ip4_u = {
-				.daddr = iph->daddr,
-				.saddr = 0,
-				.tos = RT_TOS(tos), } },
+			 .ip4_u = {
+				   .daddr = iph->daddr,
+				   .saddr = 0,
+				   .tos = RT_TOS(tos),}},
 	};
 
 	EnterFunction(10);
@@ -247,7 +767,7 @@ ip_vs_bypass_xmit(struct sk_buff *skb, s
 	mtu = dst_mtu(&rt->u.dst);
 	if ((skb->len > mtu) && (iph->frag_off & htons(IP_DF))) {
 		ip_rt_put(rt);
-		icmp_send(skb, ICMP_DEST_UNREACH,ICMP_FRAG_NEEDED, htonl(mtu));
+		icmp_send(skb, ICMP_DEST_UNREACH, ICMP_FRAG_NEEDED, htonl(mtu));
 		IP_VS_DBG_RL("%s(): frag needed\n", __func__);
 		goto tx_error;
 	}
@@ -274,9 +794,9 @@ ip_vs_bypass_xmit(struct sk_buff *skb, s
 	LeaveFunction(10);
 	return NF_STOLEN;
 
- tx_error_icmp:
+      tx_error_icmp:
 	dst_link_failure(skb);
- tx_error:
+      tx_error:
 	kfree_skb(skb);
 	LeaveFunction(10);
 	return NF_STOLEN;
@@ -287,15 +807,15 @@ int
 ip_vs_bypass_xmit_v6(struct sk_buff *skb, struct ip_vs_conn *cp,
 		     struct ip_vs_protocol *pp)
 {
-	struct rt6_info *rt;			/* Route to the other host */
-	struct ipv6hdr  *iph = ipv6_hdr(skb);
-	int    mtu;
+	struct rt6_info *rt;	/* Route to the other host */
+	struct ipv6hdr *iph = ipv6_hdr(skb);
+	int mtu;
 	struct flowi fl = {
 		.oif = 0,
 		.nl_u = {
-			.ip6_u = {
-				.daddr = iph->daddr,
-				.saddr = { .s6_addr32 = {0, 0, 0, 0} }, } },
+			 .ip6_u = {
+				   .daddr = iph->daddr,
+				   .saddr = {.s6_addr32 = {0, 0, 0, 0}},}},
 	};
 
 	EnterFunction(10);
@@ -338,9 +858,9 @@ ip_vs_bypass_xmit_v6(struct sk_buff *skb
 	LeaveFunction(10);
 	return NF_STOLEN;
 
- tx_error_icmp:
+      tx_error_icmp:
 	dst_link_failure(skb);
- tx_error:
+      tx_error:
 	kfree_skb(skb);
 	LeaveFunction(10);
 	return NF_STOLEN;
@@ -355,7 +875,7 @@ int
 ip_vs_nat_xmit(struct sk_buff *skb, struct ip_vs_conn *cp,
 	       struct ip_vs_protocol *pp)
 {
-	struct rtable *rt;		/* Route to the other host */
+	struct rtable *rt;	/* Route to the other host */
 	int mtu;
 	struct iphdr *iph = ip_hdr(skb);
 
@@ -364,7 +884,7 @@ ip_vs_nat_xmit(struct sk_buff *skb, stru
 	/* check if it is a connection of no-client-port */
 	if (unlikely(cp->flags & IP_VS_CONN_F_NO_CPORT)) {
 		__be16 _pt, *p;
-		p = skb_header_pointer(skb, iph->ihl*4, sizeof(_pt), &_pt);
+		p = skb_header_pointer(skb, iph->ihl * 4, sizeof(_pt), &_pt);
 		if (p == NULL)
 			goto tx_error;
 		ip_vs_conn_fill_cport(cp, *p);
@@ -378,8 +898,9 @@ ip_vs_nat_xmit(struct sk_buff *skb, stru
 	mtu = dst_mtu(&rt->u.dst);
 	if ((skb->len > mtu) && (iph->frag_off & htons(IP_DF))) {
 		ip_rt_put(rt);
-		icmp_send(skb, ICMP_DEST_UNREACH,ICMP_FRAG_NEEDED, htonl(mtu));
-		IP_VS_DBG_RL_PKT(0, pp, skb, 0, "ip_vs_nat_xmit(): frag needed for");
+		icmp_send(skb, ICMP_DEST_UNREACH, ICMP_FRAG_NEEDED, htonl(mtu));
+		IP_VS_DBG_RL_PKT(0, pp, skb, 0,
+				 "ip_vs_nat_xmit(): frag needed for");
 		goto tx_error;
 	}
 
@@ -414,13 +935,13 @@ ip_vs_nat_xmit(struct sk_buff *skb, stru
 	LeaveFunction(10);
 	return NF_STOLEN;
 
-  tx_error_icmp:
+      tx_error_icmp:
 	dst_link_failure(skb);
-  tx_error:
+      tx_error:
 	LeaveFunction(10);
 	kfree_skb(skb);
 	return NF_STOLEN;
-  tx_error_put:
+      tx_error_put:
 	ip_rt_put(rt);
 	goto tx_error;
 }
@@ -430,7 +951,7 @@ int
 ip_vs_nat_xmit_v6(struct sk_buff *skb, struct ip_vs_conn *cp,
 		  struct ip_vs_protocol *pp)
 {
-	struct rt6_info *rt;		/* Route to the other host */
+	struct rt6_info *rt;	/* Route to the other host */
 	int mtu;
 
 	EnterFunction(10);
@@ -490,18 +1011,175 @@ ip_vs_nat_xmit_v6(struct sk_buff *skb, s
 	LeaveFunction(10);
 	return NF_STOLEN;
 
-tx_error_icmp:
+      tx_error_icmp:
 	dst_link_failure(skb);
-tx_error:
+      tx_error:
 	LeaveFunction(10);
 	kfree_skb(skb);
 	return NF_STOLEN;
-tx_error_put:
+      tx_error_put:
 	dst_release(&rt->u.dst);
 	goto tx_error;
 }
 #endif
 
+/*
+ *      FULLNAT transmitter (only for outside-to-inside fullnat forwarding)
+ *      Not used for related ICMP
+ */
+int
+ip_vs_fnat_xmit(struct sk_buff *skb, struct ip_vs_conn *cp,
+		struct ip_vs_protocol *pp)
+{
+	struct rtable *rt;	/* Route to the other host */
+	int mtu;
+	struct iphdr *iph = ip_hdr(skb);
+
+	EnterFunction(10);
+
+	/* check if it is a connection of no-client-port */
+	if (unlikely(cp->flags & IP_VS_CONN_F_NO_CPORT)) {
+		__be16 _pt, *p;
+		p = skb_header_pointer(skb, iph->ihl * 4, sizeof(_pt), &_pt);
+		if (p == NULL)
+			goto tx_error;
+		ip_vs_conn_fill_cport(cp, *p);
+		IP_VS_DBG(10, "filled cport=%d\n", ntohs(*p));
+	}
+
+	if (!(rt = __ip_vs_get_out_rt(cp, RT_TOS(iph->tos))))
+		goto tx_error_icmp;
+
+	/* MTU checking */
+	mtu = dst_mtu(&rt->u.dst);
+	if ((skb->len > mtu) && (iph->frag_off & htons(IP_DF))) {
+		ip_rt_put(rt);
+		icmp_send(skb, ICMP_DEST_UNREACH, ICMP_FRAG_NEEDED, htonl(mtu));
+		IP_VS_DBG_RL_PKT(0, pp, skb, 0,
+				 "ip_vs_nat_xmit(): frag needed for");
+		goto tx_error;
+	}
+
+	/* copy-on-write the packet before mangling it */
+	if (!skb_make_writable(skb, sizeof(struct iphdr)))
+		goto tx_error_put;
+
+	if (skb_cow(skb, rt->u.dst.dev->hard_header_len))
+		goto tx_error_put;
+
+	/* drop old route */
+	skb_dst_drop(skb);
+	skb_dst_set(skb, &rt->u.dst);
+
+	/* mangle the packet */
+	if (pp->fnat_in_handler && !pp->fnat_in_handler(&skb, pp, cp))
+		goto tx_error;
+	ip_hdr(skb)->saddr = cp->laddr.ip;
+	ip_hdr(skb)->daddr = cp->daddr.ip;
+	ip_send_check(ip_hdr(skb));
+
+	IP_VS_DBG_PKT(10, pp, skb, 0, "After FNAT-IN");
+
+	/* FIXME: when application helper enlarges the packet and the length
+	   is larger than the MTU of outgoing device, there will be still
+	   MTU problem. */
+
+	/* Another hack: avoid icmp_send in ip_fragment */
+	skb->local_df = 1;
+
+	IP_VS_XMIT(PF_INET, skb, rt);
+
+	LeaveFunction(10);
+	return NF_STOLEN;
+
+      tx_error_icmp:
+	dst_link_failure(skb);
+      tx_error:
+	LeaveFunction(10);
+	kfree_skb(skb);
+	return NF_STOLEN;
+      tx_error_put:
+	ip_rt_put(rt);
+	goto tx_error;
+}
+
+#ifdef CONFIG_IP_VS_IPV6
+int
+ip_vs_fnat_xmit_v6(struct sk_buff *skb, struct ip_vs_conn *cp,
+		   struct ip_vs_protocol *pp)
+{
+	struct rt6_info *rt;	/* Route to the other host */
+	int mtu;
+
+	EnterFunction(10);
+
+	/* check if it is a connection of no-client-port */
+	if (unlikely(cp->flags & IP_VS_CONN_F_NO_CPORT)) {
+		__be16 _pt, *p;
+		p = skb_header_pointer(skb, sizeof(struct ipv6hdr),
+				       sizeof(_pt), &_pt);
+		if (p == NULL)
+			goto tx_error;
+		ip_vs_conn_fill_cport(cp, *p);
+		IP_VS_DBG(10, "filled cport=%d\n", ntohs(*p));
+	}
+
+	rt = __ip_vs_get_out_rt_v6(cp);
+	if (!rt)
+		goto tx_error_icmp;
+
+	/* MTU checking */
+	mtu = dst_mtu(&rt->u.dst);
+	if (skb->len > mtu) {
+		dst_release(&rt->u.dst);
+		icmpv6_send(skb, ICMPV6_PKT_TOOBIG, 0, mtu, skb->dev);
+		IP_VS_DBG_RL_PKT(0, pp, skb, 0,
+				 "ip_vs_nat_xmit_v6(): frag needed for");
+		goto tx_error;
+	}
+
+	/* copy-on-write the packet before mangling it */
+	if (!skb_make_writable(skb, sizeof(struct ipv6hdr)))
+		goto tx_error_put;
+
+	if (skb_cow(skb, rt->u.dst.dev->hard_header_len))
+		goto tx_error_put;
+
+	/* drop old route */
+	skb_dst_drop(skb);
+	skb_dst_set(skb, &rt->u.dst);
+
+	/* mangle the packet */
+	if (pp->fnat_in_handler && !pp->fnat_in_handler(&skb, pp, cp))
+		goto tx_error;
+	ipv6_hdr(skb)->saddr = cp->laddr.in6;
+	ipv6_hdr(skb)->daddr = cp->daddr.in6;
+
+	IP_VS_DBG_PKT(10, pp, skb, 0, "After FNAT-IN");
+
+	/* FIXME: when application helper enlarges the packet and the length
+	   is larger than the MTU of outgoing device, there will be still
+	   MTU problem. */
+
+	/* Another hack: avoid icmp_send in ip_fragment */
+	skb->local_df = 1;
+
+	IP_VS_XMIT(PF_INET6, skb, rt);
+
+	LeaveFunction(10);
+	return NF_STOLEN;
+
+      tx_error_icmp:
+	dst_link_failure(skb);
+      tx_error:
+	LeaveFunction(10);
+	kfree_skb(skb);
+	return NF_STOLEN;
+      tx_error_put:
+	dst_release(&rt->u.dst);
+	goto tx_error;
+}
+#endif
 
 /*
  *   IP Tunneling transmitter
@@ -526,15 +1204,15 @@ int
 ip_vs_tunnel_xmit(struct sk_buff *skb, struct ip_vs_conn *cp,
 		  struct ip_vs_protocol *pp)
 {
-	struct rtable *rt;			/* Route to the other host */
-	struct net_device *tdev;		/* Device to other host */
-	struct iphdr  *old_iph = ip_hdr(skb);
-	u8     tos = old_iph->tos;
+	struct rtable *rt;	/* Route to the other host */
+	struct net_device *tdev;	/* Device to other host */
+	struct iphdr *old_iph = ip_hdr(skb);
+	u8 tos = old_iph->tos;
 	__be16 df = old_iph->frag_off;
 	sk_buff_data_t old_transport_header = skb->transport_header;
-	struct iphdr  *iph;			/* Our new IP header */
-	unsigned int max_headroom;		/* The extra header space needed */
-	int    mtu;
+	struct iphdr *iph;	/* Our new IP header */
+	unsigned int max_headroom;	/* The extra header space needed */
+	int mtu;
 
 	EnterFunction(10);
 
@@ -563,7 +1241,7 @@ ip_vs_tunnel_xmit(struct sk_buff *skb, s
 
 	if ((old_iph->frag_off & htons(IP_DF))
 	    && mtu < ntohs(old_iph->tot_len)) {
-		icmp_send(skb, ICMP_DEST_UNREACH,ICMP_FRAG_NEEDED, htonl(mtu));
+		icmp_send(skb, ICMP_DEST_UNREACH, ICMP_FRAG_NEEDED, htonl(mtu));
 		ip_rt_put(rt);
 		IP_VS_DBG_RL("%s(): frag needed\n", __func__);
 		goto tx_error;
@@ -577,7 +1255,7 @@ ip_vs_tunnel_xmit(struct sk_buff *skb, s
 	if (skb_headroom(skb) < max_headroom
 	    || skb_cloned(skb) || skb_shared(skb)) {
 		struct sk_buff *new_skb =
-			skb_realloc_headroom(skb, max_headroom);
+		    skb_realloc_headroom(skb, max_headroom);
 		if (!new_skb) {
 			ip_rt_put(rt);
 			kfree_skb(skb);
@@ -603,17 +1281,17 @@ ip_vs_tunnel_xmit(struct sk_buff *skb, s
 	skb_dst_set(skb, &rt->u.dst);
 
 	/*
-	 *	Push down and install the IPIP header.
+	 *      Push down and install the IPIP header.
 	 */
-	iph			=	ip_hdr(skb);
-	iph->version		=	4;
-	iph->ihl		=	sizeof(struct iphdr)>>2;
-	iph->frag_off		=	df;
-	iph->protocol		=	IPPROTO_IPIP;
-	iph->tos		=	tos;
-	iph->daddr		=	rt->rt_dst;
-	iph->saddr		=	rt->rt_src;
-	iph->ttl		=	old_iph->ttl;
+	iph = ip_hdr(skb);
+	iph->version = 4;
+	iph->ihl = sizeof(struct iphdr) >> 2;
+	iph->frag_off = df;
+	iph->protocol = IPPROTO_IPIP;
+	iph->tos = tos;
+	iph->daddr = rt->rt_dst;
+	iph->saddr = rt->rt_src;
+	iph->ttl = old_iph->ttl;
 	ip_select_ident(iph, &rt->u.dst, NULL);
 
 	/* Another hack: avoid icmp_send in ip_fragment */
@@ -625,9 +1303,9 @@ ip_vs_tunnel_xmit(struct sk_buff *skb, s
 
 	return NF_STOLEN;
 
-  tx_error_icmp:
+      tx_error_icmp:
 	dst_link_failure(skb);
-  tx_error:
+      tx_error:
 	kfree_skb(skb);
 	LeaveFunction(10);
 	return NF_STOLEN;
@@ -638,13 +1316,13 @@ int
 ip_vs_tunnel_xmit_v6(struct sk_buff *skb, struct ip_vs_conn *cp,
 		     struct ip_vs_protocol *pp)
 {
-	struct rt6_info *rt;		/* Route to the other host */
+	struct rt6_info *rt;	/* Route to the other host */
 	struct net_device *tdev;	/* Device to other host */
-	struct ipv6hdr  *old_iph = ipv6_hdr(skb);
+	struct ipv6hdr *old_iph = ipv6_hdr(skb);
 	sk_buff_data_t old_transport_header = skb->transport_header;
-	struct ipv6hdr  *iph;		/* Our new IP header */
+	struct ipv6hdr *iph;	/* Our new IP header */
 	unsigned int max_headroom;	/* The extra header space needed */
-	int    mtu;
+	int mtu;
 
 	EnterFunction(10);
 
@@ -686,7 +1364,7 @@ ip_vs_tunnel_xmit_v6(struct sk_buff *skb
 	if (skb_headroom(skb) < max_headroom
 	    || skb_cloned(skb) || skb_shared(skb)) {
 		struct sk_buff *new_skb =
-			skb_realloc_headroom(skb, max_headroom);
+		    skb_realloc_headroom(skb, max_headroom);
 		if (!new_skb) {
 			dst_release(&rt->u.dst);
 			kfree_skb(skb);
@@ -709,18 +1387,18 @@ ip_vs_tunnel_xmit_v6(struct sk_buff *skb
 	skb_dst_set(skb, &rt->u.dst);
 
 	/*
-	 *	Push down and install the IPIP header.
+	 *      Push down and install the IPIP header.
 	 */
-	iph			=	ipv6_hdr(skb);
-	iph->version		=	6;
-	iph->nexthdr		=	IPPROTO_IPV6;
-	iph->payload_len	=	old_iph->payload_len;
+	iph = ipv6_hdr(skb);
+	iph->version = 6;
+	iph->nexthdr = IPPROTO_IPV6;
+	iph->payload_len = old_iph->payload_len;
 	be16_add_cpu(&iph->payload_len, sizeof(*old_iph));
-	iph->priority		=	old_iph->priority;
+	iph->priority = old_iph->priority;
 	memset(&iph->flow_lbl, 0, sizeof(iph->flow_lbl));
-	iph->daddr		=	rt->rt6i_dst.addr;
-	iph->saddr		=	cp->vaddr.in6; /* rt->rt6i_src.addr; */
-	iph->hop_limit		=	old_iph->hop_limit;
+	iph->daddr = rt->rt6i_dst.addr;
+	iph->saddr = cp->vaddr.in6;	/* rt->rt6i_src.addr; */
+	iph->hop_limit = old_iph->hop_limit;
 
 	/* Another hack: avoid icmp_send in ip_fragment */
 	skb->local_df = 1;
@@ -731,16 +1409,15 @@ ip_vs_tunnel_xmit_v6(struct sk_buff *skb
 
 	return NF_STOLEN;
 
-tx_error_icmp:
+      tx_error_icmp:
 	dst_link_failure(skb);
-tx_error:
+      tx_error:
 	kfree_skb(skb);
 	LeaveFunction(10);
 	return NF_STOLEN;
 }
 #endif
 
-
 /*
  *      Direct Routing transmitter
  *      Used for ANY protocol
@@ -749,9 +1426,9 @@ int
 ip_vs_dr_xmit(struct sk_buff *skb, struct ip_vs_conn *cp,
 	      struct ip_vs_protocol *pp)
 {
-	struct rtable *rt;			/* Route to the other host */
-	struct iphdr  *iph = ip_hdr(skb);
-	int    mtu;
+	struct rtable *rt;	/* Route to the other host */
+	struct iphdr *iph = ip_hdr(skb);
+	int mtu;
 
 	EnterFunction(10);
 
@@ -761,7 +1438,7 @@ ip_vs_dr_xmit(struct sk_buff *skb, struc
 	/* MTU checking */
 	mtu = dst_mtu(&rt->u.dst);
 	if ((iph->frag_off & htons(IP_DF)) && skb->len > mtu) {
-		icmp_send(skb, ICMP_DEST_UNREACH,ICMP_FRAG_NEEDED, htonl(mtu));
+		icmp_send(skb, ICMP_DEST_UNREACH, ICMP_FRAG_NEEDED, htonl(mtu));
 		ip_rt_put(rt);
 		IP_VS_DBG_RL("%s(): frag needed\n", __func__);
 		goto tx_error;
@@ -789,9 +1466,9 @@ ip_vs_dr_xmit(struct sk_buff *skb, struc
 	LeaveFunction(10);
 	return NF_STOLEN;
 
-  tx_error_icmp:
+      tx_error_icmp:
 	dst_link_failure(skb);
-  tx_error:
+      tx_error:
 	kfree_skb(skb);
 	LeaveFunction(10);
 	return NF_STOLEN;
@@ -802,8 +1479,8 @@ int
 ip_vs_dr_xmit_v6(struct sk_buff *skb, struct ip_vs_conn *cp,
 		 struct ip_vs_protocol *pp)
 {
-	struct rt6_info *rt;			/* Route to the other host */
-	int    mtu;
+	struct rt6_info *rt;	/* Route to the other host */
+	int mtu;
 
 	EnterFunction(10);
 
@@ -842,16 +1519,15 @@ ip_vs_dr_xmit_v6(struct sk_buff *skb, st
 	LeaveFunction(10);
 	return NF_STOLEN;
 
-tx_error_icmp:
+      tx_error_icmp:
 	dst_link_failure(skb);
-tx_error:
+      tx_error:
 	kfree_skb(skb);
 	LeaveFunction(10);
 	return NF_STOLEN;
 }
 #endif
 
-
 /*
  *	ICMP packet transmitter
  *	called by the ip_vs_in_icmp
@@ -860,7 +1536,7 @@ int
 ip_vs_icmp_xmit(struct sk_buff *skb, struct ip_vs_conn *cp,
 		struct ip_vs_protocol *pp, int offset)
 {
-	struct rtable	*rt;	/* Route to the other host */
+	struct rtable *rt;	/* Route to the other host */
 	int mtu;
 	int rc;
 
@@ -869,7 +1545,8 @@ ip_vs_icmp_xmit(struct sk_buff *skb, str
 	/* The ICMP packet for VS/TUN, VS/DR and LOCALNODE will be
 	   forwarded directly here, because there is no need to
 	   translate address/port back */
-	if (IP_VS_FWD_METHOD(cp) != IP_VS_CONN_F_MASQ) {
+	if ((IP_VS_FWD_METHOD(cp) != IP_VS_CONN_F_MASQ) &&
+	    (IP_VS_FWD_METHOD(cp) != IP_VS_CONN_F_FULLNAT)) {
 		if (cp->packet_xmit)
 			rc = cp->packet_xmit(skb, cp, pp);
 		else
@@ -916,15 +1593,15 @@ ip_vs_icmp_xmit(struct sk_buff *skb, str
 	rc = NF_STOLEN;
 	goto out;
 
-  tx_error_icmp:
+      tx_error_icmp:
 	dst_link_failure(skb);
-  tx_error:
+      tx_error:
 	dev_kfree_skb(skb);
 	rc = NF_STOLEN;
-  out:
+      out:
 	LeaveFunction(10);
 	return rc;
-  tx_error_put:
+      tx_error_put:
 	ip_rt_put(rt);
 	goto tx_error;
 }
@@ -932,9 +1609,9 @@ ip_vs_icmp_xmit(struct sk_buff *skb, str
 #ifdef CONFIG_IP_VS_IPV6
 int
 ip_vs_icmp_xmit_v6(struct sk_buff *skb, struct ip_vs_conn *cp,
-		struct ip_vs_protocol *pp, int offset)
+		   struct ip_vs_protocol *pp, int offset)
 {
-	struct rt6_info	*rt;	/* Route to the other host */
+	struct rt6_info *rt;	/* Route to the other host */
 	int mtu;
 	int rc;
 
@@ -943,7 +1620,8 @@ ip_vs_icmp_xmit_v6(struct sk_buff *skb, 
 	/* The ICMP packet for VS/TUN, VS/DR and LOCALNODE will be
 	   forwarded directly here, because there is no need to
 	   translate address/port back */
-	if (IP_VS_FWD_METHOD(cp) != IP_VS_CONN_F_MASQ) {
+	if ((IP_VS_FWD_METHOD(cp) != IP_VS_CONN_F_MASQ) &&
+	    (IP_VS_FWD_METHOD(cp) != IP_VS_CONN_F_FULLNAT)) {
 		if (cp->packet_xmit)
 			rc = cp->packet_xmit(skb, cp, pp);
 		else
@@ -991,15 +1669,15 @@ ip_vs_icmp_xmit_v6(struct sk_buff *skb, 
 	rc = NF_STOLEN;
 	goto out;
 
-tx_error_icmp:
+      tx_error_icmp:
 	dst_link_failure(skb);
-tx_error:
+      tx_error:
 	dev_kfree_skb(skb);
 	rc = NF_STOLEN;
-out:
+      out:
 	LeaveFunction(10);
 	return rc;
-tx_error_put:
+      tx_error_put:
 	dst_release(&rt->u.dst);
 	goto tx_error;
 }
diff -uprN linux-2.6.32-220.23.1.el6.x86_64.bak/net/netfilter/ipvs/Kconfig linux-2.6.32-220.23.1.el6.x86_64/net/netfilter/ipvs/Kconfig
--- linux-2.6.32-220.23.1.el6.x86_64.bak/net/netfilter/ipvs/Kconfig	2012-10-31 19:49:06.122132895 +0800
+++ linux-2.6.32-220.23.1.el6.x86_64/net/netfilter/ipvs/Kconfig	2013-04-11 16:06:03.000000000 +0800
@@ -43,8 +43,8 @@ config	IP_VS_DEBUG
 
 config	IP_VS_TAB_BITS
 	int "IPVS connection table size (the Nth power of 2)"
-	range 8 20
-	default 12
+	range 8 22
+	default 22
 	---help---
 	  The IPVS connection hash table uses the chaining scheme to handle
 	  hash collisions. Using a big IPVS connection hash table will greatly
@@ -82,6 +82,13 @@ config	IP_VS_PROTO_UDP
 	  This option enables support for load balancing UDP transport
 	  protocol. Say Y if unsure.
 
+config	IP_VS_PROTO_ICMP
+	bool "ICMP load balancing support for dsnat"
+	---help---
+	  This option enables support for dsnat load balancing UDP transport
+	  protocol. Say Y if unsure.
+
+config	IP_VS_PROTO_AH_ESP
 config	IP_VS_PROTO_AH_ESP
 	bool
 	depends on UNDEFINED
diff -uprN linux-2.6.32-220.23.1.el6.x86_64.bak/net/netfilter/ipvs/Makefile linux-2.6.32-220.23.1.el6.x86_64/net/netfilter/ipvs/Makefile
--- linux-2.6.32-220.23.1.el6.x86_64.bak/net/netfilter/ipvs/Makefile	2012-10-31 19:49:01.156805439 +0800
+++ linux-2.6.32-220.23.1.el6.x86_64/net/netfilter/ipvs/Makefile	2013-04-11 15:49:26.000000000 +0800
@@ -6,11 +6,13 @@
 ip_vs_proto-objs-y :=
 ip_vs_proto-objs-$(CONFIG_IP_VS_PROTO_TCP) += ip_vs_proto_tcp.o
 ip_vs_proto-objs-$(CONFIG_IP_VS_PROTO_UDP) += ip_vs_proto_udp.o
+ip_vs_proto-objs-$(CONFIG_IP_VS_PROTO_ICMP) += ip_vs_proto_icmp.o
 ip_vs_proto-objs-$(CONFIG_IP_VS_PROTO_AH_ESP) += ip_vs_proto_ah_esp.o
 
 ip_vs-objs :=	ip_vs_conn.o ip_vs_core.o ip_vs_ctl.o ip_vs_sched.o	   \
 		ip_vs_xmit.o ip_vs_app.o ip_vs_sync.o	   		   \
 		ip_vs_est.o ip_vs_proto.o 				   \
+		ip_vs_synproxy.o					   \
 		$(ip_vs_proto-objs-y)
 
 
