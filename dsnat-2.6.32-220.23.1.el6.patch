diff --git a/include/linux/ip_vs.h b/include/linux/ip_vs.h
index fc3e41b..dfdc2fe 100644
--- a/include/linux/ip_vs.h
+++ b/include/linux/ip_vs.h
@@ -1,6 +1,9 @@
 /*
  *      IP Virtual Server
  *      data structure and functionality definitions
+ *
+ * Changes:
+ *   Yu Bo        <yubo@xiaomi.com>
  */
 
 #ifndef _IP_VS_H
@@ -55,7 +58,11 @@
 #define IP_VS_SO_SET_RESTORE    (IP_VS_BASE_CTL+13)
 #define IP_VS_SO_SET_SAVE       (IP_VS_BASE_CTL+14)
 #define IP_VS_SO_SET_ZERO	(IP_VS_BASE_CTL+15)
-#define IP_VS_SO_SET_MAX	IP_VS_SO_SET_ZERO
+#define IP_VS_SO_SET_ADDLADDR	(IP_VS_BASE_CTL+16)
+#define IP_VS_SO_SET_DELLADDR	(IP_VS_BASE_CTL+17)
+#define IP_VS_SO_SET_ADDZONE	(IP_VS_BASE_CTL+18)
+#define IP_VS_SO_SET_DELZONE	(IP_VS_BASE_CTL+19)
+#define IP_VS_SO_SET_MAX	IP_VS_SO_SET_DELZONE
 
 #define IP_VS_SO_GET_VERSION	IP_VS_BASE_CTL
 #define IP_VS_SO_GET_INFO	(IP_VS_BASE_CTL+1)
@@ -65,8 +72,10 @@
 #define IP_VS_SO_GET_DEST	(IP_VS_BASE_CTL+5)	/* not used now */
 #define IP_VS_SO_GET_TIMEOUT	(IP_VS_BASE_CTL+6)
 #define IP_VS_SO_GET_DAEMON	(IP_VS_BASE_CTL+7)
-#define IP_VS_SO_GET_MAX	IP_VS_SO_GET_DAEMON
-
+#define IP_VS_SO_GET_LADDRS	(IP_VS_BASE_CTL+8)
+#define IP_VS_SO_GET_ZONES	(IP_VS_BASE_CTL+9)
+#define IP_VS_SO_GET_ZONE	(IP_VS_BASE_CTL+10)
+#define IP_VS_SO_GET_MAX	IP_VS_SO_GET_ZONE
 
 /*
  *      IPVS Connection Flags
@@ -77,6 +86,7 @@
 #define IP_VS_CONN_F_TUNNEL	0x0002		/* tunneling */
 #define IP_VS_CONN_F_DROUTE	0x0003		/* direct routing */
 #define IP_VS_CONN_F_BYPASS	0x0004		/* cache bypass */
+#define IP_VS_CONN_F_FULLNAT	0x0005	/* full nat */
 #define IP_VS_CONN_F_SYNC	0x0020		/* entry created by sync */
 #define IP_VS_CONN_F_HASHED	0x0040		/* hashed entry */
 #define IP_VS_CONN_F_NOOUTPUT	0x0080		/* no output packets */
@@ -87,6 +97,9 @@
 #define IP_VS_CONN_F_NO_CPORT	0x0800		/* no client port set yet */
 #define IP_VS_CONN_F_TEMPLATE	0x1000		/* template, not connection */
 #define IP_VS_CONN_F_ONE_PACKET	0x2000		/* forward only one packet */
+#define IP_VS_CONN_F_CIP_INSERTED 0x4000	/* client ip address has inserted */
+#define IP_VS_CONN_F_SYNPROXY	0x8000	/* syn proxy flag */
+#define IP_VS_CONN_F_DSNAT	0x010000
 
 #define IP_VS_SCHEDNAME_MAXLEN	16
 #define IP_VS_IFNAME_MAXLEN	16
@@ -125,15 +138,23 @@ struct ip_vs_dest_user {
 	__u32		l_threshold;	/* lower threshold */
 };
 
+struct ip_vs_zone_user {
+	__be32 addr;		/* ipv4 address */
+	__be32 netmask;		/* ipv4 netmask */
+};
+
+
+struct ip_vs_laddr_user {
+	__be32 addr;		/* ipv4 address */
+};
 
 /*
  *	IPVS statistics object (for user space)
  */
-struct ip_vs_stats_user
-{
-	__u32                   conns;          /* connections scheduled */
-	__u32                   inpkts;         /* incoming packets */
-	__u32                   outpkts;        /* outgoing packets */
+struct ip_vs_stats_user {
+	__u64 conns;		/* connections scheduled */
+	__u64 inpkts;		/* incoming packets */
+	__u64 outpkts;		/* outgoing packets */
 	__u64                   inbytes;        /* incoming bytes */
 	__u64                   outbytes;       /* outgoing bytes */
 
@@ -155,6 +176,9 @@ struct ip_vs_getinfo {
 
 	/* number of virtual services */
 	unsigned int		num_services;
+
+	/* number of zones */
+	unsigned int		num_zones;	
 };
 
 
@@ -175,10 +199,20 @@ struct ip_vs_service_entry {
 	/* number of real servers */
 	unsigned int		num_dests;
 
+	/* number of local address */
+	unsigned int num_laddrs;
+
 	/* statistics */
 	struct ip_vs_stats_user stats;
 };
 
+struct ip_vs_zone_entry {
+	__be32 addr;
+	__be32 netmask;
+
+	/* number of local address */
+	unsigned int num_laddrs;
+};
 
 struct ip_vs_dest_entry {
 	__be32			addr;		/* destination address */
@@ -197,6 +231,12 @@ struct ip_vs_dest_entry {
 	struct ip_vs_stats_user stats;
 };
 
+struct ip_vs_laddr_entry {
+	__be32 addr;		/* ipv4 address */
+
+	__u64 port_conflict;	/* conflict counts */
+	__u32 conn_counts;	/* current connects */
+};
 
 /* The argument to IP_VS_SO_GET_DESTS */
 struct ip_vs_get_dests {
@@ -214,6 +254,32 @@ struct ip_vs_get_dests {
 };
 
 
+/* The argument to IP_VS_SO_GET_ZONES */
+struct ip_vs_get_zones {
+	/* number of virtual services */
+	unsigned int		num_zones;
+
+	/* service table */
+	struct ip_vs_zone_entry entrytable[0];
+};
+
+
+/* The argument to IP_VS_SO_GET_LADDRS */
+struct ip_vs_get_laddrs {
+	/* which service: user fills in these */
+	//__u16 protocol;
+	__be32 addr;		/* virtual address */
+	//__be16 port;
+	//__u32 fwmark;		/* firwall mark of service */
+	__be32 netmask;
+
+	/* number of local address */
+	unsigned int num_laddrs;
+
+	/* the real servers */
+	struct ip_vs_laddr_entry entrytable[0];
+};
+
 /* The argument to IP_VS_SO_GET_SERVICES */
 struct ip_vs_get_services {
 	/* number of virtual services */
@@ -287,6 +353,14 @@ enum {
 	IPVS_CMD_ZERO,			/* zero all counters and stats */
 	IPVS_CMD_FLUSH,			/* flush services and dests */
 
+	IPVS_CMD_NEW_LADDR,	/* add local address */
+	IPVS_CMD_DEL_LADDR,	/* del local address */
+	IPVS_CMD_GET_LADDR,	/* dump local address */
+
+	IPVS_CMD_NEW_ZONE,	/* add zone */
+	IPVS_CMD_DEL_ZONE,	/* del zone */
+	IPVS_CMD_GET_ZONE,	/* get zone info */
+
 	__IPVS_CMD_MAX,
 };
 
@@ -301,11 +375,28 @@ enum {
 	IPVS_CMD_ATTR_TIMEOUT_TCP,	/* TCP connection timeout */
 	IPVS_CMD_ATTR_TIMEOUT_TCP_FIN,	/* TCP FIN wait timeout */
 	IPVS_CMD_ATTR_TIMEOUT_UDP,	/* UDP timeout */
+	IPVS_CMD_ATTR_LADDR,	/* nested local address attribute */
+	IPVS_CMD_ATTR_ZONE,	/* nested zone attribute */
 	__IPVS_CMD_ATTR_MAX,
 };
 
 #define IPVS_CMD_ATTR_MAX (__IPVS_SVC_ATTR_MAX - 1)
 
+
+/*
+ * Attributes used to describe a service
+ *
+ * Used inside nested attribute IPVS_CMD_ATTR_ZONE
+ */
+enum {
+	IPVS_ZONE_ATTR_UNSPEC = 0,
+	IPVS_ZONE_ATTR_ADDR,		/* address  */
+	IPVS_ZONE_ATTR_NETMASK,		/* netmask */
+	__IPVS_ZONE_ATTR_MAX,
+};
+
+#define IPVS_ZONE_ATTR_MAX (__IPVS_ZONE_ATTR_MAX - 1)
+
 /*
  * Attributes used to describe a service
  *
@@ -357,6 +448,21 @@ enum {
 #define IPVS_DEST_ATTR_MAX (__IPVS_DEST_ATTR_MAX - 1)
 
 /*
+ *  * Attirbutes used to describe a local address
+ *   *
+ *    */
+
+enum {
+	IPVS_LADDR_ATTR_UNSPEC = 0,
+	IPVS_LADDR_ATTR_ADDR,
+	IPVS_LADDR_ATTR_PORT_CONFLICT,
+	IPVS_LADDR_ATTR_CONN_COUNTS,
+	__IPVS_LADDR_ATTR_MAX,
+};
+
+#define IPVS_LADDR_ATTR_MAX (__IPVS_LADDR_ATTR_MAX - 1)
+
+/*
  * Attributes describing a sync daemon
  *
  * Used inside nested attribute IPVS_CMD_ATTR_DAEMON
diff --git a/include/net/inet_common.h b/include/net/inet_common.h
index ad5535e..6463f8b 100644
--- a/include/net/inet_common.h
+++ b/include/net/inet_common.h
@@ -1,7 +1,7 @@
 #ifndef _INET_COMMON_H
 #define _INET_COMMON_H
 
-extern struct proto_ops		inet_stream_ops;
+extern struct proto_ops			inet_stream_ops;
 extern const struct proto_ops		inet_dgram_ops;
 
 /*
diff --git a/include/net/ip_vs.h b/include/net/ip_vs.h
index 98978e7..2d7f72f 100644
--- a/include/net/ip_vs.h
+++ b/include/net/ip_vs.h
@@ -1,6 +1,8 @@
 /*
  *      IP Virtual Server
  *      data structure and functionality definitions
+ * Changes:
+ *   Yu Bo        <yubo@xiaomi.com>
  */
 
 #ifndef _NET_IP_VS_H
@@ -69,6 +71,7 @@ static inline int ip_vs_addr_equal(int af, const union nf_inet_addr *a,
 				   const union nf_inet_addr *b)
 {
 #ifdef CONFIG_IP_VS_IPV6
+	af &= ~IP_VS_CONN_F_DSNAT;
 	if (af == AF_INET6)
 		return ipv6_addr_equal(&a->in6, &b->in6);
 #endif
@@ -178,13 +181,15 @@ static inline const char *ip_vs_dbg_addr(int af, char *buf, size_t buf_len,
 
 #define	IP_VS_WAIT_WHILE(expr)	while (expr) { cpu_relax(); }
 
-
 /*
  *      The port number of FTP service (in network order).
  */
 #define FTPPORT  cpu_to_be16(21)
 #define FTPDATA  cpu_to_be16(20)
 
+#define IP_VS_DSNAT_RS_ADDR	cpu_to_be32((1<<24)+1)
+#define IP_VS_DSNAT_RS_PORT	cpu_to_be16(1)
+
 /*
  *      TCP State Values
  */
@@ -229,34 +234,18 @@ struct ip_vs_seq {
 	__u32			delta;		/* Delta in sequence numbers */
 	__u32			previous_delta;	/* Delta in sequence numbers
 						   before last resized pkt */
+	__u32 fdata_seq;	/* sequence of first data packet */
 };
 
-
 /*
  *	IPVS statistics objects
  */
-struct ip_vs_estimator {
-	struct list_head	list;
-
-	u64			last_inbytes;
-	u64			last_outbytes;
-	u32			last_conns;
-	u32			last_inpkts;
-	u32			last_outpkts;
-
-	u32			cps;
-	u32			inpps;
-	u32			outpps;
-	u32			inbps;
-	u32			outbps;
-};
-
-struct ip_vs_stats
-{
-	struct ip_vs_stats_user	ustats;         /* statistics */
-	struct ip_vs_estimator	est;		/* estimator */
-
-	spinlock_t              lock;           /* spin lock */
+struct ip_vs_stats {
+	__u64 conns;		/* connections scheduled */
+	__u64 inpkts;		/* incoming packets */
+	__u64 outpkts;		/* outgoing packets */
+	__u64 inbytes;		/* incoming bytes */
+	__u64 outbytes;		/* outgoing bytes */
 };
 
 struct dst_entry;
@@ -287,22 +276,30 @@ struct ip_vs_protocol {
 		       const struct sk_buff *skb,
 		       struct ip_vs_protocol *pp,
 		       const struct ip_vs_iphdr *iph,
-		       unsigned int proto_off,
-		       int inverse);
+			    unsigned int proto_off, int inverse, int *res_dir);
 
 	struct ip_vs_conn *
 	(*conn_out_get)(int af,
 			const struct sk_buff *skb,
 			struct ip_vs_protocol *pp,
 			const struct ip_vs_iphdr *iph,
-			unsigned int proto_off,
-			int inverse);
+			     unsigned int proto_off, int inverse, int *res_dir);
 
 	int (*snat_handler)(struct sk_buff *skb,
-			    struct ip_vs_protocol *pp, struct ip_vs_conn *cp);
+			     struct ip_vs_protocol * pp,
+			     struct ip_vs_conn * cp);
 
 	int (*dnat_handler)(struct sk_buff *skb,
-			    struct ip_vs_protocol *pp, struct ip_vs_conn *cp);
+			     struct ip_vs_protocol * pp,
+			     struct ip_vs_conn * cp);
+
+	int (*fnat_in_handler) (struct sk_buff ** skb_p,
+				struct ip_vs_protocol * pp,
+				struct ip_vs_conn * cp);
+
+	int (*fnat_out_handler) (struct sk_buff * skb,
+				 struct ip_vs_protocol * pp,
+				 struct ip_vs_conn * cp);
 
 	int (*csum_check)(int af, struct sk_buff *skb,
 			  struct ip_vs_protocol *pp);
@@ -321,31 +318,61 @@ struct ip_vs_protocol {
 
 	void (*debug_packet)(struct ip_vs_protocol *pp,
 			     const struct sk_buff *skb,
-			     int offset,
-			     const char *msg);
+			      int offset, const char *msg);
 
 	void (*timeout_change)(struct ip_vs_protocol *pp, int flags);
 
-	int (*set_state_timeout)(struct ip_vs_protocol *pp, char *sname, int to);
+	int (*set_state_timeout) (struct ip_vs_protocol * pp, char *sname,
+				  int to);
+
+	void (*conn_expire_handler) (struct ip_vs_protocol * pp,
+				     struct ip_vs_conn * cp);
 };
 
 extern struct ip_vs_protocol * ip_vs_proto_get(unsigned short proto);
 
 /*
+ *      Connection Index Flags
+ */
+#define IP_VS_CIDX_F_OUT2IN     0x0001	/* packet director, OUTside2INside */
+#define IP_VS_CIDX_F_IN2OUT     0x0002	/* packet director, INside2OUTside */
+#define IP_VS_CIDX_F_DIR_MASK	0x0003	/* packet director mask */
+
+/*
+ *      Connection index in HASH TABLE, each connection has two index
+ */
+struct ip_vs_conn_idx {
+	struct list_head c_list;	/* hashed list heads */
+
+	u16 af;			/* address family */
+	__u16 protocol;		/* Which protocol (TCP/UDP) */
+	union nf_inet_addr s_addr;	/* source address */
+	union nf_inet_addr d_addr;	/* destination address */
+	__be16 s_port;		/* source port */
+	__be16 d_port;		/* destination port */
+
+	struct ip_vs_conn *cp;	/* point to connection */
+	volatile __u16 flags;	/* status flags */
+};
+
+/*
  *	IP_VS structure allocated for each dynamically scheduled connection
  */
 struct ip_vs_conn {
-	struct list_head        c_list;         /* hashed list heads */
+	struct ip_vs_conn_idx *in_idx;	/* client-vs hash index */
+	struct ip_vs_conn_idx *out_idx;	/* rs-vs hash index */
 
 	/* Protocol, addresses and port numbers */
 	u16                      af;		/* address family */
+	__u16 protocol;		/* Which protocol (TCP/UDP) */
 	union nf_inet_addr       caddr;          /* client address */
 	union nf_inet_addr       vaddr;          /* virtual address */
+	union nf_inet_addr laddr;	/* local address */
 	union nf_inet_addr       daddr;          /* destination address */
 	__be16                   cport;
 	__be16                   vport;
+	__be16 lport;
 	__be16                   dport;
-	__u16                   protocol;       /* Which protocol (TCP/UDP) */
 
 	/* counter and timer */
 	atomic_t		refcnt;		/* reference count */
@@ -365,8 +392,12 @@ struct ip_vs_conn {
 	struct ip_vs_conn       *control;       /* Master control connection */
 	atomic_t                n_control;      /* Number of controlled ones */
 	struct ip_vs_dest       *dest;          /* real server */
+	struct ip_vs_laddr *local;	/* local address */
 	atomic_t                in_pkts;        /* incoming packet counter */
 
+	/* for fullnat */
+	struct ip_vs_seq fnat_seq;
+
 	/* packet transmitter for different forwarding methods.  If it
 	   mangles the packet, it must return NF_DROP or better NF_STOLEN,
 	   otherwise this must be changed to a sk_buff **.
@@ -381,6 +412,27 @@ struct ip_vs_conn {
 	void                    *app_data;      /* Application private data */
 	struct ip_vs_seq        in_seq;         /* incoming seq. struct */
 	struct ip_vs_seq        out_seq;        /* outgoing seq. struct */
+
+	/* syn-proxy related members
+	 */
+	struct ip_vs_seq syn_proxy_seq;	/* seq. used in syn proxy */
+	struct sk_buff_head ack_skb;	/* ack skb, save in step2 */
+	struct sk_buff *syn_skb;	/* saved rs syn packet */
+	atomic_t syn_retry_max;	/* syn retransmition max count */
+
+	/* add for stopping ack storm */
+	__u32 last_seq;		/* seq of the last ack packet */
+	__u32 last_ack_seq;	/* ack seq of the last ack packet */
+	atomic_t dup_ack_cnt;	/* count of repeated ack packets */
+
+	/* for RST */
+	__u32 rs_end_seq;	/* end seq(seq+datalen) of the last ack packet from rs */
+	__u32 rs_ack_seq;	/* ack seq of the last ack packet from rs */
+
+	/* L2 direct response xmit */
+	struct net_device	*indev;
+	unsigned char		src_hwaddr[MAX_ADDR_LEN];
+	unsigned char		dst_hwaddr[MAX_ADDR_LEN];
 };
 
 
@@ -422,6 +474,27 @@ struct ip_vs_dest_user_kern {
 	u32			l_threshold;	/* lower threshold */
 };
 
+struct ip_vs_laddr_user_kern {
+	union nf_inet_addr addr;	/* ip address */
+};
+
+struct ip_vs_zone_user_kern {
+	union nf_inet_addr addr;	/* ip address */
+	__be32			netmask;
+};
+
+
+/*
+ * dsnat 
+ */
+struct ip_vs_dsnat {
+	struct ip_vs_service *svc[IPPROTO_MAX];
+	char iniface[IFNAMSIZ], outiface[IFNAMSIZ];
+	unsigned char iniface_mask[IFNAMSIZ], outiface_mask[IFNAMSIZ];	
+};
+
+
+
 
 /*
  *	The information about the virtual service offered to the net
@@ -442,9 +515,17 @@ struct ip_vs_service {
 	unsigned		timeout;  /* persistent timeout in ticks */
 	__be32			netmask;  /* grouping granularity */
 
+	/* for realservers list */
 	struct list_head	destinations;  /* real server d-linked list */
 	__u32			num_dests;     /* number of servers */
-	struct ip_vs_stats      stats;         /* statistics for the service */
+
+	/* for local ip address list, now only used in FULL NAT model */
+	struct list_head laddr_list;	/* local ip address list */
+	rwlock_t laddr_lock;	/* lock for protect curr_laddr */
+	__u32 num_laddrs;	/* number of local ip address */
+	struct list_head *curr_laddr;	/* laddr data list head */
+
+	struct ip_vs_stats *stats;	/* Use per-cpu statistics for the service */
 	struct ip_vs_app	*inc;	  /* bind conns to this app inc */
 
 	/* for scheduling */
@@ -470,7 +551,7 @@ struct ip_vs_dest {
 	atomic_t		weight;		/* server weight */
 
 	atomic_t		refcnt;		/* reference counter */
-	struct ip_vs_stats      stats;          /* statistics */
+	struct ip_vs_stats *stats;	/* Use per-cpu statistics for destination server */
 
 	/* connection counters and thresholds */
 	atomic_t		activeconns;	/* active connections */
@@ -494,6 +575,42 @@ struct ip_vs_dest {
 
 
 /*
+ *	The information about the  zone 
+ */
+struct ip_vs_zone {
+	struct list_head	s_list;   /* for normal zone table */
+	u16 af;
+	atomic_t		refcnt;   /* reference counter */
+	atomic_t		usecnt;   /* use counter */
+
+	union nf_inet_addr	addr;	  /* IP address for virtual service */
+	__be32			netmask;
+
+	/* for local ip address list, now only used in FULL NAT model */
+	struct list_head laddr_list;	/* local ip address list */
+	rwlock_t laddr_lock;	/* lock for protect curr_laddr */
+	__u32 num_laddrs;	/* number of local ip address */
+	struct list_head *curr_laddr;	/* laddr data list head */
+
+};
+
+
+
+/*
+ *	Local ip address object, now only used in FULL NAT model
+ */
+struct ip_vs_laddr {
+	struct list_head n_list;	/* for the local address in the service */
+	u16 af;			/* address family */
+	union nf_inet_addr addr;	/* ip address */
+	atomic64_t port;	/* port counts */
+	atomic_t refcnt;	/* reference count */
+
+	atomic64_t port_conflict;	/* conflict counts */
+	atomic_t conn_counts;	/* connects counts */
+};
+
+/*
  *	The scheduler object
  */
 struct ip_vs_scheduler {
@@ -578,6 +695,110 @@ struct ip_vs_app
 	void (*timeout_change)(struct ip_vs_app *app, int flags);
 };
 
+#define TCPOPT_ADDR  254
+#define TCPOLEN_ADDR 8		/* |opcode|size|ip+port| = 1 + 1 + 6 */
+
+/*
+ * insert client ip in tcp option, now only support IPV4,
+ * must be 4 bytes alignment.
+ */
+struct ip_vs_tcpo_addr {
+	__u8 opcode;
+	__u8 opsize;
+	__u16 port;
+	__u32 addr;
+};
+
+#ifdef CONFIG_IP_VS_IPV6
+#define TCPOPT_ADDR_V6	253
+#define TCPOLEN_ADDR_V6	20	/* |opcode|size|port|ipv6| = 1 + 1 + 2 + 16 */
+
+/*
+ * insert client ip in tcp option, for IPv6
+ * must be 4 bytes alignment.
+ */
+struct ip_vs_tcpo_addr_v6 {
+	__u8	opcode;
+	__u8	opsize;
+	__be16	port;
+	struct in6_addr addr;
+};
+#endif
+
+/*
+ * statistics for FULLNAT and SYNPROXY
+ * in /proc/net/ip_vs_ext_stats
+ */
+enum {
+	FULLNAT_ADD_TOA_OK = 1,
+	FULLNAT_ADD_TOA_FAIL_LEN,
+	FULLNAT_ADD_TOA_HEAD_FULL,
+	FULLNAT_ADD_TOA_FAIL_MEM,
+	FULLNAT_ADD_TOA_FAIL_PROTO,
+	FULLNAT_CONN_REUSED,
+	FULLNAT_CONN_REUSED_CLOSE,
+	FULLNAT_CONN_REUSED_TIMEWAIT,
+	FULLNAT_CONN_REUSED_FINWAIT,
+	FULLNAT_CONN_REUSED_CLOSEWAIT,
+	FULLNAT_CONN_REUSED_LASTACK,
+	FULLNAT_CONN_REUSED_ESTAB,
+	SYNPROXY_RS_ERROR,
+	SYNPROXY_NULL_ACK,
+	SYNPROXY_BAD_ACK,
+	SYNPROXY_OK_ACK,
+	SYNPROXY_SYN_CNT,
+	SYNPROXY_ACK_STORM,
+	SYNPROXY_SYNSEND_QLEN,
+	SYNPROXY_CONN_REUSED,
+	SYNPROXY_CONN_REUSED_CLOSE,
+	SYNPROXY_CONN_REUSED_TIMEWAIT,
+	SYNPROXY_CONN_REUSED_FINWAIT,
+	SYNPROXY_CONN_REUSED_CLOSEWAIT,
+	SYNPROXY_CONN_REUSED_LASTACK,
+	DEFENCE_IP_FRAG_DROP,
+	DEFENCE_IP_FRAG_GATHER,
+	DEFENCE_TCP_DROP,
+	DEFENCE_UDP_DROP,
+	FAST_XMIT_REJECT,
+	FAST_XMIT_PASS,
+	FAST_XMIT_SKB_COPY,
+	FAST_XMIT_NO_MAC,
+	FAST_XMIT_SYNPROXY_SAVE,
+	FAST_XMIT_DEV_LOST,
+	RST_IN_SYN_SENT,
+	RST_OUT_SYN_SENT,
+	RST_IN_ESTABLISHED,
+	RST_OUT_ESTABLISHED,
+	GRO_PASS,
+	LRO_REJECT,
+	XMIT_UNEXPECTED_MTU,
+	CONN_SCHED_UNREACH,
+	IP_VS_EXT_STAT_LAST
+};
+
+struct ip_vs_estats_entry {
+	char *name;
+	int entry;
+};
+
+#define IP_VS_ESTATS_ITEM(_name, _entry) { \
+        .name = _name,            \
+        .entry = _entry,          \
+}
+
+#define IP_VS_ESTATS_LAST {    \
+        NULL,           \
+        0,              \
+}
+
+struct ip_vs_estats_mib {
+	unsigned long mibs[IP_VS_EXT_STAT_LAST];
+};
+
+#define IP_VS_INC_ESTATS(mib, field)         \
+        (per_cpu_ptr(mib, smp_processor_id())->mibs[field]++)
+
+extern struct ip_vs_estats_mib *ip_vs_esmib;
 
 /*
  *      IPVS core functions
@@ -598,7 +819,7 @@ extern void ip_vs_init_hash_table(struct list_head *table, int rows);
  *     IPVS connection entry hash table
  */
 #ifndef CONFIG_IP_VS_TAB_BITS
-#define CONFIG_IP_VS_TAB_BITS   12
+#define CONFIG_IP_VS_TAB_BITS   22
 #endif
 
 #define IP_VS_CONN_TAB_BITS	CONFIG_IP_VS_TAB_BITS
@@ -612,17 +833,13 @@ enum {
 	IP_VS_DIR_LAST,
 };
 
-extern struct ip_vs_conn *ip_vs_conn_in_get
-(int af, int protocol, const union nf_inet_addr *s_addr, __be16 s_port,
- const union nf_inet_addr *d_addr, __be16 d_port);
+extern struct ip_vs_conn *ip_vs_conn_get
+    (int af, int protocol, const union nf_inet_addr *s_addr, __be16 s_port,
+     const union nf_inet_addr *d_addr, __be16 d_port, int *res_dir);
 
 extern struct ip_vs_conn *ip_vs_ct_in_get
-(int af, int protocol, const union nf_inet_addr *s_addr, __be16 s_port,
- const union nf_inet_addr *d_addr, __be16 d_port);
-
-extern struct ip_vs_conn *ip_vs_conn_out_get
-(int af, int protocol, const union nf_inet_addr *s_addr, __be16 s_port,
- const union nf_inet_addr *d_addr, __be16 d_port);
+    (int af, int protocol, const union nf_inet_addr *s_addr, __be16 s_port,
+     const union nf_inet_addr *d_addr, __be16 d_port);
 
 /* put back the conn without restarting its timer */
 static inline void __ip_vs_conn_put(struct ip_vs_conn *cp)
@@ -632,11 +849,16 @@ static inline void __ip_vs_conn_put(struct ip_vs_conn *cp)
 extern void ip_vs_conn_put(struct ip_vs_conn *cp);
 extern void ip_vs_conn_fill_cport(struct ip_vs_conn *cp, __be16 cport);
 
-extern struct ip_vs_conn *
-ip_vs_conn_new(int af, int proto, const union nf_inet_addr *caddr, __be16 cport,
-	       const union nf_inet_addr *vaddr, __be16 vport,
-	       const union nf_inet_addr *daddr, __be16 dport, unsigned flags,
-	       struct ip_vs_dest *dest);
+extern struct ip_vs_conn *ip_vs_conn_new(int af, int proto,
+					 const union nf_inet_addr *caddr,
+					 __be16 cport,
+					 const union nf_inet_addr *vaddr,
+					 __be16 vport,
+					 const union nf_inet_addr *daddr,
+					 __be16 dport, unsigned flags,
+					 struct ip_vs_dest *dest,
+					 struct sk_buff *skb,
+					 int is_synproxy_on);
 extern void ip_vs_conn_expire_now(struct ip_vs_conn *cp);
 
 extern const char * ip_vs_state_name(__u16 proto, int state);
@@ -762,8 +984,9 @@ extern int ip_vs_bind_scheduler(struct ip_vs_service *svc,
 extern int ip_vs_unbind_scheduler(struct ip_vs_service *svc);
 extern struct ip_vs_scheduler *ip_vs_scheduler_get(const char *sched_name);
 extern void ip_vs_scheduler_put(struct ip_vs_scheduler *scheduler);
-extern struct ip_vs_conn *
-ip_vs_schedule(struct ip_vs_service *svc, const struct sk_buff *skb);
+extern struct ip_vs_conn *ip_vs_schedule(struct ip_vs_service *svc,
+					 struct sk_buff *skb,
+					 int is_synproxy_on);
 extern int ip_vs_leave(struct ip_vs_service *svc, struct sk_buff *skb,
 			struct ip_vs_protocol *pp);
 
@@ -776,31 +999,56 @@ extern int sysctl_ip_vs_expire_nodest_conn;
 extern int sysctl_ip_vs_expire_quiescent_template;
 extern int sysctl_ip_vs_sync_threshold[2];
 extern int sysctl_ip_vs_nat_icmp_send;
-extern struct ip_vs_stats ip_vs_stats;
+extern struct ip_vs_stats *ip_vs_stats;
 extern const struct ctl_path net_vs_ctl_path[];
-
-extern struct ip_vs_service *
-ip_vs_service_get(int af, __u32 fwmark, __u16 protocol,
-		  const union nf_inet_addr *vaddr, __be16 vport);
+extern int sysctl_ip_vs_timestamp_remove_entry;
+extern int sysctl_ip_vs_mss_adjust_entry;
+extern int sysctl_ip_vs_conn_reused_entry;
+extern int sysctl_ip_vs_toa_entry;
+extern int sysctl_ip_vs_lport_max;
+extern int sysctl_ip_vs_lport_min;
+extern int sysctl_ip_vs_lport_tries;
+extern int sysctl_ip_vs_frag_drop_entry;
+extern int sysctl_ip_vs_tcp_drop_entry;
+extern int sysctl_ip_vs_udp_drop_entry;
+extern int sysctl_ip_vs_conn_expire_tcp_rst;
+extern struct ip_vs_zone  *ip_vs_zone_get(const union nf_inet_addr *addr);
+extern int sysctl_ip_vs_fast_xmit;
+
+extern struct ip_vs_service *ip_vs_service_get(int af, __u32 fwmark,
+					       __u16 protocol,
+					       const union nf_inet_addr *vaddr,
+					       __be16 vport);
+extern struct ip_vs_service *ip_vs_lookup_vip(int af, __u16 protocol,
+					      const union nf_inet_addr *vaddr);
 
 static inline void ip_vs_service_put(struct ip_vs_service *svc)
 {
 	atomic_dec(&svc->usecnt);
 }
 
-extern struct ip_vs_dest *
-ip_vs_lookup_real_service(int af, __u16 protocol,
-			  const union nf_inet_addr *daddr, __be16 dport);
+static inline void ip_vs_zone_put(struct ip_vs_zone *zone)
+{
+	atomic_dec(&zone->usecnt);
+}
+
+extern struct ip_vs_dest *ip_vs_lookup_real_service(int af, __u16 protocol,
+						    const union nf_inet_addr
+						    *daddr, __be16 dport);
 
 extern int ip_vs_use_count_inc(void);
 extern void ip_vs_use_count_dec(void);
 extern int ip_vs_control_init(void);
 extern void ip_vs_control_cleanup(void);
-extern struct ip_vs_dest *
-ip_vs_find_dest(int af, const union nf_inet_addr *daddr, __be16 dport,
-		const union nf_inet_addr *vaddr, __be16 vport, __u16 protocol);
+extern struct ip_vs_dest *ip_vs_find_dest(int af,
+					  const union nf_inet_addr *daddr,
+					  __be16 dport,
+					  const union nf_inet_addr *vaddr,
+					  __be16 vport, __u16 protocol);
 extern struct ip_vs_dest *ip_vs_try_bind_dest(struct ip_vs_conn *cp);
 
+extern void ip_vs_laddr_hold(struct ip_vs_laddr *addr);
+extern void ip_vs_laddr_put(struct ip_vs_laddr *addr);
 
 /*
  *      IPVS sync daemon data and function prototypes
@@ -817,13 +1065,29 @@ extern void ip_vs_sync_conn(struct ip_vs_conn *cp);
 
 
 /*
- *      IPVS rate estimator prototypes (from ip_vs_est.c)
+ *      IPVS statistic prototypes (from ip_vs_stats.c)
  */
-extern int ip_vs_estimator_init(void);
-extern void ip_vs_estimator_cleanup(void);
-extern void ip_vs_new_estimator(struct ip_vs_stats *stats);
-extern void ip_vs_kill_estimator(struct ip_vs_stats *stats);
-extern void ip_vs_zero_estimator(struct ip_vs_stats *stats);
+#define ip_vs_stats_cpu(stats,cpu)  \
+	(*per_cpu_ptr((stats), (cpu)))
+
+#define ip_vs_stats_this_cpu(stats) \
+	(*this_cpu_ptr((stats)))
+
+extern int ip_vs_new_stats(struct ip_vs_stats** p);
+extern void ip_vs_del_stats(struct ip_vs_stats* p);
+extern void ip_vs_zero_stats(struct ip_vs_stats* stats);
+extern void ip_vs_in_stats(struct ip_vs_conn *cp, struct sk_buff *skb);
+extern void ip_vs_out_stats(struct ip_vs_conn *cp, struct sk_buff *skb);
+extern void ip_vs_conn_stats(struct ip_vs_conn *cp, struct ip_vs_service *svc);
+
+/*
+ *	Lookup route table
+ */
+extern struct rtable *ip_vs_get_rt(union nf_inet_addr *addr, u32 rtos);
+
+#ifdef CONFIG_IP_VS_IPV6
+extern struct rt6_info *ip_vs_get_rt_v6(union nf_inet_addr *addr);
+#endif
 
 /*
  *	Various IPVS packet transmitters (from ip_vs_xmit.c)
@@ -834,26 +1098,56 @@ extern int ip_vs_bypass_xmit
 (struct sk_buff *skb, struct ip_vs_conn *cp, struct ip_vs_protocol *pp);
 extern int ip_vs_nat_xmit
 (struct sk_buff *skb, struct ip_vs_conn *cp, struct ip_vs_protocol *pp);
+extern int ip_vs_fnat_xmit
+    (struct sk_buff *skb, struct ip_vs_conn *cp, struct ip_vs_protocol *pp);
 extern int ip_vs_tunnel_xmit
 (struct sk_buff *skb, struct ip_vs_conn *cp, struct ip_vs_protocol *pp);
 extern int ip_vs_dr_xmit
 (struct sk_buff *skb, struct ip_vs_conn *cp, struct ip_vs_protocol *pp);
 extern int ip_vs_icmp_xmit
-(struct sk_buff *skb, struct ip_vs_conn *cp, struct ip_vs_protocol *pp, int offset);
+    (struct sk_buff *skb, struct ip_vs_conn *cp, struct ip_vs_protocol *pp,
+     int offset);
 extern void ip_vs_dst_reset(struct ip_vs_dest *dest);
+extern int ip_vs_normal_response_xmit
+    (struct sk_buff *skb, struct ip_vs_protocol *pp, struct ip_vs_conn *cp,
+     int ihl);
+extern int ip_vs_fnat_response_xmit(struct sk_buff *skb,
+				    struct ip_vs_protocol *pp,
+				    struct ip_vs_conn *cp, int ihl);
+extern int ip_vs_normal_response_icmp_xmit(struct sk_buff *skb,
+					   struct ip_vs_protocol *pp,
+					   struct ip_vs_conn *cp, int offset);
+extern int ip_vs_fnat_response_icmp_xmit(struct sk_buff *skb,
+					 struct ip_vs_protocol *pp,
+					 struct ip_vs_conn *cp, int offset);
 
 #ifdef CONFIG_IP_VS_IPV6
 extern int ip_vs_bypass_xmit_v6
 (struct sk_buff *skb, struct ip_vs_conn *cp, struct ip_vs_protocol *pp);
 extern int ip_vs_nat_xmit_v6
+    (struct sk_buff *skb, struct ip_vs_conn *cp, struct ip_vs_protocol *pp);
+extern int ip_vs_fnat_xmit_v6
 (struct sk_buff *skb, struct ip_vs_conn *cp, struct ip_vs_protocol *pp);
 extern int ip_vs_tunnel_xmit_v6
 (struct sk_buff *skb, struct ip_vs_conn *cp, struct ip_vs_protocol *pp);
 extern int ip_vs_dr_xmit_v6
 (struct sk_buff *skb, struct ip_vs_conn *cp, struct ip_vs_protocol *pp);
 extern int ip_vs_icmp_xmit_v6
-(struct sk_buff *skb, struct ip_vs_conn *cp, struct ip_vs_protocol *pp,
- int offset);
+    (struct sk_buff *skb, struct ip_vs_conn *cp, struct ip_vs_protocol *pp,
+     int offset);
+extern int ip_vs_normal_response_xmit_v6
+    (struct sk_buff *skb, struct ip_vs_protocol *pp, struct ip_vs_conn *cp,
+     int ihl);
+extern int ip_vs_fnat_response_xmit_v6(struct sk_buff *skb,
+				       struct ip_vs_protocol *pp,
+				       struct ip_vs_conn *cp, int ihl);
+extern int ip_vs_normal_response_icmp_xmit_v6(struct sk_buff *skb,
+					      struct ip_vs_protocol *pp,
+					      struct ip_vs_conn *cp,
+					      int offset);
+extern int ip_vs_fnat_response_icmp_xmit_v6(struct sk_buff *skb,
+					    struct ip_vs_protocol *pp,
+					    struct ip_vs_conn *cp, int offset);
 #endif
 
 /*
@@ -866,8 +1160,10 @@ extern int ip_vs_drop_counter;
 
 static __inline__ int ip_vs_todrop(void)
 {
-	if (!ip_vs_drop_rate) return 0;
-	if (--ip_vs_drop_counter > 0) return 0;
+	if (!ip_vs_drop_rate)
+		return 0;
+	if (--ip_vs_drop_counter > 0)
+		return 0;
 	ip_vs_drop_counter = ip_vs_drop_rate;
 	return 1;
 }
@@ -883,29 +1179,30 @@ static inline char ip_vs_fwd_tag(struct ip_vs_conn *cp)
 
 	switch (IP_VS_FWD_METHOD(cp)) {
 	case IP_VS_CONN_F_MASQ:
-		fwd = 'M'; break;
+		fwd = 'M';
+		break;
 	case IP_VS_CONN_F_LOCALNODE:
-		fwd = 'L'; break;
+		fwd = 'L';
+		break;
 	case IP_VS_CONN_F_TUNNEL:
-		fwd = 'T'; break;
+		fwd = 'T';
+		break;
 	case IP_VS_CONN_F_DROUTE:
-		fwd = 'R'; break;
+		fwd = 'R';
+		break;
 	case IP_VS_CONN_F_BYPASS:
-		fwd = 'B'; break;
+		fwd = 'B';
+		break;
+	case IP_VS_CONN_F_FULLNAT:
+		fwd = 'F';
+		break;
 	default:
-		fwd = '?'; break;
+		fwd = '?';
+		break;
 	}
 	return fwd;
 }
 
-extern void ip_vs_nat_icmp(struct sk_buff *skb, struct ip_vs_protocol *pp,
-			   struct ip_vs_conn *cp, int dir);
-
-#ifdef CONFIG_IP_VS_IPV6
-extern void ip_vs_nat_icmp_v6(struct sk_buff *skb, struct ip_vs_protocol *pp,
-			      struct ip_vs_conn *cp, int dir);
-#endif
-
 extern __sum16 ip_vs_checksum_complete(struct sk_buff *skb, int offset);
 
 static inline __wsum ip_vs_check_diff4(__be32 old, __be32 new, __wsum oldsum)
@@ -920,7 +1217,8 @@ static inline __wsum ip_vs_check_diff16(const __be32 *old, const __be32 *new,
 					__wsum oldsum)
 {
 	__be32 diff[8] = { ~old[3], ~old[2], ~old[1], ~old[0],
-			    new[3],  new[2],  new[1],  new[0] };
+		new[3], new[2], new[1], new[0]
+	};
 
 	return csum_partial(diff, sizeof(diff), oldsum);
 }
diff --git a/include/net/ip_vs_synproxy.h b/include/net/ip_vs_synproxy.h
new file mode 100644
index 0000000..98631b5
--- /dev/null
+++ b/include/net/ip_vs_synproxy.h
@@ -0,0 +1,135 @@
+/*
+ *     IP Virtual Server Syn-Proxy
+ *     data structure and functionality definitions
+ */
+
+#ifndef _NET_IP_VS_SYNPROXY_H
+#define _NET_IP_VS_SYNPROXY_H
+
+#include <net/ip_vs.h>
+
+/* Add MASKs for TCP OPT in "data" coded in cookie */
+/* |[21][20][19-16][15-0]|
+ * [21]    SACK
+ * [20]    TimeStamp
+ * [19-16] snd_wscale
+ * [15-0]  MSSIND
+ */
+#define IP_VS_SYNPROXY_MSS_BITS 16
+#define IP_VS_SYNPROXY_MSS_MASK (((__u32)1 << IP_VS_SYNPROXY_MSS_BITS) - 1)
+
+#define IP_VS_SYNPROXY_SACKOK_BIT 21
+#define IP_VS_SYNPROXY_SACKOK_MASK ((__u32)1 << IP_VS_SYNPROXY_SACKOK_BIT)
+
+#define IP_VS_SYNPROXY_TSOK_BIT 20
+#define IP_VS_SYNPROXY_TSOK_MASK ((__u32)1 << IP_VS_SYNPROXY_TSOK_BIT)
+
+#define IP_VS_SYNPROXY_SND_WSCALE_BITS 16
+#define IP_VS_SYNPROXY_SND_WSCALE_MASK ((__u32)0xf << IP_VS_SYNPROXY_SND_WSCALE_BITS)
+
+#define IP_VS_SYNPROXY_WSCALE_MAX          14
+
+/* add for supporting tcp options' in syn-proxy */
+struct ip_vs_synproxy_opt {
+	u16 snd_wscale:8,	/* Window scaling received from sender          */
+	 tstamp_ok:1,		/* TIMESTAMP seen on SYN packet                 */
+	 wscale_ok:1,		/* Wscale seen on SYN packet                    */
+	 sack_ok:1;		/* SACK seen on SYN packet                      */
+	u16 mss_clamp;		/* Maximal mss, negotiated at connection setup  */
+};
+
+/* 
+ * For syncookie compute and check 
+ */
+extern __u32 ip_vs_synproxy_cookie_v4_init_sequence(struct sk_buff *skb,
+						    struct ip_vs_synproxy_opt
+						    *opts);
+extern int ip_vs_synproxy_v4_cookie_check(struct sk_buff *skb, __u32 cookie,
+					  struct ip_vs_synproxy_opt *opt);
+
+extern __u32 ip_vs_synproxy_cookie_v6_init_sequence(struct sk_buff *skb,
+						    struct ip_vs_synproxy_opt
+						    *opts);
+extern int ip_vs_synproxy_v6_cookie_check(struct sk_buff *skb, __u32 cookie,
+					  struct ip_vs_synproxy_opt *opt);
+
+/*
+ * Syn-proxy step 1 logic: receive client's Syn.
+ */
+extern int ip_vs_synproxy_syn_rcv(int af, struct sk_buff *skb,
+				  struct ip_vs_iphdr *iph, int *verdict);
+/*
+ * Syn-proxy step 2 logic: receive client's Ack.
+ */
+extern int ip_vs_synproxy_ack_rcv(int af, struct sk_buff *skb,
+				  struct tcphdr *th, struct ip_vs_protocol *pp,
+				  struct ip_vs_conn **cpp,
+				  struct ip_vs_iphdr *iph, int *verdict);
+/*
+ * Syn-proxy step 3 logic: receive rs's Syn/Ack.
+ */
+extern int ip_vs_synproxy_synack_rcv(struct sk_buff *skb, struct ip_vs_conn *cp,
+				     struct ip_vs_protocol *pp,
+				     int ihl, int *verdict);
+/*
+ * Syn-proxy conn reuse logic: receive client's Ack.
+ */
+extern int ip_vs_synproxy_reuse_conn(int af, struct sk_buff *skb,
+				     struct ip_vs_conn *cp,
+				     struct ip_vs_protocol *pp,
+				     struct ip_vs_iphdr *iph, int *verdict);
+/*
+ * Store or drop client's ack packet, when lvs is waiting for 
+ * rs's Syn/Ack packet.
+ */
+extern int ip_vs_synproxy_filter_ack(struct sk_buff *skb, struct ip_vs_conn *cp,
+				     struct ip_vs_protocol *pp,
+				     struct ip_vs_iphdr *iph, int *verdict);
+
+/*
+ * Tranfer ack seq and sack opt for Out-In packet.
+ */
+extern void ip_vs_synproxy_dnat_handler(struct tcphdr *tcph,
+					struct ip_vs_seq *sp_seq);
+/*
+ * Tranfer seq for In-Out packet.
+ */
+extern int ip_vs_synproxy_snat_handler(struct tcphdr *tcph,
+				       struct ip_vs_conn *cp);
+
+/* syn-proxy sysctl variables */
+#define IP_VS_SYNPROXY_INIT_MSS_DEFAULT  	1452
+#define IP_VS_SYNPROXY_TTL_DEFAULT     		63
+#define IP_VS_SYNPROXY_TTL_MIN     		1
+#define IP_VS_SYNPROXY_TTL_MAX     		255
+#define IP_VS_SYNPROXY_SACK_DEFAULT  		1
+#define IP_VS_SYNPROXY_WSCALE_DEFAULT  		0
+#define IP_VS_SYNPROXY_TIMESTAMP_DEFAULT  	0
+#define IP_VS_SYNPROXY_DEFER_DEFAULT   		0
+#define IP_VS_SYNPROXY_DUP_ACK_DEFAULT     	10
+#define IP_VS_SYNPROXY_SKB_STORE_DEFAULT     	3
+#define IP_VS_SYNPROXY_CONN_REUSE_DEFAULT	1
+#define	IP_VS_SYNPROXY_CONN_REUSE_CL_DEFAULT	1
+#define	IP_VS_SYNPROXY_CONN_REUSE_TW_DEFAULT	1
+#define	IP_VS_SYNPROXY_CONN_REUSE_FW_DEFAULT	0
+#define	IP_VS_SYNPROXY_CONN_REUSE_CW_DEFAULT	0
+#define	IP_VS_SYNPROXY_CONN_REUSE_LA_DEFAULT	0
+#define	IP_VS_SYNPROXY_SYN_RETRY_DEFAULT	3
+
+extern int sysctl_ip_vs_synproxy_sack;
+extern int sysctl_ip_vs_synproxy_wscale;
+extern int sysctl_ip_vs_synproxy_timestamp;
+extern int sysctl_ip_vs_synproxy_synack_ttl;
+extern int sysctl_ip_vs_synproxy_init_mss;
+extern int sysctl_ip_vs_synproxy_defer;
+extern int sysctl_ip_vs_synproxy_dup_ack_thresh;
+extern int sysctl_ip_vs_synproxy_skb_store_thresh;
+extern int sysctl_ip_vs_synproxy_syn_retry;
+extern int sysctl_ip_vs_synproxy_conn_reuse;
+extern int sysctl_ip_vs_synproxy_conn_reuse_cl;
+extern int sysctl_ip_vs_synproxy_conn_reuse_tw;
+extern int sysctl_ip_vs_synproxy_conn_reuse_fw;
+extern int sysctl_ip_vs_synproxy_conn_reuse_cw;
+extern int sysctl_ip_vs_synproxy_conn_reuse_la;
+
+#endif
diff --git a/net/core/secure_seq.c b/net/core/secure_seq.c
index 07502b4..95a2566 100644
--- a/net/core/secure_seq.c
+++ b/net/core/secure_seq.c
@@ -105,6 +105,7 @@ __u32 secure_tcp_sequence_number(__be32 saddr, __be32 daddr,
 
 	return seq_scale(hash[0]);
 }
+EXPORT_SYMBOL(secure_tcp_sequence_number);
 
 u32 secure_ipv4_port_ephemeral(__be32 saddr, __be32 daddr, __be16 dport)
 {
diff --git a/net/ipv4/syncookies.c b/net/ipv4/syncookies.c
index a6e0e07..e733ca7 100644
--- a/net/ipv4/syncookies.c
+++ b/net/ipv4/syncookies.c
@@ -8,6 +8,12 @@
  *      modify it under the terms of the GNU General Public License
  *      as published by the Free Software Foundation; either version
  *      2 of the License, or (at your option) any later version.
+ *
+ * Changes:
+ *	Jian Chen <jian.chen1225@gmail.com>
+ *	Yan Tian <tianyan.7c00@gmail.com>
+ *
+ *	add synproxy cookies for ipvs module
  */
 
 #include <linux/tcp.h>
@@ -17,6 +23,7 @@
 #include <linux/kernel.h>
 #include <net/tcp.h>
 #include <net/route.h>
+#include <net/ip_vs_synproxy.h>
 
 /* Timestamps: lowest 9 bits store TCP options */
 #define TSBITS 9
@@ -363,3 +370,89 @@ struct sock *cookie_v4_check(struct sock *sk, struct sk_buff *skb,
 	ret = get_cookie_sock(sk, skb, req, &rt->u.dst);
 out:	return ret;
 }
+
+/*
+ * Generate a syncookie for ip_vs module. 
+ * Besides mss, we store additional tcp options in cookie "data".
+ * 
+ * Cookie "data" format: 
+ * |[21][20][19-16][15-0]|
+ * [21] SACKOK
+ * [20] TimeStampOK
+ * [19-16] snd_wscale
+ * [15-0] MSSIND 
+ */
+__u32 ip_vs_synproxy_cookie_v4_init_sequence(struct sk_buff *skb, 
+                                            struct ip_vs_synproxy_opt *opts) 
+{
+       const struct iphdr *iph = ip_hdr(skb);
+       const struct tcphdr *th = tcp_hdr(skb);
+       int mssind;
+       const __u16 mss = opts->mss_clamp;
+       __u32 data = 0;
+
+       /* XXX sort msstab[] by probability?  Binary search? */
+       for (mssind = 0; mss > msstab[mssind + 1]; mssind++)
+               ;
+       opts->mss_clamp = msstab[mssind] + 1;
+
+       data = mssind & IP_VS_SYNPROXY_MSS_MASK;
+       data |= opts->sack_ok << IP_VS_SYNPROXY_SACKOK_BIT;
+       data |= opts->tstamp_ok << IP_VS_SYNPROXY_TSOK_BIT;
+       data |= ((opts->snd_wscale & 0x0f) << IP_VS_SYNPROXY_SND_WSCALE_BITS);
+
+       return secure_tcp_syn_cookie(iph->saddr, iph->daddr,
+                                    th->source, th->dest, ntohl(th->seq),
+                                    jiffies / (HZ * 60), data);
+}
+EXPORT_SYMBOL(ip_vs_synproxy_cookie_v4_init_sequence);
+
+
+/*
+ * when ip_vs_synproxy_cookie_v4_init_sequence is used, we check
+ * cookie as follow:
+ *  1. mssind check.
+ *  2. get sack/timestamp/wscale options.
+ */
+int ip_vs_synproxy_v4_cookie_check(struct sk_buff * skb, __u32 cookie, 
+                             struct ip_vs_synproxy_opt * opt) 
+{
+       const struct iphdr *iph = ip_hdr(skb);
+       const struct tcphdr *th = tcp_hdr(skb);
+       __u32 seq = ntohl(th->seq) - 1;
+       __u32 mssind;
+       int   ret = 0;
+       __u32 res = check_tcp_syn_cookie(cookie, iph->saddr, iph->daddr,
+                                        th->source, th->dest, seq,
+                                        jiffies / (HZ * 60),
+                                        COUNTER_TRIES);
+
+       if(res == (__u32)-1) /* count is invalid, jiffies' >> jiffies */
+               goto out;
+
+       mssind = res & IP_VS_SYNPROXY_MSS_MASK;
+
+       memset(opt, 0, sizeof(struct ip_vs_synproxy_opt));
+
+       if (mssind < NUM_MSS) {
+               opt->mss_clamp = msstab[mssind] + 1;
+               opt->sack_ok = (res & IP_VS_SYNPROXY_SACKOK_MASK) >> 
+                                       IP_VS_SYNPROXY_SACKOK_BIT;
+               opt->tstamp_ok = (res & IP_VS_SYNPROXY_TSOK_MASK) >> 
+                                       IP_VS_SYNPROXY_TSOK_BIT;
+               opt->snd_wscale = (res & IP_VS_SYNPROXY_SND_WSCALE_MASK) >> 
+                                       IP_VS_SYNPROXY_SND_WSCALE_BITS;
+                if (opt->snd_wscale > 0 && 
+                   opt->snd_wscale <= IP_VS_SYNPROXY_WSCALE_MAX)
+                        opt->wscale_ok = 1;
+                else if (opt->snd_wscale == 0)
+                        opt->wscale_ok = 0;
+                else
+                        goto out;
+
+               ret = 1;
+       }
+
+out:   return ret;
+}
+EXPORT_SYMBOL(ip_vs_synproxy_v4_cookie_check);
diff --git a/net/ipv6/syncookies.c b/net/ipv6/syncookies.c
index 6b6ae91..005e6df 100644
--- a/net/ipv6/syncookies.c
+++ b/net/ipv6/syncookies.c
@@ -20,6 +20,7 @@
 #include <linux/kernel.h>
 #include <net/ipv6.h>
 #include <net/tcp.h>
+#include <net/ip_vs_synproxy.h>
 
 extern int sysctl_tcp_syncookies;
 extern __u32 syncookie_secret[2][16-4+SHA_DIGEST_WORDS];
@@ -279,3 +280,87 @@ out_free:
 	return NULL;
 }
 
+/*
+ * Generate a syncookie for ip_vs module. 
+ * Besides mss, we store additional tcp options in cookie "data".
+ * 
+ * Cookie "data" format: 
+ * |[21][20][19-16][15-0]|
+ * [21] SACKOK
+ * [20] TimeStampOK
+ * [19-16] snd_wscale
+ * [15-0] MSSIND 
+ */
+__u32 ip_vs_synproxy_cookie_v6_init_sequence(struct sk_buff *skb, 
+                                            struct ip_vs_synproxy_opt *opts) 
+{
+       struct ipv6hdr *iph = ipv6_hdr(skb);
+       const struct tcphdr *th = tcp_hdr(skb);
+       int mssind;
+       const __u16 mss = opts->mss_clamp;
+       __u32 data = 0;
+
+       /* XXX sort msstab[] by probability?  Binary search? */
+       for (mssind = 0; mss > msstab[mssind + 1]; mssind++)
+               ;
+       opts->mss_clamp = msstab[mssind] + 1;
+
+       data = mssind & IP_VS_SYNPROXY_MSS_MASK;
+       data |= opts->sack_ok << IP_VS_SYNPROXY_SACKOK_BIT;
+       data |= opts->tstamp_ok << IP_VS_SYNPROXY_TSOK_BIT;
+       data |= ((opts->snd_wscale & 0x0f) << IP_VS_SYNPROXY_SND_WSCALE_BITS);
+
+       return secure_tcp_syn_cookie(&iph->saddr, &iph->daddr,
+                                    th->source, th->dest, ntohl(th->seq),
+                                    jiffies / (HZ * 60), data);
+}
+EXPORT_SYMBOL(ip_vs_synproxy_cookie_v6_init_sequence);
+
+/*
+ * when ip_vs_synproxy_cookie_v6_init_sequence is used, we check
+ * cookie as follow:
+ *  1. mssind check.
+ *  2. get sack/timestamp/wscale options.
+ */
+int ip_vs_synproxy_v6_cookie_check(struct sk_buff * skb, __u32 cookie, 
+                             struct ip_vs_synproxy_opt * opt) 
+{
+       struct ipv6hdr *iph = ipv6_hdr(skb);
+       const struct tcphdr *th = tcp_hdr(skb);
+       __u32 seq = ntohl(th->seq) - 1;
+       __u32 mssind;
+       int   ret = 0;
+       __u32 res = check_tcp_syn_cookie(cookie, &iph->saddr, &iph->daddr,
+                                        th->source, th->dest, seq,
+                                        jiffies / (HZ * 60),
+                                        COUNTER_TRIES);
+
+       if(res == (__u32)-1) /* count is invalid, jiffies' >> jiffies */
+               goto out;
+
+       mssind = res & IP_VS_SYNPROXY_MSS_MASK;
+
+       memset(opt, 0, sizeof(struct ip_vs_synproxy_opt));
+
+       if (mssind < NUM_MSS) {
+               opt->mss_clamp = msstab[mssind] + 1;
+               opt->sack_ok = (res & IP_VS_SYNPROXY_SACKOK_MASK) >> 
+                                       IP_VS_SYNPROXY_SACKOK_BIT;
+               opt->tstamp_ok = (res & IP_VS_SYNPROXY_TSOK_MASK) >> 
+                                       IP_VS_SYNPROXY_TSOK_BIT;
+               opt->snd_wscale = (res & IP_VS_SYNPROXY_SND_WSCALE_MASK) >> 
+                                       IP_VS_SYNPROXY_SND_WSCALE_BITS;
+                if (opt->snd_wscale > 0 && 
+                   opt->snd_wscale <= IP_VS_SYNPROXY_WSCALE_MAX)
+                        opt->wscale_ok = 1;
+                else if (opt->snd_wscale == 0)
+                        opt->wscale_ok = 0;
+                else
+                        goto out;
+
+               ret = 1;
+       }
+
+out:   return ret;
+}
+EXPORT_SYMBOL(ip_vs_synproxy_v6_cookie_check);
diff --git a/net/netfilter/ipvs/Kconfig b/net/netfilter/ipvs/Kconfig
index 79a6980..e1e4401 100644
--- a/net/netfilter/ipvs/Kconfig
+++ b/net/netfilter/ipvs/Kconfig
@@ -43,8 +43,8 @@ config	IP_VS_DEBUG
 
 config	IP_VS_TAB_BITS
 	int "IPVS connection table size (the Nth power of 2)"
-	range 8 20
-	default 12
+	range 8 22
+	default 22
 	---help---
 	  The IPVS connection hash table uses the chaining scheme to handle
 	  hash collisions. Using a big IPVS connection hash table will greatly
diff --git a/net/netfilter/ipvs/Makefile b/net/netfilter/ipvs/Makefile
index 73a46fe..f7493c5 100644
--- a/net/netfilter/ipvs/Makefile
+++ b/net/netfilter/ipvs/Makefile
@@ -8,9 +8,10 @@ ip_vs_proto-objs-$(CONFIG_IP_VS_PROTO_TCP) += ip_vs_proto_tcp.o
 ip_vs_proto-objs-$(CONFIG_IP_VS_PROTO_UDP) += ip_vs_proto_udp.o
 ip_vs_proto-objs-$(CONFIG_IP_VS_PROTO_AH_ESP) += ip_vs_proto_ah_esp.o
 
-ip_vs-objs :=	ip_vs_conn.o ip_vs_core.o ip_vs_ctl.o ip_vs_sched.o	   \
-		ip_vs_xmit.o ip_vs_app.o ip_vs_sync.o	   		   \
-		ip_vs_est.o ip_vs_proto.o 				   \
+ip_vs-objs :=	ip_vs_conn.o ip_vs_core.o ip_vs_ctl.o ip_vs_sched.o   \
+		ip_vs_xmit.o ip_vs_app.o ip_vs_sync.o                 \
+		ip_vs_proto.o                                         \
+		ip_vs_synproxy.o ip_vs_stats.o                        \
 		$(ip_vs_proto-objs-y)
 
 
diff --git a/net/netfilter/ipvs/ip_vs_conn.c b/net/netfilter/ipvs/ip_vs_conn.c
index 5a99f81..051afb8 100644
--- a/net/netfilter/ipvs/ip_vs_conn.c
+++ b/net/netfilter/ipvs/ip_vs_conn.c
@@ -20,6 +20,7 @@
  *
  * Changes:
  *
+ *	          Yu Bo        <yubo@xiaomi.com>
  */
 
 #define KMSG_COMPONENT "IPVS"
@@ -60,7 +61,7 @@ static unsigned int ip_vs_conn_rnd;
 /*
  *  Fine locking granularity for big connection hash table
  */
-#define CT_LOCKARRAY_BITS  4
+#define CT_LOCKARRAY_BITS  8
 #define CT_LOCKARRAY_SIZE  (1<<CT_LOCKARRAY_BITS)
 #define CT_LOCKARRAY_MASK  (CT_LOCKARRAY_SIZE-1)
 
@@ -117,41 +118,85 @@ static inline void ct_write_unlock_bh(unsigned key)
 /*
  *	Returns hash value for IPVS connection entry
  */
-static unsigned int ip_vs_conn_hashkey(int af, unsigned proto,
-				       const union nf_inet_addr *addr,
-				       __be16 port)
+static unsigned int ip_vs_conn_hashkey(int af, const union nf_inet_addr *s_addr,
+				       __be16 s_port,
+				       const union nf_inet_addr *d_addr,
+				       __be16 d_port)
 {
 #ifdef CONFIG_IP_VS_IPV6
 	if (af == AF_INET6)
-		return jhash_3words(jhash(addr, 16, ip_vs_conn_rnd),
-				    (__force u32)port, proto, ip_vs_conn_rnd)
-			& IP_VS_CONN_TAB_MASK;
+		return jhash_3words(jhash(s_addr, 16, ip_vs_conn_rnd),
+				    jhash(d_addr, 16, ip_vs_conn_rnd),
+				    ((__force u32) s_port) << 16 | (__force u32)
+				    d_port, ip_vs_conn_rnd)
+		    & IP_VS_CONN_TAB_MASK;
 #endif
-	return jhash_3words((__force u32)addr->ip, (__force u32)port, proto,
+	return jhash_3words((__force u32) s_addr->ip, (__force u32) d_addr->ip,
+			    ((__force u32) s_port) << 16 | (__force u32) d_port,
 			    ip_vs_conn_rnd)
-		& IP_VS_CONN_TAB_MASK;
+	    & IP_VS_CONN_TAB_MASK;
 }
 
-
 /*
- *	Hashes ip_vs_conn in ip_vs_conn_tab by proto,addr,port.
- *	returns bool success.
+ * Lock two buckets of ip_vs_conn_tab
  */
-static inline int ip_vs_conn_hash(struct ip_vs_conn *cp)
+static inline void ip_vs_conn_lock2(unsigned ihash, unsigned ohash)
 {
-	unsigned hash;
-	int ret;
+	unsigned ilock, olock;
+
+	ilock = ihash & CT_LOCKARRAY_MASK;
+	olock = ohash & CT_LOCKARRAY_MASK;
+
+	/* lock the conntab bucket */
+	if (ilock < olock) {
+		ct_write_lock(ihash);
+		ct_write_lock(ohash);
+	} else if (ilock > olock) {
+		ct_write_lock(ohash);
+		ct_write_lock(ihash);
+	} else {
+		ct_write_lock(ihash);
+	}
+}
 
-	if (cp->flags & IP_VS_CONN_F_ONE_PACKET)
-		return 0;
+/*
+ * Unlock two buckets of ip_vs_conn_tab
+ */
+static inline void ip_vs_conn_unlock2(unsigned ihash, unsigned ohash)
+{
+	unsigned ilock, olock;
+
+	ilock = ihash & CT_LOCKARRAY_MASK;
+	olock = ohash & CT_LOCKARRAY_MASK;
+
+	/* lock the conntab bucket */
+	if (ilock < olock) {
+		ct_write_unlock(ohash);
+		ct_write_unlock(ihash);
+	} else if (ilock > olock) {
+		ct_write_unlock(ihash);
+		ct_write_unlock(ohash);
+	} else {
+		ct_write_unlock(ihash);
+	}
+}
 
-	/* Hash by protocol, client address and port */
-	hash = ip_vs_conn_hashkey(cp->af, cp->protocol, &cp->caddr, cp->cport);
+/*
+ *      Hashed ip_vs_conn into ip_vs_conn_tab
+ *	returns bool success.
+ */
 
-	ct_write_lock(hash);
+static inline int __ip_vs_conn_hash(struct ip_vs_conn *cp, unsigned ihash,
+				    unsigned ohash)
+{
+	struct ip_vs_conn_idx *ci_idx, *co_idx;
+	int ret;
 
 	if (!(cp->flags & IP_VS_CONN_F_HASHED)) {
-		list_add(&cp->c_list, &ip_vs_conn_tab[hash]);
+		ci_idx = cp->in_idx;
+		co_idx = cp->out_idx;
+		list_add(&ci_idx->c_list, &ip_vs_conn_tab[ihash]);
+		list_add(&co_idx->c_list, &ip_vs_conn_tab[ohash]);
 		cp->flags |= IP_VS_CONN_F_HASHED;
 		atomic_inc(&cp->refcnt);
 		ret = 1;
@@ -161,7 +206,39 @@ static inline int ip_vs_conn_hash(struct ip_vs_conn *cp)
 		ret = 0;
 	}
 
-	ct_write_unlock(hash);
+	return ret;
+}
+
+/*
+ *	Hashed ip_vs_conn in two buckets of ip_vs_conn_tab
+ *	by caddr/cport/vaddr/vport and raddr/rport/laddr/lport,
+ *	returns bool success.
+ */
+static inline int ip_vs_conn_hash(struct ip_vs_conn *cp)
+{
+	unsigned ihash, ohash;
+	int ret;
+
+	if (cp->flags & IP_VS_CONN_F_ONE_PACKET)
+		return 0;
+
+	/*OUTside2INside: hashed by client address and port, virtual address and port */
+	ihash =
+	    ip_vs_conn_hashkey(cp->af, &cp->caddr, cp->cport, &cp->vaddr,
+			       cp->vport);
+	/*INside2OUTside: hashed by destination address and port, local address and port */
+	ohash =
+	    ip_vs_conn_hashkey(cp->af, &cp->daddr, cp->dport, &cp->laddr,
+			       cp->lport);
+
+	/* locked */
+	ip_vs_conn_lock2(ihash, ohash);
+
+	/* hashed */
+	ret = __ip_vs_conn_hash(cp, ihash, ohash);
+
+	/* unlocked */
+	ip_vs_conn_unlock2(ihash, ohash);
 
 	return ret;
 }
@@ -169,27 +246,43 @@ static inline int ip_vs_conn_hash(struct ip_vs_conn *cp)
 
 /*
  *	UNhashes ip_vs_conn from ip_vs_conn_tab.
+ *	cp->refcnt must be equal 2,
  *	returns bool success.
  */
 static inline int ip_vs_conn_unhash(struct ip_vs_conn *cp)
 {
-	unsigned hash;
+	unsigned ihash, ohash;
+	struct ip_vs_conn_idx *ci_idx, *co_idx;
 	int ret;
 
-	/* unhash it and decrease its reference counter */
-	hash = ip_vs_conn_hashkey(cp->af, cp->protocol, &cp->caddr, cp->cport);
-
-	ct_write_lock(hash);
-
-	if (cp->flags & IP_VS_CONN_F_HASHED) {
-		list_del(&cp->c_list);
+	/* OUTside2INside: unhash it and decrease its reference counter */
+	ihash =
+	    ip_vs_conn_hashkey(cp->af, &cp->caddr, cp->cport, &cp->vaddr,
+			       cp->vport);
+	/* INside2OUTside: unhash it and decrease its reference counter */
+	ohash =
+	    ip_vs_conn_hashkey(cp->af, &cp->daddr, cp->dport, &cp->laddr,
+			       cp->lport);
+
+	/* locked */
+	ip_vs_conn_lock2(ihash, ohash);
+
+	/* unhashed */
+	if ((cp->flags & IP_VS_CONN_F_HASHED)
+	    && (atomic_read(&cp->refcnt) == 2)) {
+		ci_idx = cp->in_idx;
+		co_idx = cp->out_idx;
+		list_del(&ci_idx->c_list);
+		list_del(&co_idx->c_list);
 		cp->flags &= ~IP_VS_CONN_F_HASHED;
 		atomic_dec(&cp->refcnt);
 		ret = 1;
-	} else
+	} else {
 		ret = 0;
+	}
 
-	ct_write_unlock(hash);
+	/* unlocked */
+	ip_vs_conn_unlock2(ihash, ohash);
 
 	return ret;
 }
@@ -197,30 +290,32 @@ static inline int ip_vs_conn_unhash(struct ip_vs_conn *cp)
 
 /*
  *  Gets ip_vs_conn associated with supplied parameters in the ip_vs_conn_tab.
- *  Called for pkts coming from OUTside-to-INside.
- *	s_addr, s_port: pkt source address (foreign host)
- *	d_addr, d_port: pkt dest address (load balancer)
+ *  Return director: OUTside-to-INside or INside-to-OUTside in res_dir.
+ *	s_addr, s_port: pkt source address (foreign host/realserver)
+ *	d_addr, d_port: pkt dest address (virtual address/local address)
  */
-static inline struct ip_vs_conn *__ip_vs_conn_in_get
-(int af, int protocol, const union nf_inet_addr *s_addr, __be16 s_port,
- const union nf_inet_addr *d_addr, __be16 d_port)
-{
+static inline struct ip_vs_conn *__ip_vs_conn_get
+    (int af, int protocol, const union nf_inet_addr *s_addr, __be16 s_port,
+     const union nf_inet_addr *d_addr, __be16 d_port, int *res_dir) {
 	unsigned hash;
 	struct ip_vs_conn *cp;
+	struct ip_vs_conn_idx *cidx;
 
-	hash = ip_vs_conn_hashkey(af, protocol, s_addr, s_port);
+	hash = ip_vs_conn_hashkey(af, s_addr, s_port, d_addr, d_port);
 
 	ct_read_lock(hash);
 
-	list_for_each_entry(cp, &ip_vs_conn_tab[hash], c_list) {
-		if (cp->af == af &&
-		    ip_vs_addr_equal(af, s_addr, &cp->caddr) &&
-		    ip_vs_addr_equal(af, d_addr, &cp->vaddr) &&
-		    s_port == cp->cport && d_port == cp->vport &&
+	list_for_each_entry(cidx, &ip_vs_conn_tab[hash], c_list) {
+		cp = cidx->cp;
+		if (cidx->af == af &&
+		    ip_vs_addr_equal(af, s_addr, &cidx->s_addr) &&
+		    ip_vs_addr_equal(af, d_addr, &cidx->d_addr) &&
+		    s_port == cidx->s_port && d_port == cidx->d_port &&
 		    ((!s_port) ^ (!(cp->flags & IP_VS_CONN_F_NO_CPORT))) &&
-		    protocol == cp->protocol) {
+		    protocol == cidx->protocol) {
 			/* HIT */
 			atomic_inc(&cp->refcnt);
+			*res_dir = cidx->flags & IP_VS_CIDX_F_DIR_MASK;
 			ct_read_unlock(hash);
 			return cp;
 		}
@@ -231,18 +326,18 @@ static inline struct ip_vs_conn *__ip_vs_conn_in_get
 	return NULL;
 }
 
-struct ip_vs_conn *ip_vs_conn_in_get
-(int af, int protocol, const union nf_inet_addr *s_addr, __be16 s_port,
- const union nf_inet_addr *d_addr, __be16 d_port)
-{
+struct ip_vs_conn *ip_vs_conn_get
+    (int af, int protocol, const union nf_inet_addr *s_addr, __be16 s_port,
+     const union nf_inet_addr *d_addr, __be16 d_port, int *res_dir) {
 	struct ip_vs_conn *cp;
 
-	cp = __ip_vs_conn_in_get(af, protocol, s_addr, s_port, d_addr, d_port);
+	cp = __ip_vs_conn_get(af, protocol, s_addr, s_port, d_addr, d_port,
+			      res_dir);
 	if (!cp && atomic_read(&ip_vs_conn_no_cport_cnt))
-		cp = __ip_vs_conn_in_get(af, protocol, s_addr, 0, d_addr,
-					 d_port);
+		cp = __ip_vs_conn_get(af, protocol, s_addr, 0, d_addr, d_port,
+				      res_dir);
 
-	IP_VS_DBG_BUF(9, "lookup/in %s %s:%d->%s:%d %s\n",
+	IP_VS_DBG_BUF(21, "lookup %s %s:%d->%s:%d %s\n",
 		      ip_vs_proto_name(protocol),
 		      IP_VS_DBG_ADDR(af, s_addr), ntohs(s_port),
 		      IP_VS_DBG_ADDR(af, d_addr), ntohs(d_port),
@@ -257,22 +352,24 @@ struct ip_vs_conn *ip_vs_ct_in_get
  const union nf_inet_addr *d_addr, __be16 d_port)
 {
 	unsigned hash;
+	struct ip_vs_conn_idx *cidx;
 	struct ip_vs_conn *cp;
 
-	hash = ip_vs_conn_hashkey(af, protocol, s_addr, s_port);
+	hash = ip_vs_conn_hashkey(af, s_addr, s_port, d_addr, d_port);
 
 	ct_read_lock(hash);
 
-	list_for_each_entry(cp, &ip_vs_conn_tab[hash], c_list) {
-		if (cp->af == af &&
-		    ip_vs_addr_equal(af, s_addr, &cp->caddr) &&
+	list_for_each_entry(cidx, &ip_vs_conn_tab[hash], c_list) {
+		cp = cidx->cp;
+		if (cidx->af == af &&
+		    ip_vs_addr_equal(af, s_addr, &cidx->s_addr) &&
 		    /* protocol should only be IPPROTO_IP if
 		     * d_addr is a fwmark */
 		    ip_vs_addr_equal(protocol == IPPROTO_IP ? AF_UNSPEC : af,
-		                     d_addr, &cp->vaddr) &&
-		    s_port == cp->cport && d_port == cp->vport &&
+				     d_addr, &cidx->d_addr) &&
+		    s_port == cidx->s_port && d_port == cidx->d_port &&
 		    cp->flags & IP_VS_CONN_F_TEMPLATE &&
-		    protocol == cp->protocol) {
+		    protocol == cidx->protocol) {
 			/* HIT */
 			atomic_inc(&cp->refcnt);
 			goto out;
@@ -283,7 +380,7 @@ struct ip_vs_conn *ip_vs_ct_in_get
   out:
 	ct_read_unlock(hash);
 
-	IP_VS_DBG_BUF(9, "template lookup/in %s %s:%d->%s:%d %s\n",
+	IP_VS_DBG_BUF(21, "template lookup %s %s:%d->%s:%d %s\n",
 		      ip_vs_proto_name(protocol),
 		      IP_VS_DBG_ADDR(af, s_addr), ntohs(s_port),
 		      IP_VS_DBG_ADDR(af, d_addr), ntohs(d_port),
@@ -293,51 +390,6 @@ struct ip_vs_conn *ip_vs_ct_in_get
 }
 
 /*
- *  Gets ip_vs_conn associated with supplied parameters in the ip_vs_conn_tab.
- *  Called for pkts coming from inside-to-OUTside.
- *	s_addr, s_port: pkt source address (inside host)
- *	d_addr, d_port: pkt dest address (foreign host)
- */
-struct ip_vs_conn *ip_vs_conn_out_get
-(int af, int protocol, const union nf_inet_addr *s_addr, __be16 s_port,
- const union nf_inet_addr *d_addr, __be16 d_port)
-{
-	unsigned hash;
-	struct ip_vs_conn *cp, *ret=NULL;
-
-	/*
-	 *	Check for "full" addressed entries
-	 */
-	hash = ip_vs_conn_hashkey(af, protocol, d_addr, d_port);
-
-	ct_read_lock(hash);
-
-	list_for_each_entry(cp, &ip_vs_conn_tab[hash], c_list) {
-		if (cp->af == af &&
-		    ip_vs_addr_equal(af, d_addr, &cp->caddr) &&
-		    ip_vs_addr_equal(af, s_addr, &cp->daddr) &&
-		    d_port == cp->cport && s_port == cp->dport &&
-		    protocol == cp->protocol) {
-			/* HIT */
-			atomic_inc(&cp->refcnt);
-			ret = cp;
-			break;
-		}
-	}
-
-	ct_read_unlock(hash);
-
-	IP_VS_DBG_BUF(9, "lookup/out %s %s:%d->%s:%d %s\n",
-		      ip_vs_proto_name(protocol),
-		      IP_VS_DBG_ADDR(af, s_addr), ntohs(s_port),
-		      IP_VS_DBG_ADDR(af, d_addr), ntohs(d_port),
-		      ret ? "hit" : "not hit");
-
-	return ret;
-}
-
-
-/*
  *      Put back the conn and restart its timer with its timeout
  */
 void ip_vs_conn_put(struct ip_vs_conn *cp)
@@ -385,6 +437,10 @@ static inline void ip_vs_bind_xmit(struct ip_vs_conn *cp)
 		cp->packet_xmit = ip_vs_nat_xmit;
 		break;
 
+	case IP_VS_CONN_F_FULLNAT:
+		cp->packet_xmit = ip_vs_fnat_xmit;
+		break;
+
 	case IP_VS_CONN_F_TUNNEL:
 		cp->packet_xmit = ip_vs_tunnel_xmit;
 		break;
@@ -411,6 +467,10 @@ static inline void ip_vs_bind_xmit_v6(struct ip_vs_conn *cp)
 		cp->packet_xmit = ip_vs_nat_xmit_v6;
 		break;
 
+	case IP_VS_CONN_F_FULLNAT:
+		cp->packet_xmit = ip_vs_fnat_xmit_v6;
+		break;
+
 	case IP_VS_CONN_F_TUNNEL:
 		cp->packet_xmit = ip_vs_tunnel_xmit_v6;
 		break;
@@ -571,6 +631,185 @@ static inline void ip_vs_unbind_dest(struct ip_vs_conn *cp)
 	atomic_dec(&dest->refcnt);
 }
 
+/*
+ * get a local address from given virtual service
+ */
+static struct ip_vs_laddr *ip_vs_get_laddr(const union nf_inet_addr *caddr)
+{
+	struct ip_vs_laddr *local;
+	struct list_head *p, *q;
+	struct ip_vs_zone *zone;
+
+	zone = ip_vs_zone_get(caddr);
+	if (zone == NULL)
+		return NULL;
+	
+	write_lock(&zone->laddr_lock);
+	p = zone->curr_laddr;
+	p = p->next;
+	q = p;
+	do {
+		/* skip list head */
+		if (q == &zone->laddr_list) {
+			q = q->next;
+			continue;
+		}
+		local = list_entry(q, struct ip_vs_laddr, n_list);
+		goto out;
+	} while (q != p);
+	write_unlock(&zone->laddr_lock);
+	return NULL;
+
+      out:
+	zone->curr_laddr = q;
+	write_unlock(&zone->laddr_lock);
+	return local;
+}
+
+/*
+ *	Bind a connection entry with a local address
+ *	and hashed it in connection table.
+ *	Called just after a new connection entry is created and destination has binded.
+ *	returns bool success.
+ */
+static inline int ip_vs_hbind_laddr(struct ip_vs_conn *cp)
+{
+	struct ip_vs_laddr *local;
+	int ret = 0;
+	int remaining, i, tport, hit = 0;
+	unsigned ihash, ohash;
+	struct ip_vs_conn_idx *cidx;
+
+	/* fwd methods: not IP_VS_CONN_F_FULLNAT */
+	switch (IP_VS_FWD_METHOD(cp)) {
+	case IP_VS_CONN_F_MASQ:
+	case IP_VS_CONN_F_TUNNEL:
+	case IP_VS_CONN_F_DROUTE:
+	case IP_VS_CONN_F_LOCALNODE:
+	case IP_VS_CONN_F_BYPASS:
+		ip_vs_addr_copy(cp->af, &cp->out_idx->d_addr, &cp->caddr);
+		cp->out_idx->d_port = cp->cport;
+		ip_vs_addr_copy(cp->af, &cp->laddr, &cp->caddr);
+		cp->lport = cp->cport;
+		cp->local = NULL;
+		ip_vs_conn_hash(cp);
+		ret = 1;
+		goto out;
+	}
+
+	if (cp->flags & IP_VS_CONN_F_TEMPLATE) {
+		ip_vs_addr_copy(cp->af, &cp->out_idx->d_addr, &cp->caddr);
+		cp->out_idx->d_port = cp->cport;
+		ip_vs_addr_copy(cp->af, &cp->laddr, &cp->caddr);
+		cp->lport = cp->cport;
+		cp->local = NULL;
+		ip_vs_conn_hash(cp);
+		ret = 1;
+		goto out;
+	}
+	/*
+	 * fwd methods: IP_VS_CONN_F_FULLNAT
+	 */
+	/* choose a local address by round-robin */
+	local = ip_vs_get_laddr(&cp->caddr);
+	if (local != NULL) {
+		/*OUTside2INside: hashed by client address and port, virtual address and port */
+		ihash =
+		    ip_vs_conn_hashkey(cp->af, &cp->caddr, cp->cport,
+				       &cp->vaddr, cp->vport);
+
+		/* increase the refcnt counter of the local address */
+		ip_vs_laddr_hold(local);
+		ip_vs_addr_copy(cp->af, &cp->out_idx->d_addr, &local->addr);
+		ip_vs_addr_copy(cp->af, &cp->laddr, &local->addr);
+		remaining = sysctl_ip_vs_lport_max - sysctl_ip_vs_lport_min + 1;
+		for (i = 0; i < sysctl_ip_vs_lport_tries; i++) {
+			/* choose a port */
+			tport =
+			    sysctl_ip_vs_lport_min +
+			    atomic64_inc_return(&local->port) % remaining;
+			cp->out_idx->d_port = cp->lport = htons(tport);
+
+			/* init hit everytime before lookup the tuple */
+			hit = 0;
+
+			/*INside2OUTside: hashed by destination address and port, local address and port */
+			ohash =
+			    ip_vs_conn_hashkey(cp->af, &cp->daddr, cp->dport,
+					       &cp->laddr, cp->lport);
+			/* lock the conntab bucket */
+			ip_vs_conn_lock2(ihash, ohash);
+			/*
+			 * check local address and port is valid by lookup connection table
+			 */
+			list_for_each_entry(cidx, &ip_vs_conn_tab[ohash],
+					    c_list) {
+				if (cidx->af == cp->af
+				    && ip_vs_addr_equal(cp->af, &cp->daddr,
+							&cidx->s_addr)
+				    && ip_vs_addr_equal(cp->af, &cp->laddr,
+							&cidx->d_addr)
+				    && cp->dport == cidx->s_port
+				    && cp->lport == cidx->d_port
+				    && cp->protocol == cidx->protocol) {
+					/* HIT */
+					atomic64_inc(&local->port_conflict);
+					hit = 1;
+					break;
+				}
+			}
+			if (hit == 0) {
+				cp->local = local;
+				/* hashed */
+				__ip_vs_conn_hash(cp, ihash, ohash);
+				ip_vs_conn_unlock2(ihash, ohash);
+				atomic_inc(&local->conn_counts);
+				ret = 1;
+				goto out;
+			}
+			ip_vs_conn_unlock2(ihash, ohash);
+		}
+		if (ret == 0) {
+			ip_vs_laddr_put(local);
+		}
+	}
+	ret = 0;
+
+      out:
+	return ret;
+}
+
+/*
+ *	Unbind a connection entry with its local address
+ *	Called by the ip_vs_conn_expire function.
+ */
+static inline void ip_vs_unbind_laddr(struct ip_vs_conn *cp)
+{
+	struct ip_vs_laddr *local = cp->local;
+
+	if (!local)
+		return;
+
+	IP_VS_DBG_BUF(7, "Unbind-laddr %s c:%s:%d v:%s:%d l:%s:%d "
+		      "d:%s:%d fwd:%c s:%u conn->flags:%X conn->refcnt:%d "
+		      "local->refcnt:%d\n",
+		      ip_vs_proto_name(cp->protocol),
+		      IP_VS_DBG_ADDR(cp->af, &cp->caddr), ntohs(cp->cport),
+		      IP_VS_DBG_ADDR(cp->af, &cp->vaddr), ntohs(cp->vport),
+		      IP_VS_DBG_ADDR(cp->af, &cp->laddr), ntohs(cp->lport),
+		      IP_VS_DBG_ADDR(cp->af, &cp->daddr), ntohs(cp->dport),
+		      ip_vs_fwd_tag(cp), cp->state,
+		      cp->flags, atomic_read(&cp->refcnt),
+		      atomic_read(&local->refcnt));
+
+	/* Update the connection counters */
+	atomic_dec(&local->conn_counts);
+
+	/*
+	 * Simply decrease the refcnt of the local address;
+	 */
+	ip_vs_laddr_put(local);
+}
 
 /*
  *	Checking if the destination of a connection template is available.
@@ -590,12 +829,14 @@ int ip_vs_check_template(struct ip_vs_conn *ct)
 	     (atomic_read(&dest->weight) == 0))) {
 		IP_VS_DBG_BUF(9, "check_template: dest not available for "
 			      "protocol %s s:%s:%d v:%s:%d "
-			      "-> d:%s:%d\n",
+			      "-> l:%s:%d d:%s:%d\n",
 			      ip_vs_proto_name(ct->protocol),
 			      IP_VS_DBG_ADDR(ct->af, &ct->caddr),
 			      ntohs(ct->cport),
 			      IP_VS_DBG_ADDR(ct->af, &ct->vaddr),
 			      ntohs(ct->vport),
+			      IP_VS_DBG_ADDR(ct->af, &ct->laddr),
+			      ntohs(ct->lport),
 			      IP_VS_DBG_ADDR(ct->af, &ct->daddr),
 			      ntohs(ct->dport));
 
@@ -606,6 +847,7 @@ int ip_vs_check_template(struct ip_vs_conn *ct)
 			if (ip_vs_conn_unhash(ct)) {
 				ct->dport = htons(0xffff);
 				ct->vport = htons(0xffff);
+				ct->lport = 0;
 				ct->cport = 0;
 				ip_vs_conn_hash(ct);
 			}
@@ -621,19 +863,73 @@ int ip_vs_check_template(struct ip_vs_conn *ct)
 	return 1;
 }
 
+/* Warning: only be allowed call in ip_vs_conn_new */
+static void ip_vs_conn_del(struct ip_vs_conn *cp)
+{
+	if (cp == NULL)
+		return;
+
+	/* delete the timer if it is activated by other users */
+	if (timer_pending(&cp->timer))
+		del_timer(&cp->timer);
+
+	/* does anybody control me? */
+	if (cp->control)
+		ip_vs_control_del(cp);
+
+	if (unlikely(cp->app != NULL))
+		ip_vs_unbind_app(cp);
+	ip_vs_unbind_dest(cp);
+	ip_vs_unbind_laddr(cp);
+	if (cp->flags & IP_VS_CONN_F_NO_CPORT)
+		atomic_dec(&ip_vs_conn_no_cport_cnt);
+	atomic_dec(&ip_vs_conn_count);
+
+	kmem_cache_free(ip_vs_conn_cachep, cp);
+	cp = NULL;
+}
+
 static void ip_vs_conn_expire(unsigned long data)
 {
 	struct ip_vs_conn *cp = (struct ip_vs_conn *)data;
+	struct sk_buff *tmp_skb = NULL;
+	struct ip_vs_protocol *pp = ip_vs_proto_get(cp->protocol);
 
-	cp->timeout = 60*HZ;
+	/*
+	 * Set proper timeout.
+	 */
+	if ((pp != NULL) && (pp->timeout_table != NULL)) {
+		cp->timeout = pp->timeout_table[cp->state];
+	} else {
+		cp->timeout = 60 * HZ;
+	}
 
 	/*
-	 *	hey, I'm using it
+	 *      hey, I'm using it
 	 */
 	atomic_inc(&cp->refcnt);
 
 	/*
-	 *	do I control anybody?
+	 * Retransmit syn packet to rs.
+	 * We just check syn_skb is not NULL, as syn_skb 
+	 * is stored only if syn-proxy is enabled.
+	 */
+	spin_lock(&cp->lock);
+	if (cp->syn_skb != NULL && atomic_read(&cp->syn_retry_max) > 0) {
+		atomic_dec(&cp->syn_retry_max);
+		if (cp->packet_xmit) {
+			tmp_skb = skb_copy(cp->syn_skb, GFP_ATOMIC);
+			cp->packet_xmit(tmp_skb, cp, pp);
+		}
+		/* statistics */
+		IP_VS_INC_ESTATS(ip_vs_esmib, SYNPROXY_RS_ERROR);
+		spin_unlock(&cp->lock);
+		goto expire_later;
+	}
+	spin_unlock(&cp->lock);
+
+	/*
+	 *      do I control anybody?
 	 */
 	if (atomic_read(&cp->n_control))
 		goto expire_later;
@@ -656,13 +952,32 @@ static void ip_vs_conn_expire(unsigned long data)
 		if (cp->control)
 			ip_vs_control_del(cp);
 
+		if (pp->conn_expire_handler)
+			pp->conn_expire_handler(pp, cp);
+
 		if (unlikely(cp->app != NULL))
 			ip_vs_unbind_app(cp);
 		ip_vs_unbind_dest(cp);
+		ip_vs_unbind_laddr(cp);
 		if (cp->flags & IP_VS_CONN_F_NO_CPORT)
 			atomic_dec(&ip_vs_conn_no_cport_cnt);
 		atomic_dec(&ip_vs_conn_count);
 
+		/* free stored ack packet */
+		while ((tmp_skb = skb_dequeue(&cp->ack_skb)) != NULL) {
+			kfree_skb(tmp_skb);
+			tmp_skb = NULL;
+		}
+
+		/* free stored syn skb */
+		if ((tmp_skb = xchg(&cp->syn_skb, NULL)) != NULL) {
+			kfree_skb(tmp_skb);
+			tmp_skb = NULL;
+		}
+
+		if (cp->indev != NULL)
+			dev_put(cp->indev);
+
 		kmem_cache_free(ip_vs_conn_cachep, cp);
 		return;
 	}
@@ -672,8 +987,7 @@ static void ip_vs_conn_expire(unsigned long data)
 
   expire_later:
 	IP_VS_DBG(7, "delayed: conn->refcnt-1=%d conn->n_control=%d\n",
-		  atomic_read(&cp->refcnt)-1,
-		  atomic_read(&cp->n_control));
+		  atomic_read(&cp->refcnt) - 1, atomic_read(&cp->n_control));
 
 	ip_vs_conn_put(cp);
 }
@@ -689,14 +1003,17 @@ void ip_vs_conn_expire_now(struct ip_vs_conn *cp)
 /*
  *	Create a new connection entry and hash it into the ip_vs_conn_tab
  */
-struct ip_vs_conn *
-ip_vs_conn_new(int af, int proto, const union nf_inet_addr *caddr, __be16 cport,
-	       const union nf_inet_addr *vaddr, __be16 vport,
-	       const union nf_inet_addr *daddr, __be16 dport, unsigned flags,
-	       struct ip_vs_dest *dest)
+struct ip_vs_conn *ip_vs_conn_new(int af, int proto,
+				  const union nf_inet_addr *caddr, __be16 cport,
+				  const union nf_inet_addr *vaddr, __be16 vport,
+				  const union nf_inet_addr *daddr, __be16 dport,
+				  unsigned flags, struct ip_vs_dest *dest,
+				  struct sk_buff *skb, int is_synproxy_on)
 {
 	struct ip_vs_conn *cp;
 	struct ip_vs_protocol *pp = ip_vs_proto_get(proto);
+	struct ip_vs_conn_idx *ci_idx, *co_idx;
+	struct tcphdr _tcph, *th;
 
 	cp = kmem_cache_zalloc(ip_vs_conn_cachep, GFP_ATOMIC);
 	if (cp == NULL) {
@@ -704,7 +1021,35 @@ ip_vs_conn_new(int af, int proto, const union nf_inet_addr *caddr, __be16 cport,
 		return NULL;
 	}
 
-	INIT_LIST_HEAD(&cp->c_list);
+	/* init connection index of OUTside2INside */
+	ci_idx =
+	    (struct ip_vs_conn_idx *)(((__u8 *) cp) +
+				      sizeof(struct ip_vs_conn));
+	INIT_LIST_HEAD(&ci_idx->c_list);
+	ci_idx->af = af;
+	ci_idx->protocol = proto;
+	ip_vs_addr_copy(af, &ci_idx->s_addr, caddr);
+	ci_idx->s_port = cport;
+	ip_vs_addr_copy(af, &ci_idx->d_addr, vaddr);
+	ci_idx->d_port = vport;
+	ci_idx->flags |= IP_VS_CIDX_F_OUT2IN;
+	ci_idx->cp = cp;
+
+	/* init connection index of INside2OUTside */
+	co_idx =
+	    (struct ip_vs_conn_idx *)(((__u8 *) cp) +
+				      sizeof(struct ip_vs_conn) +
+				      sizeof(struct ip_vs_conn_idx));
+	INIT_LIST_HEAD(&co_idx->c_list);
+	co_idx->af = af;
+	co_idx->protocol = proto;
+	ip_vs_addr_copy(proto == IPPROTO_IP ? AF_UNSPEC : af,
+			&co_idx->s_addr, daddr);
+	co_idx->s_port = dport;
+	co_idx->flags |= IP_VS_CIDX_F_IN2OUT;
+	co_idx->cp = cp;
+
+	/* now init connection */
 	setup_timer(&cp->timer, ip_vs_conn_expire, (unsigned long)cp);
 	cp->af		   = af;
 	cp->protocol	   = proto;
@@ -718,6 +1063,8 @@ ip_vs_conn_new(int af, int proto, const union nf_inet_addr *caddr, __be16 cport,
 	cp->dport          = dport;
 	cp->flags	   = flags;
 	spin_lock_init(&cp->lock);
+	cp->in_idx = ci_idx;
+	cp->out_idx = co_idx;
 
 	/*
 	 * Set the entry is referenced by the current thread before hashing
@@ -751,8 +1098,54 @@ ip_vs_conn_new(int af, int proto, const union nf_inet_addr *caddr, __be16 cport,
 	if (unlikely(pp && atomic_read(&pp->appcnt)))
 		ip_vs_bind_app(cp, pp);
 
-	/* Hash it in the ip_vs_conn_tab finally */
-	ip_vs_conn_hash(cp);
+	/* Set syn-proxy members 
+	 * Set cp->flag manually to avoid svn->flags change when 
+	 * ack_skb is on the way
+	 */
+	skb_queue_head_init(&cp->ack_skb);
+	atomic_set(&cp->syn_retry_max, 0);
+	if (is_synproxy_on == 1 && skb != NULL) {
+		unsigned int tcphoff;
+
+#ifdef CONFIG_IP_VS_IPV6
+		if (af == AF_INET6)
+			tcphoff = sizeof(struct ipv6hdr);
+		else
+#endif
+			tcphoff = ip_hdr(skb)->ihl * 4;
+		th = skb_header_pointer(skb, tcphoff, sizeof(_tcph), &_tcph);
+		if (th == NULL) {
+			IP_VS_ERR_RL("%s(): get tcphdr failed\n", __func__);
+			ip_vs_conn_del(cp);
+			return NULL;
+		}
+		/* Set syn-proxy flag */
+		cp->flags |= IP_VS_CONN_F_SYNPROXY;
+
+		/* Save ack packet */
+		skb_queue_tail(&cp->ack_skb, skb);
+		/* Save ack_seq - 1 */
+		cp->syn_proxy_seq.init_seq =
+		    htonl((__u32) ((htonl(th->ack_seq) - 1)));
+		/* Save ack_seq */
+		cp->fnat_seq.fdata_seq = htonl(th->ack_seq);
+		/* Use IP_VS_TCP_S_SYN_SENT for syn */
+		cp->timeout = pp->timeout_table[cp->state =
+						IP_VS_TCP_S_SYN_SENT];
+	} else {
+		/* Unset syn-proxy flag */
+		cp->flags &= ~IP_VS_CONN_F_SYNPROXY;
+	}
+
+	/*
+	 * bind the connection with a local address
+	 * and hash it in the ip_vs_conn_tab finally.
+	 */
+	if (unlikely(ip_vs_hbind_laddr(cp) == 0)) {
+		IP_VS_ERR_RL("bind local address: no port available\n");
+		ip_vs_conn_del(cp);
+		return NULL;
+	}
 
 	return cp;
 }
@@ -766,14 +1159,14 @@ ip_vs_conn_new(int af, int proto, const union nf_inet_addr *caddr, __be16 cport,
 static void *ip_vs_conn_array(struct seq_file *seq, loff_t pos)
 {
 	int idx;
-	struct ip_vs_conn *cp;
+	struct ip_vs_conn_idx *cidx;
 
 	for(idx = 0; idx < IP_VS_CONN_TAB_SIZE; idx++) {
 		ct_read_lock_bh(idx);
-		list_for_each_entry(cp, &ip_vs_conn_tab[idx], c_list) {
-			if (pos-- == 0) {
+		list_for_each_entry(cidx, &ip_vs_conn_tab[idx], c_list) {
+			if ((cidx->flags & IP_VS_CIDX_F_OUT2IN) && (pos-- == 0)) {
 				seq->private = &ip_vs_conn_tab[idx];
-				return cp;
+				return cidx->cp;
 			}
 		}
 		ct_read_unlock_bh(idx);
@@ -792,24 +1185,32 @@ static void *ip_vs_conn_seq_next(struct seq_file *seq, void *v, loff_t *pos)
 {
 	struct ip_vs_conn *cp = v;
 	struct list_head *e, *l = seq->private;
+	struct ip_vs_conn_idx *cidx;
 	int idx;
 
 	++*pos;
 	if (v == SEQ_START_TOKEN)
 		return ip_vs_conn_array(seq, 0);
 
+	cidx = cp->in_idx;
 	/* more on same hash chain? */
-	if ((e = cp->c_list.next) != l)
-		return list_entry(e, struct ip_vs_conn, c_list);
+	while ((e = cidx->c_list.next) != l) {
+		cidx = list_entry(e, struct ip_vs_conn_idx, c_list);
+		if (cidx->flags & IP_VS_CIDX_F_OUT2IN) {
+			return cidx->cp;
+		}
+	}
 
 	idx = l - ip_vs_conn_tab;
 	ct_read_unlock_bh(idx);
 
 	while (++idx < IP_VS_CONN_TAB_SIZE) {
 		ct_read_lock_bh(idx);
-		list_for_each_entry(cp, &ip_vs_conn_tab[idx], c_list) {
-			seq->private = &ip_vs_conn_tab[idx];
-			return cp;
+		list_for_each_entry(cidx, &ip_vs_conn_tab[idx], c_list) {
+			if (cidx->flags & IP_VS_CIDX_F_OUT2IN) {
+				seq->private = &ip_vs_conn_tab[idx];
+				return cidx->cp;
+			}
 		}
 		ct_read_unlock_bh(idx);
 	}
@@ -830,30 +1231,33 @@ static int ip_vs_conn_seq_show(struct seq_file *seq, void *v)
 
 	if (v == SEQ_START_TOKEN)
 		seq_puts(seq,
-   "Pro FromIP   FPrt ToIP     TPrt DestIP   DPrt State       Expires\n");
+			 "Pro FromIP   FPrt ToIP     TPrt LocalIP  LPrt DestIP   DPrt State       Expires\n");
 	else {
 		const struct ip_vs_conn *cp = v;
 
 #ifdef CONFIG_IP_VS_IPV6
 		if (cp->af == AF_INET6)
-			seq_printf(seq, "%-3s %pI6 %04X %pI6 %04X %pI6 %04X %-11s %7lu\n",
-				ip_vs_proto_name(cp->protocol),
-				&cp->caddr.in6, ntohs(cp->cport),
-				&cp->vaddr.in6, ntohs(cp->vport),
-				&cp->daddr.in6, ntohs(cp->dport),
-				ip_vs_state_name(cp->protocol, cp->state),
-				(cp->timer.expires-jiffies)/HZ);
+			seq_printf(seq,
+				   "%-3s %pI6 %04X %pI6 %04X %pI6 %04X %pI6 %04X %-11s %7lu\n",
+				   ip_vs_proto_name(cp->protocol),
+				   &cp->caddr.in6, ntohs(cp->cport),
+				   &cp->vaddr.in6, ntohs(cp->vport),
+				   &cp->laddr.in6, ntohs(cp->lport),
+				   &cp->daddr.in6, ntohs(cp->dport),
+				   ip_vs_state_name(cp->protocol, cp->state),
+				   (cp->timer.expires - jiffies) / HZ);
 		else
 #endif
 			seq_printf(seq,
-				"%-3s %08X %04X %08X %04X"
-				" %08X %04X %-11s %7lu\n",
-				ip_vs_proto_name(cp->protocol),
-				ntohl(cp->caddr.ip), ntohs(cp->cport),
-				ntohl(cp->vaddr.ip), ntohs(cp->vport),
-				ntohl(cp->daddr.ip), ntohs(cp->dport),
-				ip_vs_state_name(cp->protocol, cp->state),
-				(cp->timer.expires-jiffies)/HZ);
+				   "%-3s %08X %04X %08X %04X"
+				   " %08X %04X %08X %04X %-11s %7lu\n",
+				   ip_vs_proto_name(cp->protocol),
+				   ntohl(cp->caddr.ip), ntohs(cp->cport),
+				   ntohl(cp->vaddr.ip), ntohs(cp->vport),
+				   ntohl(cp->laddr.ip), ntohs(cp->lport),
+				   ntohl(cp->daddr.ip), ntohs(cp->dport),
+				   ip_vs_state_name(cp->protocol, cp->state),
+				   (cp->timer.expires - jiffies) / HZ);
 	}
 	return 0;
 }
@@ -891,32 +1295,35 @@ static int ip_vs_conn_sync_seq_show(struct seq_file *seq, void *v)
 
 	if (v == SEQ_START_TOKEN)
 		seq_puts(seq,
-   "Pro FromIP   FPrt ToIP     TPrt DestIP   DPrt State       Origin Expires\n");
+			 "Pro FromIP   FPrt ToIP     TPrt LocalIP  LPrt DestIP   DPrt State       Origin Expires\n");
 	else {
 		const struct ip_vs_conn *cp = v;
 
 #ifdef CONFIG_IP_VS_IPV6
 		if (cp->af == AF_INET6)
-			seq_printf(seq, "%-3s %pI6 %04X %pI6 %04X %pI6 %04X %-11s %-6s %7lu\n",
-				ip_vs_proto_name(cp->protocol),
-				&cp->caddr.in6, ntohs(cp->cport),
-				&cp->vaddr.in6, ntohs(cp->vport),
-				&cp->daddr.in6, ntohs(cp->dport),
-				ip_vs_state_name(cp->protocol, cp->state),
-				ip_vs_origin_name(cp->flags),
-				(cp->timer.expires-jiffies)/HZ);
+			seq_printf(seq,
+				   "%-3s %pI6 %04X %pI6 %04X %pI6 %04X %pI6 %04X %-11s %-6s %7lu\n",
+				   ip_vs_proto_name(cp->protocol),
+				   &cp->caddr.in6, ntohs(cp->cport),
+				   &cp->vaddr.in6, ntohs(cp->vport),
+				   &cp->laddr.in6, ntohs(cp->lport),
+				   &cp->daddr.in6, ntohs(cp->dport),
+				   ip_vs_state_name(cp->protocol, cp->state),
+				   ip_vs_origin_name(cp->flags),
+				   (cp->timer.expires - jiffies) / HZ);
 		else
 #endif
 			seq_printf(seq,
-				"%-3s %08X %04X %08X %04X "
-				"%08X %04X %-11s %-6s %7lu\n",
-				ip_vs_proto_name(cp->protocol),
-				ntohl(cp->caddr.ip), ntohs(cp->cport),
-				ntohl(cp->vaddr.ip), ntohs(cp->vport),
-				ntohl(cp->daddr.ip), ntohs(cp->dport),
-				ip_vs_state_name(cp->protocol, cp->state),
-				ip_vs_origin_name(cp->flags),
-				(cp->timer.expires-jiffies)/HZ);
+				   "%-3s %08X %04X %08X %04X "
+				   "%08X %04X %08X %04X %-11s %-6s %7lu\n",
+				   ip_vs_proto_name(cp->protocol),
+				   ntohl(cp->caddr.ip), ntohs(cp->cport),
+				   ntohl(cp->vaddr.ip), ntohs(cp->vport),
+				   ntohl(cp->laddr.ip), ntohs(cp->lport),
+				   ntohl(cp->daddr.ip), ntohs(cp->dport),
+				   ip_vs_state_name(cp->protocol, cp->state),
+				   ip_vs_origin_name(cp->flags),
+				   (cp->timer.expires - jiffies) / HZ);
 	}
 	return 0;
 }
@@ -966,10 +1373,13 @@ static inline int todrop_entry(struct ip_vs_conn *cp)
 	/* Don't drop the entry if its number of incoming packets is not
 	   located in [0, 8] */
 	i = atomic_read(&cp->in_pkts);
-	if (i > 8 || i < 0) return 0;
+	if (i > 8 || i < 0)
+		return 0;
 
-	if (!todrop_rate[i]) return 0;
-	if (--todrop_counter[i] > 0) return 0;
+	if (!todrop_rate[i])
+		return 0;
+	if (--todrop_counter[i] > 0)
+		return 0;
 
 	todrop_counter[i] = todrop_rate[i];
 	return 1;
@@ -980,6 +1390,7 @@ void ip_vs_random_dropentry(void)
 {
 	int idx;
 	struct ip_vs_conn *cp;
+	struct ip_vs_conn_idx *cidx;
 
 	/*
 	 * Randomly scan 1/32 of the whole table every second
@@ -992,7 +1403,8 @@ void ip_vs_random_dropentry(void)
 		 */
 		ct_write_lock_bh(hash);
 
-		list_for_each_entry(cp, &ip_vs_conn_tab[hash], c_list) {
+		list_for_each_entry(cidx, &ip_vs_conn_tab[hash], c_list) {
+			cp = cidx->cp;
 			if (cp->flags & IP_VS_CONN_F_TEMPLATE)
 				/* connection template */
 				continue;
@@ -1035,6 +1447,7 @@ static void ip_vs_conn_flush(void)
 {
 	int idx;
 	struct ip_vs_conn *cp;
+	struct ip_vs_conn_idx *cidx;
 
   flush_again:
 	for (idx=0; idx<IP_VS_CONN_TAB_SIZE; idx++) {
@@ -1043,9 +1456,9 @@ static void ip_vs_conn_flush(void)
 		 */
 		ct_write_lock_bh(idx);
 
-		list_for_each_entry(cp, &ip_vs_conn_tab[idx], c_list) {
-
+		list_for_each_entry(cidx, &ip_vs_conn_tab[idx], c_list) {
 			IP_VS_DBG(4, "del connection\n");
+			cp = cidx->cp;
 			ip_vs_conn_expire_now(cp);
 			if (cp->control) {
 				IP_VS_DBG(4, "del conn template\n");
@@ -1071,14 +1484,16 @@ int __init ip_vs_conn_init(void)
 	/*
 	 * Allocate the connection hash table and initialize its list heads
 	 */
-	ip_vs_conn_tab = vmalloc(IP_VS_CONN_TAB_SIZE*sizeof(struct list_head));
+	ip_vs_conn_tab =
+	    vmalloc(IP_VS_CONN_TAB_SIZE * (sizeof(struct list_head)));
 	if (!ip_vs_conn_tab)
 		return -ENOMEM;
 
 	/* Allocate ip_vs_conn slab cache */
 	ip_vs_conn_cachep = kmem_cache_create("ip_vs_conn",
-					      sizeof(struct ip_vs_conn), 0,
-					      SLAB_HWCACHE_ALIGN, NULL);
+					      sizeof(struct ip_vs_conn) +
+					      2 * sizeof(struct ip_vs_conn_idx),
+					      0, SLAB_HWCACHE_ALIGN, NULL);
 	if (!ip_vs_conn_cachep) {
 		vfree(ip_vs_conn_tab);
 		return -ENOMEM;
@@ -1089,7 +1504,8 @@ int __init ip_vs_conn_init(void)
 		IP_VS_CONN_TAB_SIZE,
 		(long)(IP_VS_CONN_TAB_SIZE*sizeof(struct list_head))/1024);
 	IP_VS_DBG(0, "Each connection entry needs %Zd bytes at least\n",
-		  sizeof(struct ip_vs_conn));
+		  sizeof(struct ip_vs_conn) +
+		  2 * sizeof(struct ip_vs_conn_idx));
 
 	for (idx = 0; idx < IP_VS_CONN_TAB_SIZE; idx++) {
 		INIT_LIST_HEAD(&ip_vs_conn_tab[idx]);
@@ -1100,7 +1516,8 @@ int __init ip_vs_conn_init(void)
 	}
 
 	proc_net_fops_create(&init_net, "ip_vs_conn", 0, &ip_vs_conn_fops);
-	proc_net_fops_create(&init_net, "ip_vs_conn_sync", 0, &ip_vs_conn_sync_fops);
+	proc_net_fops_create(&init_net, "ip_vs_conn_sync", 0,
+			     &ip_vs_conn_sync_fops);
 
 	/* calculate the random value for connection hash */
 	get_random_bytes(&ip_vs_conn_rnd, sizeof(ip_vs_conn_rnd));
diff --git a/net/netfilter/ipvs/ip_vs_core.c b/net/netfilter/ipvs/ip_vs_core.c
index 9269f51..744af78 100644
--- a/net/netfilter/ipvs/ip_vs_core.c
+++ b/net/netfilter/ipvs/ip_vs_core.c
@@ -22,6 +22,7 @@
  *	Paul `Rusty' Russell		properly handle non-linear skbs
  *	Harald Welte			don't use nfcache
  *
+ *   Yu Bo        <yubo@xiaomi.com>
  */
 
 #define KMSG_COMPONENT "IPVS"
@@ -36,7 +37,7 @@
 #include <net/ip.h>
 #include <net/tcp.h>
 #include <net/udp.h>
-#include <net/icmp.h>                   /* for icmp_send */
+#include <net/icmp.h>		/* for icmp_send */
 #include <net/route.h>
 
 #include <linux/netfilter.h>
@@ -48,15 +49,14 @@
 #endif
 
 #include <net/ip_vs.h>
-
+#include <net/ip_vs_synproxy.h>
 
 EXPORT_SYMBOL(register_ip_vs_scheduler);
 EXPORT_SYMBOL(unregister_ip_vs_scheduler);
 EXPORT_SYMBOL(ip_vs_skb_replace);
 EXPORT_SYMBOL(ip_vs_proto_name);
 EXPORT_SYMBOL(ip_vs_conn_new);
-EXPORT_SYMBOL(ip_vs_conn_in_get);
-EXPORT_SYMBOL(ip_vs_conn_out_get);
+EXPORT_SYMBOL(ip_vs_conn_get);
 #ifdef CONFIG_IP_VS_PROTO_TCP
 EXPORT_SYMBOL(ip_vs_tcp_conn_listen);
 #endif
@@ -99,73 +99,9 @@ void ip_vs_init_hash_table(struct list_head *table, int rows)
 		INIT_LIST_HEAD(&table[rows]);
 }
 
-static inline void
-ip_vs_in_stats(struct ip_vs_conn *cp, struct sk_buff *skb)
-{
-	struct ip_vs_dest *dest = cp->dest;
-	if (dest && (dest->flags & IP_VS_DEST_F_AVAILABLE)) {
-		spin_lock(&dest->stats.lock);
-		dest->stats.ustats.inpkts++;
-		dest->stats.ustats.inbytes += skb->len;
-		spin_unlock(&dest->stats.lock);
-
-		spin_lock(&dest->svc->stats.lock);
-		dest->svc->stats.ustats.inpkts++;
-		dest->svc->stats.ustats.inbytes += skb->len;
-		spin_unlock(&dest->svc->stats.lock);
-
-		spin_lock(&ip_vs_stats.lock);
-		ip_vs_stats.ustats.inpkts++;
-		ip_vs_stats.ustats.inbytes += skb->len;
-		spin_unlock(&ip_vs_stats.lock);
-	}
-}
-
-
-static inline void
-ip_vs_out_stats(struct ip_vs_conn *cp, struct sk_buff *skb)
-{
-	struct ip_vs_dest *dest = cp->dest;
-	if (dest && (dest->flags & IP_VS_DEST_F_AVAILABLE)) {
-		spin_lock(&dest->stats.lock);
-		dest->stats.ustats.outpkts++;
-		dest->stats.ustats.outbytes += skb->len;
-		spin_unlock(&dest->stats.lock);
-
-		spin_lock(&dest->svc->stats.lock);
-		dest->svc->stats.ustats.outpkts++;
-		dest->svc->stats.ustats.outbytes += skb->len;
-		spin_unlock(&dest->svc->stats.lock);
-
-		spin_lock(&ip_vs_stats.lock);
-		ip_vs_stats.ustats.outpkts++;
-		ip_vs_stats.ustats.outbytes += skb->len;
-		spin_unlock(&ip_vs_stats.lock);
-	}
-}
-
-
-static inline void
-ip_vs_conn_stats(struct ip_vs_conn *cp, struct ip_vs_service *svc)
-{
-	spin_lock(&cp->dest->stats.lock);
-	cp->dest->stats.ustats.conns++;
-	spin_unlock(&cp->dest->stats.lock);
-
-	spin_lock(&svc->stats.lock);
-	svc->stats.ustats.conns++;
-	spin_unlock(&svc->stats.lock);
-
-	spin_lock(&ip_vs_stats.lock);
-	ip_vs_stats.ustats.conns++;
-	spin_unlock(&ip_vs_stats.lock);
-}
-
-
 static inline int
 ip_vs_set_state(struct ip_vs_conn *cp, int direction,
-		const struct sk_buff *skb,
-		struct ip_vs_protocol *pp)
+		const struct sk_buff *skb, struct ip_vs_protocol *pp)
 {
 	if (unlikely(!pp->state_transition))
 		return 0;
@@ -187,10 +123,10 @@ ip_vs_onepacket_enabled(struct ip_vs_service *svc, struct ip_vs_iphdr *iph)
  *  Locking: we are svc user (svc->refcnt), so we hold all dests too
  *  Protocols supported: TCP, UDP
  */
-static struct ip_vs_conn *
-ip_vs_sched_persist(struct ip_vs_service *svc,
-		    const struct sk_buff *skb,
-		    __be16 ports[2])
+static struct ip_vs_conn *ip_vs_sched_persist(struct ip_vs_service *svc,
+					      struct sk_buff *skb,
+					      __be16 ports[2],
+					      int is_synproxy_on)
 {
 	struct ip_vs_conn *cp = NULL;
 	struct ip_vs_iphdr iph;
@@ -262,14 +198,14 @@ ip_vs_sched_persist(struct ip_vs_service *svc,
 						    ports[1],
 						    &dest->addr, dest->port,
 						    IP_VS_CONN_F_TEMPLATE,
-						    dest);
+						    dest, NULL, 0);
 			else
 				ct = ip_vs_conn_new(svc->af, iph.protocol,
 						    &snet, 0,
 						    &iph.daddr, 0,
 						    &dest->addr, 0,
 						    IP_VS_CONN_F_TEMPLATE,
-						    dest);
+						    dest, NULL, 0);
 			if (ct == NULL)
 				return NULL;
 
@@ -324,14 +260,14 @@ ip_vs_sched_persist(struct ip_vs_service *svc,
 						    &fwmark, 0,
 						    &dest->addr, 0,
 						    IP_VS_CONN_F_TEMPLATE,
-						    dest);
+						    dest, NULL, 0);
 			} else
 				ct = ip_vs_conn_new(svc->af, iph.protocol,
 						    &snet, 0,
 						    &iph.daddr, 0,
 						    &dest->addr, 0,
 						    IP_VS_CONN_F_TEMPLATE,
-						    dest);
+						    dest, NULL, 0);
 			if (ct == NULL)
 				return NULL;
 
@@ -351,7 +287,7 @@ ip_vs_sched_persist(struct ip_vs_service *svc,
 			    &iph.daddr, ports[1],
 			    &dest->addr, dport,
 			    ip_vs_onepacket_enabled(svc, &iph),
-			    dest);
+			    dest, skb, is_synproxy_on);
 	if (cp == NULL) {
 		ip_vs_conn_put(ct);
 		return NULL;
@@ -367,15 +303,14 @@ ip_vs_sched_persist(struct ip_vs_service *svc,
 	return cp;
 }
 
-
 /*
  *  IPVS main scheduling function
  *  It selects a server according to the virtual service, and
  *  creates a connection entry.
  *  Protocols supported: TCP, UDP
  */
-struct ip_vs_conn *
-ip_vs_schedule(struct ip_vs_service *svc, const struct sk_buff *skb)
+struct ip_vs_conn *ip_vs_schedule(struct ip_vs_service *svc,
+				  struct sk_buff *skb, int is_synproxy_on)
 {
 	struct ip_vs_conn *cp = NULL;
 	struct ip_vs_iphdr iph;
@@ -391,34 +326,69 @@ ip_vs_schedule(struct ip_vs_service *svc, const struct sk_buff *skb)
 	 *    Persistent service
 	 */
 	if (svc->flags & IP_VS_SVC_F_PERSISTENT)
-		return ip_vs_sched_persist(svc, skb, pptr);
+		return ip_vs_sched_persist(svc, skb, pptr, is_synproxy_on);
+
 
 	/*
-	 *    Non-persistent service
+	 *    dsnat yubo@xiaomi.com
 	 */
-	if (!svc->fwmark && pptr[1] != svc->port) {
-		if (!svc->port)
-			pr_err("Schedule: port zero only supported "
-			       "in persistent services, "
-			       "check your ipvs configuration\n");
-		return NULL;
-	}
+	if (svc->port == 0){	
+EnterFunction(11);
 
-	dest = svc->scheduler->schedule(svc, skb);
-	if (dest == NULL) {
-		IP_VS_DBG(1, "Schedule: no dest found.\n");
-		return NULL;
-	}
+		dest = (struct ip_vs_dest *)svc->destinations.next;
+		if (dest == NULL) {
+			IP_VS_DBG(1, "Schedule: no dest found.\n");
+			return NULL;
+		}
 
+		/*
+		 *    Create a connection entry.
+		 */
+		cp = ip_vs_conn_new(svc->af, iph.protocol,
+				    &iph.saddr, pptr[0],
+				    &iph.daddr, pptr[1],
+				    &iph.daddr, pptr[1],
+				    ip_vs_onepacket_enabled(svc, &iph),
+				    dest, skb, is_synproxy_on);
+		
+		IP_VS_DBG_BUF(6, "Schedule dsnat fwd:%c c:%s:%u v:%s:%u "
+	      "l:%s:%u d:%s:%u conn->flags:%X conn->refcnt:%d\n",
+	      ip_vs_fwd_tag(cp),
+	      IP_VS_DBG_ADDR(svc->af, &cp->caddr), ntohs(cp->cport),
+	      IP_VS_DBG_ADDR(svc->af, &cp->vaddr), ntohs(cp->vport),
+	      IP_VS_DBG_ADDR(svc->af, &cp->laddr), ntohs(cp->lport),
+	      IP_VS_DBG_ADDR(svc->af, &cp->daddr), ntohs(cp->dport),
+	      cp->flags, atomic_read(&cp->refcnt));
+
+			
+	}else{
 	/*
-	 *    Create a connection entry.
+	 *    Non-persistent service
 	 */
-	cp = ip_vs_conn_new(svc->af, iph.protocol,
-			    &iph.saddr, pptr[0],
-			    &iph.daddr, pptr[1],
-			    &dest->addr, dest->port ? dest->port : pptr[1],
-			    ip_vs_onepacket_enabled(svc, &iph),
-			    dest);
+		if (!svc->fwmark && pptr[1] != svc->port) {
+			if (!svc->port)
+				pr_err("Schedule: port zero only supported "
+				       "in persistent services, "
+				       "check your ipvs configuration\n");
+			return NULL;
+		}
+
+		dest = svc->scheduler->schedule(svc, skb);
+		if (dest == NULL) {
+			IP_VS_DBG(1, "Schedule: no dest found.\n");
+			return NULL;
+		}
+
+		/*
+		 *    Create a connection entry.
+		 */
+		cp = ip_vs_conn_new(svc->af, iph.protocol,
+				    &iph.saddr, pptr[0],
+				    &iph.daddr, pptr[1],
+				    &dest->addr, dest->port ? dest->port : pptr[1],
+				    ip_vs_onepacket_enabled(svc, &iph),
+				    dest, skb, is_synproxy_on);
+	}
 	if (cp == NULL)
 		return NULL;
 
@@ -459,7 +429,8 @@ int ip_vs_leave(struct ip_vs_service *svc, struct sk_buff *skb,
 		unicast = ipv6_addr_type(&iph.daddr.in6) & IPV6_ADDR_UNICAST;
 	else
 #endif
-		unicast = (inet_addr_type(&init_net, iph.daddr.ip) == RTN_UNICAST);
+		unicast =
+		    (inet_addr_type(&init_net, iph.daddr.ip) == RTN_UNICAST);
 
 	/* if it is fwmark-based service, the cache_bypass sysctl is up
 	   and the destination is a non-local unicast, then create
@@ -479,7 +450,7 @@ int ip_vs_leave(struct ip_vs_service *svc, struct sk_buff *skb,
 				    &daddr, 0,
 				    IP_VS_CONN_F_BYPASS |
 				    ip_vs_onepacket_enabled(svc, &iph),
-				    NULL);
+				    NULL, NULL, 0);
 		if (cp == NULL)
 			return NF_DROP;
 
@@ -526,6 +497,8 @@ int ip_vs_leave(struct ip_vs_service *svc, struct sk_buff *skb,
 #endif
 		icmp_send(skb, ICMP_DEST_UNREACH, ICMP_PORT_UNREACH, 0);
 
+	IP_VS_INC_ESTATS(ip_vs_esmib, CONN_SCHED_UNREACH);
+
 	return NF_DROP;
 }
 
@@ -572,99 +545,8 @@ static inline int ip_vs_gather_frags_v6(struct sk_buff *skb, u_int32_t user)
 }
 #endif
 
-/*
- * Packet has been made sufficiently writable in caller
- * - inout: 1=in->out, 0=out->in
- */
-void ip_vs_nat_icmp(struct sk_buff *skb, struct ip_vs_protocol *pp,
-		    struct ip_vs_conn *cp, int inout)
-{
-	struct iphdr *iph	 = ip_hdr(skb);
-	unsigned int icmp_offset = iph->ihl*4;
-	struct icmphdr *icmph	 = (struct icmphdr *)(skb_network_header(skb) +
-						      icmp_offset);
-	struct iphdr *ciph	 = (struct iphdr *)(icmph + 1);
-
-	if (inout) {
-		iph->saddr = cp->vaddr.ip;
-		ip_send_check(iph);
-		ciph->daddr = cp->vaddr.ip;
-		ip_send_check(ciph);
-	} else {
-		iph->daddr = cp->daddr.ip;
-		ip_send_check(iph);
-		ciph->saddr = cp->daddr.ip;
-		ip_send_check(ciph);
-	}
-
-	/* the TCP/UDP port */
-	if (IPPROTO_TCP == ciph->protocol || IPPROTO_UDP == ciph->protocol) {
-		__be16 *ports = (void *)ciph + ciph->ihl*4;
-
-		if (inout)
-			ports[1] = cp->vport;
-		else
-			ports[0] = cp->dport;
-	}
-
-	/* And finally the ICMP checksum */
-	icmph->checksum = 0;
-	icmph->checksum = ip_vs_checksum_complete(skb, icmp_offset);
-	skb->ip_summed = CHECKSUM_UNNECESSARY;
-
-	if (inout)
-		IP_VS_DBG_PKT(11, pp, skb, (void *)ciph - (void *)iph,
-			"Forwarding altered outgoing ICMP");
-	else
-		IP_VS_DBG_PKT(11, pp, skb, (void *)ciph - (void *)iph,
-			"Forwarding altered incoming ICMP");
-}
-
-#ifdef CONFIG_IP_VS_IPV6
-void ip_vs_nat_icmp_v6(struct sk_buff *skb, struct ip_vs_protocol *pp,
-		    struct ip_vs_conn *cp, int inout)
-{
-	struct ipv6hdr *iph	 = ipv6_hdr(skb);
-	unsigned int icmp_offset = sizeof(struct ipv6hdr);
-	struct icmp6hdr *icmph	 = (struct icmp6hdr *)(skb_network_header(skb) +
-						      icmp_offset);
-	struct ipv6hdr *ciph	 = (struct ipv6hdr *)(icmph + 1);
-
-	if (inout) {
-		iph->saddr = cp->vaddr.in6;
-		ciph->daddr = cp->vaddr.in6;
-	} else {
-		iph->daddr = cp->daddr.in6;
-		ciph->saddr = cp->daddr.in6;
-	}
-
-	/* the TCP/UDP port */
-	if (IPPROTO_TCP == ciph->nexthdr || IPPROTO_UDP == ciph->nexthdr) {
-		__be16 *ports = (void *)ciph + sizeof(struct ipv6hdr);
-
-		if (inout)
-			ports[1] = cp->vport;
-		else
-			ports[0] = cp->dport;
-	}
-
-	/* And finally the ICMP checksum */
-	icmph->icmp6_cksum = 0;
-	/* TODO IPv6: is this correct for ICMPv6? */
-	ip_vs_checksum_complete(skb, icmp_offset);
-	skb->ip_summed = CHECKSUM_UNNECESSARY;
-
-	if (inout)
-		IP_VS_DBG_PKT(11, pp, skb, (void *)ciph - (void *)iph,
-			"Forwarding altered outgoing ICMPv6");
-	else
-		IP_VS_DBG_PKT(11, pp, skb, (void *)ciph - (void *)iph,
-			"Forwarding altered incoming ICMPv6");
-}
-#endif
-
 /* Handle relevant response ICMP messages - forward to the right
- * destination host. Used for NAT and local client.
+ * destination host. Used for NAT / local client / FULLNAT.
  */
 static int handle_response_icmp(int af, struct sk_buff *skb,
 				union nf_inet_addr *snet,
@@ -674,7 +556,8 @@ static int handle_response_icmp(int af, struct sk_buff *skb,
 {
 	unsigned int verdict = NF_DROP;
 
-	if (IP_VS_FWD_METHOD(cp) != 0) {
+	if ((IP_VS_FWD_METHOD(cp) != IP_VS_CONN_F_MASQ)
+	    && (IP_VS_FWD_METHOD(cp) != IP_VS_CONN_F_FULLNAT)) {
 		pr_err("shouldn't reach here, because the box is on the "
 		       "half connection in the tun/dr module.\n");
 	}
@@ -689,21 +572,33 @@ static int handle_response_icmp(int af, struct sk_buff *skb,
 
 	if (IPPROTO_TCP == protocol || IPPROTO_UDP == protocol)
 		offset += 2 * sizeof(__u16);
-	if (!skb_make_writable(skb, offset))
-		goto out;
-
-#ifdef CONFIG_IP_VS_IPV6
-	if (af == AF_INET6)
-		ip_vs_nat_icmp_v6(skb, pp, cp, 1);
-	else
-#endif
-		ip_vs_nat_icmp(skb, pp, cp, 1);
 
 	/* do the statistics and put it back */
 	ip_vs_out_stats(cp, skb);
 
-	skb->ipvs_property = 1;
-	verdict = NF_ACCEPT;
+	if (IP_VS_FWD_METHOD(cp) == IP_VS_CONN_F_FULLNAT) {
+#ifdef CONFIG_IP_VS_IPV6
+		if (af == AF_INET6)
+			verdict =
+			    ip_vs_fnat_response_icmp_xmit_v6(skb, pp, cp,
+							     offset);
+		else
+#endif
+			verdict =
+			    ip_vs_fnat_response_icmp_xmit(skb, pp, cp, offset);
+
+	} else {
+#ifdef CONFIG_IP_VS_IPV6
+		if (af == AF_INET6)
+			verdict =
+			    ip_vs_normal_response_icmp_xmit_v6(skb, pp, cp,
+							       offset);
+		else
+#endif
+			verdict =
+			    ip_vs_normal_response_icmp_xmit(skb, pp, cp,
+							    offset);
+	}
 
 out:
 	__ip_vs_conn_put(cp);
@@ -726,6 +621,7 @@ static int ip_vs_out_icmp(struct sk_buff *skb, int *related)
 	struct ip_vs_protocol *pp;
 	unsigned int offset, ihl;
 	union nf_inet_addr snet;
+	int res_dir;
 
 	*related = 1;
 
@@ -742,8 +638,7 @@ static int ip_vs_out_icmp(struct sk_buff *skb, int *related)
 		return NF_DROP;
 
 	IP_VS_DBG(12, "Outgoing ICMP (%d,%d) %pI4->%pI4\n",
-		  ic->type, ntohs(icmp_id(ic)),
-		  &iph->saddr, &iph->daddr);
+		  ic->type, ntohs(icmp_id(ic)), &iph->saddr, &iph->daddr);
 
 	/*
 	 * Work through seeing if this is for us.
@@ -770,8 +665,7 @@ static int ip_vs_out_icmp(struct sk_buff *skb, int *related)
 		return NF_ACCEPT;
 
 	/* Is the embedded protocol header present? */
-	if (unlikely(cih->frag_off & htons(IP_OFFSET) &&
-		     pp->dont_defrag))
+	if (unlikely(cih->frag_off & htons(IP_OFFSET) && pp->dont_defrag))
 		return NF_ACCEPT;
 
 	IP_VS_DBG_PKT(11, pp, skb, offset, "Checking outgoing ICMP for");
@@ -780,7 +674,7 @@ static int ip_vs_out_icmp(struct sk_buff *skb, int *related)
 
 	ip_vs_fill_iphdr(AF_INET, cih, &ciph);
 	/* The embedded headers contain source and dest in reverse order */
-	cp = pp->conn_out_get(AF_INET, skb, pp, &ciph, offset, 1);
+	cp = pp->conn_out_get(AF_INET, skb, pp, &ciph, offset, 1, &res_dir);
 	if (!cp)
 		return NF_ACCEPT;
 
@@ -801,6 +695,7 @@ static int ip_vs_out_icmp_v6(struct sk_buff *skb, int *related)
 	struct ip_vs_protocol *pp;
 	unsigned int offset;
 	union nf_inet_addr snet;
+	int res_dir;
 
 	*related = 1;
 
@@ -855,7 +750,7 @@ static int ip_vs_out_icmp_v6(struct sk_buff *skb, int *related)
 
 	ip_vs_fill_iphdr(AF_INET6, cih, &ciph);
 	/* The embedded headers contain source and dest in reverse order */
-	cp = pp->conn_out_get(AF_INET6, skb, pp, &ciph, offset, 1);
+	cp = pp->conn_out_get(AF_INET6, skb, pp, &ciph, offset, 1, &res_dir);
 	if (!cp)
 		return NF_ACCEPT;
 
@@ -876,61 +771,52 @@ static inline int is_tcp_reset(const struct sk_buff *skb, int nh_len)
 }
 
 /* Handle response packets: rewrite addresses and send away...
- * Used for NAT and local client.
+ * Used for NAT / local client / FULLNAT.
  */
 static unsigned int
 handle_response(int af, struct sk_buff *skb, struct ip_vs_protocol *pp,
 		struct ip_vs_conn *cp, int ihl)
 {
-	IP_VS_DBG_PKT(11, pp, skb, 0, "Outgoing packet");
+	unsigned int ret = NF_DROP;
 
-	if (!skb_make_writable(skb, ihl))
-		goto drop;
+	/* statistics */
+	ip_vs_out_stats(cp, skb);
 
-	/* mangle the packet */
-	if (pp->snat_handler && !pp->snat_handler(skb, pp, cp))
-		goto drop;
+	/*
+	 * Syn-proxy step 3 logic: receive syn-ack from rs.
+	 */
+	if (pp->protocol == IPPROTO_TCP && ip_vs_synproxy_synack_rcv(skb, cp, pp, ihl, &ret) == 0) {
+		goto out;
+	}
 
+	/* state transition */
+	ip_vs_set_state(cp, IP_VS_DIR_OUTPUT, skb, pp);
+	/* transmit */
+
+	if (cp->flags & IP_VS_CONN_F_FULLNAT) {
 #ifdef CONFIG_IP_VS_IPV6
-	if (af == AF_INET6)
-		ipv6_hdr(skb)->saddr = cp->vaddr.in6;
-	else
+		if (af == AF_INET6) {
+			ret = ip_vs_fnat_response_xmit_v6(skb, pp, cp, ihl);
+		} else
 #endif
-	{
-		ip_hdr(skb)->saddr = cp->vaddr.ip;
-		ip_send_check(ip_hdr(skb));
-	}
-
-	/* For policy routing, packets originating from this
-	 * machine itself may be routed differently to packets
-	 * passing through.  We want this packet to be routed as
-	 * if it came from this machine itself.  So re-compute
-	 * the routing information.
-	 */
+		{
+			ret = ip_vs_fnat_response_xmit(skb, pp, cp, ihl);
+		}
+	} else {
 #ifdef CONFIG_IP_VS_IPV6
-	if (af == AF_INET6) {
-		if (ip6_route_me_harder(skb) != 0)
-			goto drop;
-	} else
+		if (af == AF_INET6) {
+			ret = ip_vs_normal_response_xmit_v6(skb, pp, cp, ihl);
+		} else
 #endif
-		if (ip_route_me_harder(skb, RTN_LOCAL) != 0)
-			goto drop;
-
-	IP_VS_DBG_PKT(10, pp, skb, 0, "After SNAT");
-
-	ip_vs_out_stats(cp, skb);
-	ip_vs_set_state(cp, IP_VS_DIR_OUTPUT, skb, pp);
-	ip_vs_conn_put(cp);
-
-	skb->ipvs_property = 1;
+		{
+			ret = ip_vs_normal_response_xmit(skb, pp, cp, ihl);
+		}
 
-	LeaveFunction(11);
-	return NF_ACCEPT;
+	}
 
-drop:
+      out:
 	ip_vs_conn_put(cp);
-	kfree_skb(skb);
-	return NF_STOLEN;
+	return ret;
 }
 
 /*
@@ -946,6 +832,11 @@ ip_vs_out(unsigned int hooknum, struct sk_buff *skb,
 	struct ip_vs_protocol *pp;
 	struct ip_vs_conn *cp;
 	int af;
+	int res_dir;
+	int ret;
+	int v;
+	int pkts;
+	int forward = 0;
 
 	EnterFunction(11);
 
@@ -972,6 +863,8 @@ ip_vs_out(unsigned int hooknum, struct sk_buff *skb,
 			if (related)
 				return verdict;
 			ip_vs_fill_iphdr(af, skb_network_header(skb), &iph);
+		} else {
+			forward = 1;
 		}
 
 	pp = ip_vs_proto_get(iph.protocol);
@@ -1002,28 +895,112 @@ ip_vs_out(unsigned int hooknum, struct sk_buff *skb,
 	/*
 	 * Check if the packet belongs to an existing entry
 	 */
-	cp = pp->conn_out_get(af, skb, pp, &iph, iph.len, 0);
+	cp = pp->conn_out_get(af, skb, pp, &iph, iph.len, 0, &res_dir);
+
+
+	//dsnat
+	if(forward){
+EnterFunction(11);		
+		if (!cp) {
+			/* create a new connection */
+			int v;
+			IP_VS_DBG(11, "forward out 2 in  conn not found! -> tcp_conn_schedule\n");
+			if (!pp->conn_schedule(af | IP_VS_CONN_F_DSNAT, skb, pp, &v, &cp))
+				return v;
+EnterFunction(11);	
+
+			if (unlikely(!cp)) {
+				/* sorry, all this trouble for a no-hit :) */
+				IP_VS_DBG_PKT(12, pp, skb, 0,
+					      "packet continues traversal as normal");
+				return NF_ACCEPT;
+			}		
+		}
 
-	if (unlikely(!cp)) {
-		if (sysctl_ip_vs_nat_icmp_send &&
-		    (pp->protocol == IPPROTO_TCP ||
-		     pp->protocol == IPPROTO_UDP)) {
-			__be16 _ports[2], *pptr;
-
-			pptr = skb_header_pointer(skb, iph.len,
-						  sizeof(_ports), _ports);
-			if (pptr == NULL)
-				return NF_ACCEPT;	/* Not for me */
-			if (ip_vs_lookup_real_service(af, iph.protocol,
-						      &iph.saddr,
-						      pptr[0])) {
-				/*
-				 * Notify the real server: there is no
-				 * existing entry if it is not RST
-				 * packet or not TCP packet.
-				 */
-				if (iph.protocol != IPPROTO_TCP
-				    || !is_tcp_reset(skb, iph.len)) {
+		IP_VS_DBG_PKT(11, pp, skb, 0, "Incoming packet");
+		
+		ip_vs_in_stats(cp, skb);
+
+		/*
+		 * Filter out-in ack packet when cp is at SYN_SENT state.
+		 * DROP it if not a valid packet, STORE it if we have 
+		 * space left. 
+		 */
+		if ((cp->flags & IP_VS_CONN_F_SYNPROXY) &&
+		    (0 == ip_vs_synproxy_filter_ack(skb, cp, pp, &iph, &v))) {
+			ip_vs_conn_put(cp);
+			return v;
+		}
+
+
+		/*
+		 * "Reuse" syn-proxy sessions.
+		 * "Reuse" means update syn_proxy_seq struct and clean ack_skb etc.
+		 */
+		if ((cp->flags & IP_VS_CONN_F_SYNPROXY) &&
+		    (0 != sysctl_ip_vs_synproxy_conn_reuse)) {
+			int v = NF_DROP;
+
+			if (0 == ip_vs_synproxy_reuse_conn(af, skb, cp, pp, &iph, &v)) {
+				ip_vs_conn_put(cp);
+				return v;
+			}
+		}
+
+		ip_vs_set_state(cp, IP_VS_DIR_OUTPUT, skb, pp);
+		
+		if (cp->packet_xmit){
+			EnterFunction(11);		
+			ret = cp->packet_xmit(skb, cp, pp);
+		/* do not touch skb anymore */
+		} else {
+			IP_VS_DBG_RL("warning: packet_xmit is null");
+			ret = NF_ACCEPT;
+		}
+
+		pkts = atomic_add_return(1, &cp->in_pkts);
+		
+		if (af == AF_INET &&
+		    (ip_vs_sync_state & IP_VS_STATE_MASTER) &&
+		    (((cp->protocol != IPPROTO_TCP ||
+		       cp->state == IP_VS_TCP_S_ESTABLISHED) &&
+		      (pkts % sysctl_ip_vs_sync_threshold[1]
+		       == sysctl_ip_vs_sync_threshold[0])) ||
+		     ((cp->protocol == IPPROTO_TCP) && (cp->old_state != cp->state) &&
+		      ((cp->state == IP_VS_TCP_S_FIN_WAIT) ||
+		       (cp->state == IP_VS_TCP_S_CLOSE_WAIT) ||
+		       (cp->state == IP_VS_TCP_S_TIME_WAIT))))){
+			ip_vs_sync_conn(cp);
+
+		}
+		cp->old_state = cp->state;
+
+		ip_vs_conn_put(cp);
+		skb->ipvs_property = 1;
+		
+		return ret;
+	}else {
+EnterFunction(11);
+
+		if (unlikely(!cp)) {
+			if (sysctl_ip_vs_nat_icmp_send &&
+			    (pp->protocol == IPPROTO_TCP ||
+			     pp->protocol == IPPROTO_UDP)) {
+				__be16 _ports[2], *pptr;
+
+				pptr = skb_header_pointer(skb, iph.len,
+							  sizeof(_ports), _ports);
+				if (pptr == NULL)
+					return NF_ACCEPT;	/* Not for me */
+				if (ip_vs_lookup_real_service(af, iph.protocol,
+							      &iph.saddr, pptr[0])) {
+					/*
+					 * Notify the real server: there is no
+					 * existing entry if it is not RST
+					 * packet or not TCP packet.
+					 */
+					if (iph.protocol != IPPROTO_TCP
+					    || !is_tcp_reset(skb, iph.len)) {
 #ifdef CONFIG_IP_VS_IPV6
 					if (af == AF_INET6)
 						icmpv6_send(skb,
@@ -1036,15 +1013,18 @@ ip_vs_out(unsigned int hooknum, struct sk_buff *skb,
 							  ICMP_DEST_UNREACH,
 							  ICMP_PORT_UNREACH, 0);
 					return NF_DROP;
+                    }
 				}
 			}
+			IP_VS_DBG_PKT(12, pp, skb, 0,
+				      "packet continues traversal as normal");
+
+
+			return NF_ACCEPT;
 		}
-		IP_VS_DBG_PKT(12, pp, skb, 0,
-			      "packet continues traversal as normal");
-		return NF_ACCEPT;
-	}
 
-	return handle_response(af, skb, pp, cp, iph.len);
+		return handle_response(af, skb, pp, cp, iph.len);
+	}
 }
 
 
@@ -1065,6 +1045,7 @@ ip_vs_in_icmp(struct sk_buff *skb, int *related, unsigned int hooknum)
 	struct ip_vs_protocol *pp;
 	unsigned int offset, ihl, verdict;
 	union nf_inet_addr snet;
+	int res_dir;
 
 	*related = 1;
 
@@ -1082,8 +1063,7 @@ ip_vs_in_icmp(struct sk_buff *skb, int *related, unsigned int hooknum)
 		return NF_DROP;
 
 	IP_VS_DBG(12, "Incoming ICMP (%d,%d) %pI4->%pI4\n",
-		  ic->type, ntohs(icmp_id(ic)),
-		  &iph->saddr, &iph->daddr);
+		  ic->type, ntohs(icmp_id(ic)), &iph->saddr, &iph->daddr);
 
 	/*
 	 * Work through seeing if this is for us.
@@ -1110,8 +1090,7 @@ ip_vs_in_icmp(struct sk_buff *skb, int *related, unsigned int hooknum)
 		return NF_ACCEPT;
 
 	/* Is the embedded protocol header present? */
-	if (unlikely(cih->frag_off & htons(IP_OFFSET) &&
-		     pp->dont_defrag))
+	if (unlikely(cih->frag_off & htons(IP_OFFSET) && pp->dont_defrag))
 		return NF_ACCEPT;
 
 	IP_VS_DBG_PKT(11, pp, skb, offset, "Checking incoming ICMP for");
@@ -1120,19 +1099,18 @@ ip_vs_in_icmp(struct sk_buff *skb, int *related, unsigned int hooknum)
 
 	ip_vs_fill_iphdr(AF_INET, cih, &ciph);
 	/* The embedded headers contain source and dest in reverse order */
-	cp = pp->conn_in_get(AF_INET, skb, pp, &ciph, offset, 1);
+	cp = pp->conn_in_get(AF_INET, skb, pp, &ciph, offset, 1, &res_dir);
 	if (!cp) {
-		/* The packet could also belong to a local client */
-		cp = pp->conn_out_get(AF_INET, skb, pp, &ciph, offset, 1);
-		if (cp) {
-			snet.ip = iph->saddr;
-			return handle_response_icmp(AF_INET, skb, &snet,
-						    cih->protocol, cp, pp,
-						    offset, ihl);
-		}
 		return NF_ACCEPT;
 	}
 
+	if (res_dir == IP_VS_CIDX_F_IN2OUT) {
+		/* The packet belong to a local client / fullnat */
+		snet.ip = iph->saddr;
+		return handle_response_icmp(AF_INET, skb, &snet,
+					    cih->protocol, cp, pp, offset, ihl);
+	}
+
 	verdict = NF_DROP;
 
 	/* Ensure the checksum is correct */
@@ -1169,14 +1147,14 @@ ip_vs_in_icmp_v6(struct sk_buff *skb, int *related, unsigned int hooknum)
 	struct ip_vs_protocol *pp;
 	unsigned int offset, verdict;
 	union nf_inet_addr snet;
+	int res_dir;
 
 	*related = 1;
 
 	/* reassemble IP fragments */
 	if (ipv6_hdr(skb)->nexthdr == IPPROTO_FRAGMENT) {
 		if (ip_vs_gather_frags_v6(skb, hooknum == NF_INET_LOCAL_IN ?
-					       IP_DEFRAG_VS_IN :
-					       IP_DEFRAG_VS_FWD))
+					  IP_DEFRAG_VS_IN : IP_DEFRAG_VS_FWD))
 			return NF_STOLEN;
 	}
 
@@ -1225,20 +1203,19 @@ ip_vs_in_icmp_v6(struct sk_buff *skb, int *related, unsigned int hooknum)
 
 	ip_vs_fill_iphdr(AF_INET6, cih, &ciph);
 	/* The embedded headers contain source and dest in reverse order */
-	cp = pp->conn_in_get(AF_INET6, skb, pp, &ciph, offset, 1);
+	cp = pp->conn_in_get(AF_INET6, skb, pp, &ciph, offset, 1, &res_dir);
 	if (!cp) {
-		/* The packet could also belong to a local client */
-		cp = pp->conn_out_get(AF_INET6, skb, pp, &ciph, offset, 1);
-		if (cp) {
-			ipv6_addr_copy(&snet.in6, &iph->saddr);
-			return handle_response_icmp(AF_INET6, skb, &snet,
-						    cih->nexthdr,
-						    cp, pp, offset,
-						    sizeof(struct ipv6hdr));
-		}
 		return NF_ACCEPT;
 	}
 
+	if (res_dir == IP_VS_CIDX_F_IN2OUT) {
+		ipv6_addr_copy(&snet.in6, &iph->saddr);
+		return handle_response_icmp(AF_INET6, skb, &snet,
+					    cih->nexthdr,
+					    cp, pp, offset,
+					    sizeof(struct ipv6hdr));
+	}
+
 	verdict = NF_DROP;
 
 	/* do the statistics and put it back */
@@ -1268,6 +1245,8 @@ ip_vs_in(unsigned int hooknum, struct sk_buff *skb,
 	struct ip_vs_protocol *pp;
 	struct ip_vs_conn *cp;
 	int ret, restart, af, pkts;
+	int v = NF_DROP;
+	int res_dir;
 
 	af = (skb->protocol == htons(ETH_P_IP)) ? AF_INET : AF_INET6;
 
@@ -1280,15 +1259,15 @@ ip_vs_in(unsigned int hooknum, struct sk_buff *skb,
 	if (unlikely(skb->pkt_type != PACKET_HOST)) {
 		IP_VS_DBG_BUF(12, "packet type=%d proto=%d daddr=%s ignored\n",
 			      skb->pkt_type,
-			      iph.protocol,
-			      IP_VS_DBG_ADDR(af, &iph.daddr));
+			      iph.protocol, IP_VS_DBG_ADDR(af, &iph.daddr));
 		return NF_ACCEPT;
 	}
 
 #ifdef CONFIG_IP_VS_IPV6
 	if (af == AF_INET6) {
 		if (unlikely(iph.protocol == IPPROTO_ICMPV6)) {
-			int related, verdict = ip_vs_in_icmp_v6(skb, &related, hooknum);
+			int related, verdict =
+			    ip_vs_in_icmp_v6(skb, &related, hooknum);
 
 			if (related)
 				return verdict;
@@ -1312,15 +1291,16 @@ ip_vs_in(unsigned int hooknum, struct sk_buff *skb,
 	/*
 	 * Check if the packet belongs to an existing connection entry
 	 */
-	cp = pp->conn_in_get(af, skb, pp, &iph, iph.len, 0);
-
-	if (unlikely(!cp)) {
-		int v;
+	cp = pp->conn_in_get(af, skb, pp, &iph, iph.len, 0, &res_dir);
 
-		/* For local client packets, it could be a response */
-		cp = pp->conn_out_get(af, skb, pp, &iph, iph.len, 0);
-		if (cp)
+	if (likely(cp)) {
+		/* For full-nat/local-client packets, it could be a response */
+		if (res_dir == IP_VS_CIDX_F_IN2OUT) {
 			return handle_response(af, skb, pp, cp, iph.len);
+		}
+	} else {
+		/* create a new connection */
+		int v;
 
 		if (!pp->conn_schedule(af, skb, pp, &v, &cp))
 			return v;
@@ -1350,10 +1330,36 @@ ip_vs_in(unsigned int hooknum, struct sk_buff *skb,
 	}
 
 	ip_vs_in_stats(cp, skb);
+
+	/*
+	 * Filter out-in ack packet when cp is at SYN_SENT state.
+	 * DROP it if not a valid packet, STORE it if we have 
+	 * space left. 
+	 */
+	if ((cp->flags & IP_VS_CONN_F_SYNPROXY) &&
+	    (0 == ip_vs_synproxy_filter_ack(skb, cp, pp, &iph, &v))) {
+		ip_vs_conn_put(cp);
+		return v;
+	}
+
+	/*
+	 * "Reuse" syn-proxy sessions.
+	 * "Reuse" means update syn_proxy_seq struct and clean ack_skb etc.
+	 */
+	if ((cp->flags & IP_VS_CONN_F_SYNPROXY) &&
+	    (0 != sysctl_ip_vs_synproxy_conn_reuse)) {
+		int v = NF_DROP;
+
+		if (0 == ip_vs_synproxy_reuse_conn(af, skb, cp, pp, &iph, &v)) {
+			ip_vs_conn_put(cp);
+			return v;
+		}
+	}
+
 	restart = ip_vs_set_state(cp, IP_VS_DIR_INPUT, skb, pp);
 	if (cp->packet_xmit)
 		ret = cp->packet_xmit(skb, cp, pp);
-		/* do not touch skb anymore */
+	/* do not touch skb anymore */
 	else {
 		IP_VS_DBG_RL("warning: packet_xmit is null");
 		ret = NF_ACCEPT;
@@ -1421,18 +1427,67 @@ ip_vs_forward_icmp_v6(unsigned int hooknum, struct sk_buff *skb,
 }
 #endif
 
+#define IPPROTO_OSPF 89
+static unsigned int
+ip_vs_pre_routing(unsigned int hooknum, struct sk_buff *skb,
+		  const struct net_device *in, const struct net_device *out,
+		  int (*okfn) (struct sk_buff *))
+{
+	struct ip_vs_iphdr iph;
+	int af;
+	struct ip_vs_service *svc;
+
+	af = (skb->protocol == htons(ETH_P_IP)) ? AF_INET : AF_INET6;
+
+	ip_vs_fill_iphdr(af, skb_network_header(skb), &iph);
+
+	/* drop all ip fragment except ospf */
+	if ((af == AF_INET)
+	    && (ip_hdr(skb)->frag_off & htons(IP_MF | IP_OFFSET))
+	    && (iph.protocol != IPPROTO_OSPF)) {
+		if(sysctl_ip_vs_frag_drop_entry == 1) {
+			IP_VS_INC_ESTATS(ip_vs_esmib, DEFENCE_IP_FRAG_DROP);
+			return NF_DROP;
+		} else {
+			if (ip_vs_gather_frags(skb, IP_DEFRAG_VS_IN))
+				return NF_STOLEN;
+
+			IP_VS_INC_ESTATS(ip_vs_esmib, DEFENCE_IP_FRAG_GATHER);
+			ip_vs_fill_iphdr(af, skb_network_header(skb), &iph);
+		}
+	}
+
+	/* drop udp packet which send to tcp-vip */
+	if ((sysctl_ip_vs_udp_drop_entry == 1) && (iph.protocol == IPPROTO_UDP)) {
+		if ((svc =
+		     ip_vs_lookup_vip(af, IPPROTO_TCP, &iph.daddr)) != NULL) {
+			IP_VS_INC_ESTATS(ip_vs_esmib, DEFENCE_UDP_DROP);
+			return NF_DROP;
+		}
+	}
+
+	/* synproxy: defence synflood */
+	if (iph.protocol == IPPROTO_TCP) {
+		int v = NF_ACCEPT;
+		if (0 == ip_vs_synproxy_syn_rcv(af, skb, &iph, &v)) {
+			return v;
+		}
+	}
+
+	return NF_ACCEPT;
+}
 
 static struct nf_hook_ops ip_vs_ops[] __read_mostly = {
 	/* After packet filtering, forward packet through VS/DR, VS/TUN,
 	 * or VS/NAT(change destination), so that filtering rules can be
 	 * applied to IPVS. */
 	{
-		.hook		= ip_vs_in,
-		.owner		= THIS_MODULE,
-		.pf		= PF_INET,
-		.hooknum        = NF_INET_LOCAL_IN,
-		.priority       = 100,
-	},
+	 .hook = ip_vs_in,
+	 .owner = THIS_MODULE,
+	 .pf = PF_INET,
+	 .hooknum = NF_INET_PRE_ROUTING,
+	 .priority = NF_IP_PRI_CONNTRACK - 1,
+	 },
 	/* After packet filtering, change source only for VS/NAT */
 	{
 		.hook		= ip_vs_out,
@@ -1458,17 +1513,25 @@ static struct nf_hook_ops ip_vs_ops[] __read_mostly = {
 		.hooknum        = NF_INET_POST_ROUTING,
 		.priority       = NF_IP_PRI_NAT_SRC-1,
 	},
+	/* Before the netfilter connection tracking, only deal with FULLNAT/NAT-SynProxy */
+	{
+	 .hook = ip_vs_pre_routing,
+	 .owner = THIS_MODULE,
+	 .pf = PF_INET,
+	 .hooknum = NF_INET_PRE_ROUTING,
+	 .priority = NF_IP_PRI_CONNTRACK - 2,
+	 },
 #ifdef CONFIG_IP_VS_IPV6
 	/* After packet filtering, forward packet through VS/DR, VS/TUN,
 	 * or VS/NAT(change destination), so that filtering rules can be
 	 * applied to IPVS. */
 	{
-		.hook		= ip_vs_in,
-		.owner		= THIS_MODULE,
-		.pf		= PF_INET6,
-		.hooknum        = NF_INET_LOCAL_IN,
-		.priority       = 100,
-	},
+	 .hook = ip_vs_in,
+	 .owner = THIS_MODULE,
+	 .pf = PF_INET6,
+	 .hooknum = NF_INET_PRE_ROUTING,
+	 .priority = NF_IP6_PRI_CONNTRACK - 1,
+	 },
 	/* After packet filtering, change source only for VS/NAT */
 	{
 		.hook		= ip_vs_out,
@@ -1494,6 +1557,14 @@ static struct nf_hook_ops ip_vs_ops[] __read_mostly = {
 		.hooknum        = NF_INET_POST_ROUTING,
 		.priority       = NF_IP6_PRI_NAT_SRC-1,
 	},
+	/* Before the netfilter connection tracking, only deal with FULLNAT/NAT-SynProxy */
+	{
+	 .hook = ip_vs_pre_routing,
+	 .owner = THIS_MODULE,
+	 .pf = PF_INET6,
+	 .hooknum = NF_INET_PRE_ROUTING,
+	 .priority = NF_IP6_PRI_CONNTRACK - 2,
+	 },
 #endif
 };
 
@@ -1505,12 +1576,10 @@ static int __init ip_vs_init(void)
 {
 	int ret;
 
-	ip_vs_estimator_init();
-
 	ret = ip_vs_control_init();
 	if (ret < 0) {
 		pr_err("can't setup control.\n");
-		goto cleanup_estimator;
+		goto out_err;
 	}
 
 	ip_vs_protocol_init();
@@ -1543,8 +1612,7 @@ static int __init ip_vs_init(void)
   cleanup_protocol:
 	ip_vs_protocol_cleanup();
 	ip_vs_control_cleanup();
-  cleanup_estimator:
-	ip_vs_estimator_cleanup();
+out_err:
 	return ret;
 }
 
@@ -1555,7 +1623,6 @@ static void __exit ip_vs_cleanup(void)
 	ip_vs_app_cleanup();
 	ip_vs_protocol_cleanup();
 	ip_vs_control_cleanup();
-	ip_vs_estimator_cleanup();
 	pr_info("ipvs unloaded.\n");
 }
 
diff --git a/net/netfilter/ipvs/ip_vs_ctl.c b/net/netfilter/ipvs/ip_vs_ctl.c
index 99e85e6..bfb04c2 100644
--- a/net/netfilter/ipvs/ip_vs_ctl.c
+++ b/net/netfilter/ipvs/ip_vs_ctl.c
@@ -16,6 +16,7 @@
  *
  * Changes:
  *
+ *   Yu Bo        <yubo@xiaomi.com>
  */
 
 #define KMSG_COMPONENT "IPVS"
@@ -49,6 +50,7 @@
 #include <asm/uaccess.h>
 
 #include <net/ip_vs.h>
+#include <net/ip_vs_synproxy.h>
 
 /* semaphore for IPVS sockopts. And, [gs]etsockopt may sleep. */
 static DEFINE_MUTEX(__ip_vs_mutex);
@@ -56,6 +58,9 @@ static DEFINE_MUTEX(__ip_vs_mutex);
 /* lock for service table */
 static DEFINE_RWLOCK(__ip_vs_svc_lock);
 
+/* lock for zone table */
+static DEFINE_RWLOCK(__ip_vs_zone_lock);
+
 /* lock for table with the real services */
 static DEFINE_RWLOCK(__ip_vs_rs_lock);
 
@@ -76,6 +81,9 @@ static atomic_t ip_vs_dropentry = ATOMIC_INIT(0);
 /* number of virtual services */
 static int ip_vs_num_services = 0;
 
+/* number of zones */
+static int ip_vs_num_zones = 0;
+
 /* sysctl variables */
 static int sysctl_ip_vs_drop_entry = 0;
 static int sysctl_ip_vs_drop_packet = 0;
@@ -87,7 +95,68 @@ int sysctl_ip_vs_expire_nodest_conn = 0;
 int sysctl_ip_vs_expire_quiescent_template = 0;
 int sysctl_ip_vs_sync_threshold[2] = { 3, 50 };
 int sysctl_ip_vs_nat_icmp_send = 0;
-
+/*
+ * sysctl for FULLNAT
+ */
+int sysctl_ip_vs_timestamp_remove_entry = 1;
+int sysctl_ip_vs_mss_adjust_entry = 1;
+int sysctl_ip_vs_conn_reused_entry = 1;
+int sysctl_ip_vs_toa_entry = 1;
+static int ip_vs_entry_min = 0;
+static int ip_vs_entry_max = 1;
+extern int sysctl_ip_vs_tcp_timeouts[IP_VS_TCP_S_LAST + 1];
+/*
+ * sysctl for SYNPROXY
+ */
+/* syn-proxy sysctl variables */
+int sysctl_ip_vs_synproxy_init_mss = IP_VS_SYNPROXY_INIT_MSS_DEFAULT;
+int sysctl_ip_vs_synproxy_sack = IP_VS_SYNPROXY_SACK_DEFAULT;
+int sysctl_ip_vs_synproxy_wscale = IP_VS_SYNPROXY_WSCALE_DEFAULT;
+int sysctl_ip_vs_synproxy_timestamp = IP_VS_SYNPROXY_TIMESTAMP_DEFAULT;
+int sysctl_ip_vs_synproxy_synack_ttl = IP_VS_SYNPROXY_TTL_DEFAULT;
+int sysctl_ip_vs_synproxy_defer = IP_VS_SYNPROXY_DEFER_DEFAULT;
+int sysctl_ip_vs_synproxy_conn_reuse = IP_VS_SYNPROXY_CONN_REUSE_DEFAULT;
+int sysctl_ip_vs_synproxy_conn_reuse_cl = IP_VS_SYNPROXY_CONN_REUSE_CL_DEFAULT;
+int sysctl_ip_vs_synproxy_conn_reuse_tw = IP_VS_SYNPROXY_CONN_REUSE_TW_DEFAULT;
+int sysctl_ip_vs_synproxy_conn_reuse_fw = IP_VS_SYNPROXY_CONN_REUSE_FW_DEFAULT;
+int sysctl_ip_vs_synproxy_conn_reuse_cw = IP_VS_SYNPROXY_CONN_REUSE_CW_DEFAULT;
+int sysctl_ip_vs_synproxy_conn_reuse_la = IP_VS_SYNPROXY_CONN_REUSE_LA_DEFAULT;
+int sysctl_ip_vs_synproxy_dup_ack_thresh = IP_VS_SYNPROXY_DUP_ACK_DEFAULT;
+int sysctl_ip_vs_synproxy_skb_store_thresh = IP_VS_SYNPROXY_SKB_STORE_DEFAULT;
+int sysctl_ip_vs_synproxy_syn_retry = IP_VS_SYNPROXY_SYN_RETRY_DEFAULT;
+
+static int ip_vs_synproxy_switch_min = 0;
+static int ip_vs_synproxy_switch_max = 1;
+static int ip_vs_synproxy_wscale_min = 0;
+static int ip_vs_synproxy_wscale_max = IP_VS_SYNPROXY_WSCALE_MAX;
+static int ip_vs_synproxy_init_mss_min = 0;
+static int ip_vs_synproxy_init_mss_max = 65535;
+static int ip_vs_synproxy_synack_ttl_min = IP_VS_SYNPROXY_TTL_MIN;
+static int ip_vs_synproxy_synack_ttl_max = IP_VS_SYNPROXY_TTL_MAX;
+static int ip_vs_synproxy_dup_ack_cnt_min = 0;
+static int ip_vs_synproxy_dup_ack_cnt_max = 65535;
+static int ip_vs_synproxy_syn_retry_min = 0;
+static int ip_vs_synproxy_syn_retry_max = 6;
+static int ip_vs_synproxy_skb_store_thresh_min = 0;
+static int ip_vs_synproxy_skb_store_thresh_max = 5;
+/* local address port range */
+int sysctl_ip_vs_lport_max = 65535;
+int sysctl_ip_vs_lport_min = 5000;
+int sysctl_ip_vs_lport_tries = 10000;
+static int ip_vs_port_min = 1025;
+static int ip_vs_port_max = 65535;
+static int ip_vs_port_try_min = 10;
+static int ip_vs_port_try_max = 60000;
+/*
+ * sysctl for DEFENCE ATTACK
+ */
+int sysctl_ip_vs_frag_drop_entry = 0;
+int sysctl_ip_vs_tcp_drop_entry = 1;
+int sysctl_ip_vs_udp_drop_entry = 1;
+/* send rst when tcp session expire */
+int sysctl_ip_vs_conn_expire_tcp_rst = 1;
+/* L2 fast xmit, response only (to client) */
+int sysctl_ip_vs_fast_xmit = 1;
 
 #ifdef CONFIG_IP_VS_DEBUG
 static int sysctl_ip_vs_debug_level = 0;
@@ -284,6 +353,7 @@ static struct list_head ip_vs_svc_table[IP_VS_SVC_TAB_SIZE];
 /* the service table hashed by fwmark */
 static struct list_head ip_vs_svc_fwm_table[IP_VS_SVC_TAB_SIZE];
 
+static struct list_head ip_vs_zone;
 /*
  *	Hash table: for real service lookups
  */
@@ -309,10 +379,8 @@ static atomic_t ip_vs_nullsvc_counter = ATOMIC_INIT(0);
  *	Returns hash value for virtual service
  */
 static __inline__ unsigned
-ip_vs_svc_hashkey(int af, unsigned proto, const union nf_inet_addr *addr,
-		  __be16 port)
+ip_vs_svc_hashkey(int af, unsigned proto, const union nf_inet_addr *addr)
 {
-	register unsigned porth = ntohs(port);
 	__be32 addr_fold = addr->ip;
 
 #ifdef CONFIG_IP_VS_IPV6
@@ -321,8 +389,11 @@ ip_vs_svc_hashkey(int af, unsigned proto, const union nf_inet_addr *addr,
 			    addr->ip6[2]^addr->ip6[3];
 #endif
 
-	return (proto^ntohl(addr_fold)^(porth>>IP_VS_SVC_TAB_BITS)^porth)
-		& IP_VS_SVC_TAB_MASK;
+	if(af & IP_VS_CONN_F_DSNAT){
+		addr_fold = 0;
+	} 
+
+	return (proto ^ ntohl(addr_fold)) & IP_VS_SVC_TAB_MASK;
 }
 
 /*
@@ -352,8 +423,7 @@ static int ip_vs_svc_hash(struct ip_vs_service *svc)
 		/*
 		 *  Hash it by <protocol,addr,port> in ip_vs_svc_table
 		 */
-		hash = ip_vs_svc_hashkey(svc->af, svc->protocol, &svc->addr,
-					 svc->port);
+		hash = ip_vs_svc_hashkey(svc->af, svc->protocol, &svc->addr);
 		list_add(&svc->s_list, &ip_vs_svc_table[hash]);
 	} else {
 		/*
@@ -397,26 +467,79 @@ static int ip_vs_svc_unhash(struct ip_vs_service *svc)
 
 
 /*
+ *	Get zone by {addr,netmask} in the ip_vs_zone list.
+ */
+static inline struct ip_vs_zone *
+__ip_vs_zone_get(const union nf_inet_addr *addr,
+		    __be32 netmask)
+{
+	struct ip_vs_zone *zone;
+	
+	list_for_each_entry(zone, &ip_vs_zone, s_list) {
+		if ((zone->addr.ip == addr->ip)
+			&& (zone->netmask == netmask)) {
+			atomic_inc(&zone->usecnt);
+			return zone;
+		}
+	}
+	return NULL;
+}
+
+/*
+ *	Get zone by {addr} in the ip_vs_zone list.
+ */
+struct ip_vs_zone *
+ip_vs_zone_get(const union nf_inet_addr *addr)
+{
+	struct ip_vs_zone *zone;
+	
+	list_for_each_entry(zone, &ip_vs_zone, s_list) {
+		if (((zone->addr.ip ^ addr->ip) & zone->netmask) == 0)
+		{
+			return zone;
+		}
+	}
+	return NULL;
+}
+
+
+
+/*
  *	Get service by {proto,addr,port} in the service table.
  */
-static inline struct ip_vs_service *
-__ip_vs_service_get(int af, __u16 protocol, const union nf_inet_addr *vaddr,
-		    __be16 vport)
+static inline struct ip_vs_service *__ip_vs_service_get(int af, __u16 protocol,
+							const union nf_inet_addr
+							*vaddr, __be16 vport)
 {
 	unsigned hash;
 	struct ip_vs_service *svc;
+	int dsnat = af & IP_VS_CONN_F_DSNAT;
+	af &= ~IP_VS_CONN_F_DSNAT;
 
 	/* Check for "full" addressed entries */
-	hash = ip_vs_svc_hashkey(af, protocol, vaddr, vport);
-
-	list_for_each_entry(svc, &ip_vs_svc_table[hash], s_list){
-		if ((svc->af == af)
-		    && ip_vs_addr_equal(af, &svc->addr, vaddr)
-		    && (svc->port == vport)
-		    && (svc->protocol == protocol)) {
-			/* HIT */
-			atomic_inc(&svc->usecnt);
-			return svc;
+	hash = ip_vs_svc_hashkey(af|dsnat, protocol, vaddr);
+
+	if(dsnat){
+		list_for_each_entry(svc, &ip_vs_svc_table[hash], s_list) {
+			if ((svc->af == af)
+				&& (svc->addr.ip == 0)
+				&& (svc->port == 0)
+				&& (svc->protocol == protocol)) {
+				/* HIT */
+				atomic_inc(&svc->usecnt);
+				return svc;
+			}
+		}
+	}else{
+		list_for_each_entry(svc, &ip_vs_svc_table[hash], s_list) {
+			if ((svc->af == af)
+			    && ip_vs_addr_equal(af, &svc->addr, vaddr)
+			    && (svc->port == vport)
+			    && (svc->protocol == protocol)) {
+				/* HIT */
+				atomic_inc(&svc->usecnt);
+				return svc;
+			}
 		}
 	}
 
@@ -427,11 +550,11 @@ __ip_vs_service_get(int af, __u16 protocol, const union nf_inet_addr *vaddr,
 /*
  *	Get service by {fwmark} in the service table.
  */
-static inline struct ip_vs_service *
-__ip_vs_svc_fwm_get(int af, __u32 fwmark)
+static inline struct ip_vs_service *__ip_vs_svc_fwm_get(int af, __u32 fwmark)
 {
 	unsigned hash;
 	struct ip_vs_service *svc;
+	af &= ~IP_VS_CONN_F_DSNAT;
 
 	/* Check for fwmark addressed entries */
 	hash = ip_vs_svc_fwm_hashkey(fwmark);
@@ -447,9 +570,15 @@ __ip_vs_svc_fwm_get(int af, __u32 fwmark)
 	return NULL;
 }
 
-struct ip_vs_service *
-ip_vs_service_get(int af, __u32 fwmark, __u16 protocol,
-		  const union nf_inet_addr *vaddr, __be16 vport)
+/*
+struct ip_vs_dsnat *ip_vs_zone_get(void)
+{
+	return &ip_vs_zone;
+}
+*/
+struct ip_vs_service *ip_vs_service_get(int af, __u32 fwmark, __u16 protocol,
+					const union nf_inet_addr *vaddr,
+					__be16 vport)
 {
 	struct ip_vs_service *svc;
 
@@ -468,8 +597,7 @@ ip_vs_service_get(int af, __u32 fwmark, __u16 protocol,
 	svc = __ip_vs_service_get(af, protocol, vaddr, vport);
 
 	if (svc == NULL
-	    && protocol == IPPROTO_TCP
-	    && atomic_read(&ip_vs_ftpsvc_counter)
+	    && protocol == IPPROTO_TCP && atomic_read(&ip_vs_ftpsvc_counter)
 	    && (vport == FTPDATA || ntohs(vport) >= PROT_SOCK)) {
 		/*
 		 * Check if ftp service entry exists, the packet
@@ -478,8 +606,7 @@ ip_vs_service_get(int af, __u32 fwmark, __u16 protocol,
 		svc = __ip_vs_service_get(af, protocol, vaddr, FTPPORT);
 	}
 
-	if (svc == NULL
-	    && atomic_read(&ip_vs_nullsvc_counter)) {
+	if (svc == NULL && atomic_read(&ip_vs_nullsvc_counter)) {
 		/*
 		 * Check if the catch-all port (port zero) exists
 		 */
@@ -489,7 +616,7 @@ ip_vs_service_get(int af, __u32 fwmark, __u16 protocol,
   out:
 	read_unlock(&__ip_vs_svc_lock);
 
-	IP_VS_DBG_BUF(9, "lookup service: fwm %u %s %s:%u %s\n",
+	IP_VS_DBG_BUF(21, "lookup service: fwm %u %s %s:%u %s\n",
 		      fwmark, ip_vs_proto_name(protocol),
 		      IP_VS_DBG_ADDR(af, vaddr), ntohs(vport),
 		      svc ? "hit" : "not hit");
@@ -497,6 +624,43 @@ ip_vs_service_get(int af, __u32 fwmark, __u16 protocol,
 	return svc;
 }
 
+struct ip_vs_service *ip_vs_lookup_vip(int af, __u16 protocol,
+				       const union nf_inet_addr *vaddr)
+{
+	struct ip_vs_service *svc;
+	unsigned hash;
+	int dsnat = af & IP_VS_CONN_F_DSNAT;
+	af &= ~IP_VS_CONN_F_DSNAT; 
+
+	read_lock(&__ip_vs_svc_lock);
+
+	hash = ip_vs_svc_hashkey(af|dsnat, protocol, vaddr);
+	if(dsnat){
+		list_for_each_entry(svc, &ip_vs_svc_table[hash], s_list) {
+			if ((svc->af == af)
+				&& (svc->addr.ip == 0)
+				&& (svc->port == 0) 
+				&& (svc->protocol == protocol)) {  
+				/* HIT */    
+				read_unlock(&__ip_vs_svc_lock);
+				return svc;
+			}
+		}
+	}else{
+		list_for_each_entry(svc, &ip_vs_svc_table[hash], s_list) {
+			if ((svc->af == af)
+			    && ip_vs_addr_equal(af, &svc->addr, vaddr)
+			    && (svc->protocol == protocol)) {
+				/* HIT */
+				read_unlock(&__ip_vs_svc_lock);
+				return svc;
+			}
+		}
+	}
+
+	read_unlock(&__ip_vs_svc_lock);
+	return NULL;
+}
 
 static inline void
 __ip_vs_bind_svc(struct ip_vs_dest *dest, struct ip_vs_service *svc)
@@ -505,8 +669,7 @@ __ip_vs_bind_svc(struct ip_vs_dest *dest, struct ip_vs_service *svc)
 	dest->svc = svc;
 }
 
-static inline void
-__ip_vs_unbind_svc(struct ip_vs_dest *dest)
+static inline void __ip_vs_unbind_svc(struct ip_vs_dest *dest)
 {
 	struct ip_vs_service *svc = dest->svc;
 
@@ -588,8 +751,8 @@ ip_vs_lookup_real_service(int af, __u16 protocol,
 	struct ip_vs_dest *dest;
 
 	/*
-	 *	Check for "full" addressed entries
-	 *	Return the first found entry
+	 *      Check for "full" addressed entries
+	 *      Return the first found entry
 	 */
 	hash = ip_vs_rs_hashkey(af, daddr, dport);
 
@@ -712,6 +875,10 @@ ip_vs_trash_get_dest(struct ip_vs_service *svc, const union nf_inet_addr *daddr,
 			list_del(&dest->n_list);
 			ip_vs_dst_reset(dest);
 			__ip_vs_unbind_svc(dest);
+
+			/* Delete dest dedicated statistic varible which is percpu type */
+			ip_vs_del_stats(dest->stats);
+
 			kfree(dest);
 		}
 	}
@@ -719,7 +886,6 @@ ip_vs_trash_get_dest(struct ip_vs_service *svc, const union nf_inet_addr *daddr,
 	return NULL;
 }
 
-
 /*
  *  Clean up all the destinations in the trash
  *  Called by the ip_vs_control_cleanup()
@@ -737,22 +903,11 @@ static void ip_vs_trash_cleanup(void)
 		list_del(&dest->n_list);
 		ip_vs_dst_reset(dest);
 		__ip_vs_unbind_svc(dest);
+		ip_vs_del_stats(dest->stats);
 		kfree(dest);
 	}
 }
 
-
-static void
-ip_vs_zero_stats(struct ip_vs_stats *stats)
-{
-	spin_lock_bh(&stats->lock);
-
-	memset(&stats->ustats, 0, sizeof(stats->ustats));
-	ip_vs_zero_estimator(stats);
-
-	spin_unlock_bh(&stats->lock);
-}
-
 /*
  *	Update a destination in the given service
  */
@@ -800,7 +955,7 @@ __ip_vs_update_dest(struct ip_vs_service *svc,
 	} else {
 		if (dest->svc != svc) {
 			__ip_vs_unbind_svc(dest);
-			ip_vs_zero_stats(&dest->stats);
+			ip_vs_zero_stats(dest->stats);
 			__ip_vs_bind_svc(dest, svc);
 		}
 	}
@@ -822,6 +977,7 @@ static int
 ip_vs_new_dest(struct ip_vs_service *svc, struct ip_vs_dest_user_kern *udest,
 	       struct ip_vs_dest **dest_p)
 {
+	int ret = 0;
 	struct ip_vs_dest *dest;
 	unsigned atype;
 
@@ -863,14 +1019,23 @@ ip_vs_new_dest(struct ip_vs_service *svc, struct ip_vs_dest_user_kern *udest,
 
 	INIT_LIST_HEAD(&dest->d_list);
 	spin_lock_init(&dest->dst_lock);
-	spin_lock_init(&dest->stats.lock);
+
+	/* Init statistic */
+	ret = ip_vs_new_stats(&(dest->stats));
+	if(ret)
+		goto out_err;
+
 	__ip_vs_update_dest(svc, dest, udest);
-	ip_vs_new_estimator(&dest->stats);
+
 
 	*dest_p = dest;
 
 	LeaveFunction(2);
 	return 0;
+
+out_err:
+	kfree(dest);
+	return ret;
 }
 
 
@@ -932,7 +1097,8 @@ ip_vs_add_dest(struct ip_vs_service *svc, struct ip_vs_dest_user_kern *udest)
 		 */
 		list_del(&dest->n_list);
 
-		ip_vs_new_estimator(&dest->stats);
+		/* Reset the statistic value */
+		ip_vs_zero_stats(dest->stats);
 
 		write_lock_bh(&__ip_vs_svc_lock);
 
@@ -1046,8 +1212,6 @@ ip_vs_edit_dest(struct ip_vs_service *svc, struct ip_vs_dest_user_kern *udest)
  */
 static void __ip_vs_del_dest(struct ip_vs_dest *dest)
 {
-	ip_vs_kill_estimator(&dest->stats);
-
 	/*
 	 *  Remove it from the d-linked list with the real services.
 	 */
@@ -1068,19 +1232,21 @@ static void __ip_vs_del_dest(struct ip_vs_dest *dest)
 		   and only one user context can update virtual service at a
 		   time, so the operation here is OK */
 		atomic_dec(&dest->svc->refcnt);
+
+		/* Delete dest dedicated statistic varible which is percpu type */
+		ip_vs_del_stats(dest->stats);
+
 		kfree(dest);
 	} else {
 		IP_VS_DBG_BUF(3, "Moving dest %s:%u into trash, "
 			      "dest->refcnt=%d\n",
 			      IP_VS_DBG_ADDR(dest->af, &dest->addr),
-			      ntohs(dest->port),
-			      atomic_read(&dest->refcnt));
+			      ntohs(dest->port), atomic_read(&dest->refcnt));
 		list_add(&dest->n_list, &ip_vs_dest_trash);
 		atomic_inc(&dest->refcnt);
 	}
 }
 
-
 /*
  *	Unlink a destination from the given service
  */
@@ -1146,6 +1312,261 @@ ip_vs_del_dest(struct ip_vs_service *svc, struct ip_vs_dest_user_kern *udest)
 	return 0;
 }
 
+void ip_vs_laddr_hold(struct ip_vs_laddr *laddr)
+{
+	atomic_inc(&laddr->refcnt);
+}
+
+void ip_vs_laddr_put(struct ip_vs_laddr *laddr)
+{
+	if (atomic_dec_and_test(&laddr->refcnt)) {
+		kfree(laddr);
+	}
+}
+
+static int
+ip_vs_new_laddr(struct ip_vs_zone *zone, struct ip_vs_laddr_user_kern *uladdr,
+		struct ip_vs_laddr **laddr_p)
+{
+	struct ip_vs_laddr *laddr;
+
+	laddr = kzalloc(sizeof(struct ip_vs_laddr), GFP_ATOMIC);
+	if (!laddr) {
+		pr_err("%s(): no memory.\n", __func__);
+		return -ENOMEM;
+	}
+
+	laddr->af = AF_INET;
+	ip_vs_addr_copy(AF_INET, &laddr->addr, &uladdr->addr);
+	atomic64_set(&laddr->port_conflict, 0);
+	atomic64_set(&laddr->port, 0);
+	atomic_set(&laddr->refcnt, 0);
+	atomic_set(&laddr->conn_counts, 0);
+
+	*laddr_p = laddr;
+
+	return 0;
+}
+
+static struct ip_vs_laddr *ip_vs_lookup_laddr(struct ip_vs_zone *zone,
+					      const union nf_inet_addr *addr)
+{
+	struct ip_vs_laddr *laddr;
+
+	/*
+	 * Find the local address for the given service
+	 */
+	list_for_each_entry(laddr, &zone->laddr_list, n_list) {
+		if (ip_vs_addr_equal(AF_INET, &laddr->addr, addr)) {
+			/* HIT */
+			return laddr;
+		}
+	}
+
+	return NULL;
+}
+
+static int
+ip_vs_add_laddr(struct ip_vs_zone *zone, struct ip_vs_laddr_user_kern *uladdr)
+{
+	struct ip_vs_laddr *laddr;
+	int ret;
+#ifdef CONFIG_IP_VS_DEBUG
+	union nf_inet_addr netmask = {.ip = zone->netmask};
+#endif	
+
+	IP_VS_DBG_BUF(0, "zone %s/%s add local address %s\n",
+		      IP_VS_DBG_ADDR(AF_INET, &zone->addr), IP_VS_DBG_ADDR(AF_INET, &netmask),
+		      IP_VS_DBG_ADDR(AF_INET, &uladdr->addr));
+
+	/*
+	 * Check if the local address already exists in the list
+	 */
+	laddr = ip_vs_lookup_laddr(zone, &uladdr->addr);
+	if (laddr) {
+		IP_VS_DBG(1, "%s(): local address already exists\n", __func__);
+		return -EEXIST;
+	}
+
+	/*
+	 * Allocate and initialize the dest structure
+	 */
+	ret = ip_vs_new_laddr(zone, uladdr, &laddr);
+	if (ret) {
+		return ret;
+	}
+
+	/*
+	 * Add the local adress entry into the list
+	 */
+	ip_vs_laddr_hold(laddr);
+
+	write_lock_bh(&__ip_vs_zone_lock);
+
+	/*
+	 * Wait until all other svc users go away.
+	 */
+	IP_VS_WAIT_WHILE(atomic_read(&zone->usecnt) > 1);
+
+	list_add_tail(&laddr->n_list, &zone->laddr_list);
+	zone->num_laddrs++;
+
+#ifdef CONFIG_IP_VS_DEBUG
+	/* Dump the destinations */
+	IP_VS_DBG_BUF(0, "		zone %s/%s num %d curr %p \n",
+		      IP_VS_DBG_ADDR(AF_INET, &zone->addr),
+		      IP_VS_DBG_ADDR(AF_INET, &netmask),
+		      zone->num_laddrs, zone->curr_laddr);
+	list_for_each_entry(laddr, &zone->laddr_list, n_list) {
+		IP_VS_DBG_BUF(0, "		laddr %p %s:%d \n",
+			      laddr, IP_VS_DBG_ADDR(AF_INET, &laddr->addr), 0);
+	}
+#endif
+
+	write_unlock_bh(&__ip_vs_zone_lock);
+
+	return 0;
+}
+
+static int
+ip_vs_del_laddr(struct ip_vs_zone *zone, struct ip_vs_laddr_user_kern *uladdr)
+{
+	struct ip_vs_laddr *laddr;
+#ifdef CONFIG_IP_VS_DEBUG	
+	union nf_inet_addr netmask = {.ip = zone->netmask};
+#endif
+
+	IP_VS_DBG_BUF(0, "zone %s/%s del local address %s\n",
+		      IP_VS_DBG_ADDR(AF_INET, &zone->addr),
+		      IP_VS_DBG_ADDR(AF_INET, &netmask),
+		      IP_VS_DBG_ADDR(AF_INET, &uladdr->addr));
+
+	laddr = ip_vs_lookup_laddr(zone, &uladdr->addr);
+
+	if (laddr == NULL) {
+		IP_VS_DBG(1, "%s(): local address not found!\n", __func__);
+		return -ENOENT;
+	}
+
+	write_lock_bh(&__ip_vs_zone_lock);
+
+	/*
+	 *      Wait until all other svc users go away.
+	 */
+	IP_VS_WAIT_WHILE(atomic_read(&zone->usecnt) > 1);
+
+	/* update svc->curr_laddr */
+	if (zone->curr_laddr == &laddr->n_list)
+		zone->curr_laddr = laddr->n_list.next;
+	/*
+	 *      Unlink dest from the service
+	 */
+	list_del(&laddr->n_list);
+	zone->num_laddrs--;
+
+#ifdef CONFIG_IP_VS_DEBUG
+	IP_VS_DBG_BUF(0, "	zone %s/%s num %d curr %p \n",
+		      IP_VS_DBG_ADDR(AF_INET, &zone->addr),
+		      IP_VS_DBG_ADDR(AF_INET, &netmask),
+		      zone->num_laddrs, zone->curr_laddr);
+	list_for_each_entry(laddr, &zone->laddr_list, n_list) {
+		IP_VS_DBG_BUF(0, "		laddr %p %s:%d \n",
+			      laddr, IP_VS_DBG_ADDR(AF_INET, &laddr->addr), 0);
+	}
+#endif
+
+	ip_vs_laddr_put(laddr);
+
+	write_unlock_bh(&__ip_vs_zone_lock);
+
+	return 0;
+}
+
+
+static int
+ip_vs_add_zone(struct ip_vs_zone_user_kern *u,
+		struct ip_vs_zone **zone_p)
+{
+	int ret = 0;
+	struct ip_vs_zone *zone = NULL;
+
+	/* increase the module use count */
+	ip_vs_use_count_inc();
+
+	/* Lookup the scheduler by 'u->sched_name' */
+
+	zone = kzalloc(sizeof(struct ip_vs_zone), GFP_ATOMIC);
+	if (zone == NULL) {
+		IP_VS_DBG(1, "%s(): no memory\n", __func__);
+		ret = -ENOMEM;
+		goto out_err;
+	}
+
+	/* I'm the first user of the service */
+	atomic_set(&zone->usecnt, 1);
+	atomic_set(&zone->refcnt, 0);
+
+	zone->af = AF_INET;
+	zone->addr.ip = u->addr.ip;
+	zone->netmask = u->netmask;
+
+	/* Init the local address stuff */
+	rwlock_init(&zone->laddr_lock);
+	INIT_LIST_HEAD(&zone->laddr_list);
+	zone->num_laddrs = 0;
+	zone->curr_laddr = &zone->laddr_list;
+
+
+	ip_vs_num_zones++;
+	write_lock_bh(&__ip_vs_zone_lock);
+	list_add_tail(&zone->s_list, &ip_vs_zone);
+	atomic_inc(&zone->refcnt);
+	write_unlock_bh(&__ip_vs_zone_lock);
+	
+	*zone_p = zone;
+
+	return 0;
+
+  out_err:
+	if (zone != NULL) {
+		kfree(zone);
+	}
+
+	/* decrease the module use count */
+	ip_vs_use_count_dec();
+
+	return ret;
+}
+
+
+
+/*
+ *	Delete a zone from the zone list
+ */
+static int ip_vs_del_zone(struct ip_vs_zone *zone)
+{
+	struct ip_vs_laddr *laddr, *laddr_next;
+
+	if (zone == NULL)
+		return -ENOENT;
+
+	write_lock_bh(&__ip_vs_zone_lock);
+	list_del(&zone->s_list);
+
+	ip_vs_num_zones--;
+
+	/* Unlink the whole local address list */
+	list_for_each_entry_safe(laddr, laddr_next, &zone->laddr_list, n_list) {
+		list_del(&laddr->n_list);
+		ip_vs_laddr_put(laddr);
+	}
+
+	write_unlock_bh(&__ip_vs_zone_lock);
+
+	return 0;
+}
+
+
 
 /*
  *	Add a service into the service hash table
@@ -1196,9 +1617,9 @@ ip_vs_add_service(struct ip_vs_service_user_kern *u,
 	svc->timeout = u->timeout * HZ;
 	svc->netmask = u->netmask;
 
+
 	INIT_LIST_HEAD(&svc->destinations);
 	rwlock_init(&svc->sched_lock);
-	spin_lock_init(&svc->stats.lock);
 
 	/* Bind the scheduler */
 	ret = ip_vs_bind_scheduler(svc, sched);
@@ -1212,7 +1633,10 @@ ip_vs_add_service(struct ip_vs_service_user_kern *u,
 	else if (svc->port == 0)
 		atomic_inc(&ip_vs_nullsvc_counter);
 
-	ip_vs_new_estimator(&svc->stats);
+	/* Init statistic */
+	ret = ip_vs_new_stats(&(svc->stats));
+	if(ret)
+		goto out_err;
 
 	/* Count only IPv4 services for old get/setsockopt interface */
 	if (svc->af == AF_INET)
@@ -1344,7 +1768,12 @@ static void __ip_vs_del_service(struct ip_vs_service *svc)
 	if (svc->af == AF_INET)
 		ip_vs_num_services--;
 
-	ip_vs_kill_estimator(&svc->stats);
+
+	/*
+	 *    Free statistic related per cpu memory
+	 */
+	ip_vs_del_stats(svc->stats);
+
 
 	/* Unbind scheduler */
 	old_sched = svc->scheduler;
@@ -1358,6 +1787,7 @@ static void __ip_vs_del_service(struct ip_vs_service *svc)
 		svc->inc = NULL;
 	}
 
+
 	/*
 	 *    Unlink the whole destination list
 	 */
@@ -1466,9 +1896,9 @@ static int ip_vs_zero_service(struct ip_vs_service *svc)
 
 	write_lock_bh(&__ip_vs_svc_lock);
 	list_for_each_entry(dest, &svc->destinations, n_list) {
-		ip_vs_zero_stats(&dest->stats);
+		ip_vs_zero_stats(dest->stats);
 	}
-	ip_vs_zero_stats(&svc->stats);
+	ip_vs_zero_stats(svc->stats);
 	write_unlock_bh(&__ip_vs_svc_lock);
 	return 0;
 }
@@ -1490,11 +1920,10 @@ static int ip_vs_zero_all(void)
 		}
 	}
 
-	ip_vs_zero_stats(&ip_vs_stats);
+	ip_vs_zero_stats(ip_vs_stats);
 	return 0;
 }
 
-
 static int
 proc_do_defense_mode(ctl_table *table, int write,
 		     void __user *buffer, size_t *lenp, loff_t *ppos)
@@ -1585,92 +2014,76 @@ static struct ctl_table vs_vars[] = {
 		.mode		= 0644,
 		.proc_handler	= proc_do_defense_mode,
 	},
-#if 0
 	{
-		.procname	= "timeout_established",
-		.data	= &vs_timeout_table_dos.timeout[IP_VS_S_ESTABLISHED],
-		.maxlen		= sizeof(int),
-		.mode		= 0644,
-		.proc_handler	= proc_dointvec_jiffies,
-	},
+	 .procname = "timeout_established",
+	 .data = &sysctl_ip_vs_tcp_timeouts[IP_VS_TCP_S_ESTABLISHED],
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = proc_dointvec_jiffies,
+	 },
 	{
-		.procname	= "timeout_synsent",
-		.data	= &vs_timeout_table_dos.timeout[IP_VS_S_SYN_SENT],
-		.maxlen		= sizeof(int),
-		.mode		= 0644,
-		.proc_handler	= proc_dointvec_jiffies,
-	},
+	 .procname = "timeout_synsent",
+	 .data = &sysctl_ip_vs_tcp_timeouts[IP_VS_TCP_S_SYN_SENT],
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = proc_dointvec_jiffies,
+	 },
 	{
-		.procname	= "timeout_synrecv",
-		.data	= &vs_timeout_table_dos.timeout[IP_VS_S_SYN_RECV],
-		.maxlen		= sizeof(int),
-		.mode		= 0644,
-		.proc_handler	= proc_dointvec_jiffies,
-	},
+	 .procname = "timeout_synrecv",
+	 .data = &sysctl_ip_vs_tcp_timeouts[IP_VS_TCP_S_SYN_RECV],
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = proc_dointvec_jiffies,
+	 },
 	{
-		.procname	= "timeout_finwait",
-		.data	= &vs_timeout_table_dos.timeout[IP_VS_S_FIN_WAIT],
-		.maxlen		= sizeof(int),
-		.mode		= 0644,
-		.proc_handler	= proc_dointvec_jiffies,
-	},
+	 .procname = "timeout_finwait",
+	 .data = &sysctl_ip_vs_tcp_timeouts[IP_VS_TCP_S_FIN_WAIT],
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = proc_dointvec_jiffies,
+	 },
 	{
-		.procname	= "timeout_timewait",
-		.data	= &vs_timeout_table_dos.timeout[IP_VS_S_TIME_WAIT],
-		.maxlen		= sizeof(int),
-		.mode		= 0644,
-		.proc_handler	= proc_dointvec_jiffies,
-	},
+	 .procname = "timeout_timewait",
+	 .data = &sysctl_ip_vs_tcp_timeouts[IP_VS_TCP_S_TIME_WAIT],
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = proc_dointvec_jiffies,
+	 },
 	{
-		.procname	= "timeout_close",
-		.data	= &vs_timeout_table_dos.timeout[IP_VS_S_CLOSE],
-		.maxlen		= sizeof(int),
-		.mode		= 0644,
-		.proc_handler	= proc_dointvec_jiffies,
-	},
+	 .procname = "timeout_close",
+	 .data = &sysctl_ip_vs_tcp_timeouts[IP_VS_TCP_S_CLOSE],
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = proc_dointvec_jiffies,
+	 },
 	{
-		.procname	= "timeout_closewait",
-		.data	= &vs_timeout_table_dos.timeout[IP_VS_S_CLOSE_WAIT],
-		.maxlen		= sizeof(int),
-		.mode		= 0644,
-		.proc_handler	= proc_dointvec_jiffies,
-	},
+	 .procname = "timeout_closewait",
+	 .data = &sysctl_ip_vs_tcp_timeouts[IP_VS_TCP_S_CLOSE_WAIT],
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = proc_dointvec_jiffies,
+	 },
 	{
-		.procname	= "timeout_lastack",
-		.data	= &vs_timeout_table_dos.timeout[IP_VS_S_LAST_ACK],
-		.maxlen		= sizeof(int),
-		.mode		= 0644,
-		.proc_handler	= proc_dointvec_jiffies,
-	},
+	 .procname = "timeout_lastack",
+	 .data = &sysctl_ip_vs_tcp_timeouts[IP_VS_TCP_S_LAST_ACK],
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = proc_dointvec_jiffies,
+	 },
 	{
-		.procname	= "timeout_listen",
-		.data	= &vs_timeout_table_dos.timeout[IP_VS_S_LISTEN],
-		.maxlen		= sizeof(int),
-		.mode		= 0644,
-		.proc_handler	= proc_dointvec_jiffies,
-	},
-	{
-		.procname	= "timeout_synack",
-		.data	= &vs_timeout_table_dos.timeout[IP_VS_S_SYNACK],
-		.maxlen		= sizeof(int),
-		.mode		= 0644,
-		.proc_handler	= proc_dointvec_jiffies,
-	},
+	 .procname = "timeout_listen",
+	 .data = &sysctl_ip_vs_tcp_timeouts[IP_VS_TCP_S_LISTEN],
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = proc_dointvec_jiffies,
+	 },
 	{
-		.procname	= "timeout_udp",
-		.data	= &vs_timeout_table_dos.timeout[IP_VS_S_UDP],
-		.maxlen		= sizeof(int),
-		.mode		= 0644,
-		.proc_handler	= proc_dointvec_jiffies,
-	},
-	{
-		.procname	= "timeout_icmp",
-		.data	= &vs_timeout_table_dos.timeout[IP_VS_S_ICMP],
-		.maxlen		= sizeof(int),
-		.mode		= 0644,
-		.proc_handler	= proc_dointvec_jiffies,
-	},
-#endif
+	 .procname = "timeout_synack",
+	 .data = &sysctl_ip_vs_tcp_timeouts[IP_VS_TCP_S_SYNACK],
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = proc_dointvec_jiffies,
+	 },
 	{
 		.procname	= "cache_bypass",
 		.data		= &sysctl_ip_vs_cache_bypass,
@@ -1700,11 +2113,284 @@ static struct ctl_table vs_vars[] = {
 		.proc_handler	= proc_do_sync_threshold,
 	},
 	{
-		.procname	= "nat_icmp_send",
-		.data		= &sysctl_ip_vs_nat_icmp_send,
-		.maxlen		= sizeof(int),
-		.mode		= 0644,
-		.proc_handler	= proc_dointvec,
+	 .procname = "nat_icmp_send",
+	 .data = &sysctl_ip_vs_nat_icmp_send,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = proc_dointvec,
+	 },
+	{
+	 .procname = "fullnat_timestamp_remove_entry",
+	 .data = &sysctl_ip_vs_timestamp_remove_entry,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = &proc_dointvec_minmax,
+	 .strategy = &sysctl_intvec,
+	 .extra1 = &ip_vs_entry_min,
+	 .extra2 = &ip_vs_entry_max,
+	 },
+	{
+	 .procname = "fullnat_mss_adjust_entry",
+	 .data = &sysctl_ip_vs_mss_adjust_entry,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = &proc_dointvec_minmax,
+	 .strategy = &sysctl_intvec,
+	 .extra1 = &ip_vs_entry_min,
+	 .extra2 = &ip_vs_entry_max,
+	 },
+	{
+	 .procname = "fullnat_conn_reused_entry",
+	 .data = &sysctl_ip_vs_conn_reused_entry,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = &proc_dointvec_minmax,
+	 .strategy = &sysctl_intvec,
+	 .extra1 = &ip_vs_entry_min,
+	 .extra2 = &ip_vs_entry_max,
+	 },
+	{
+	 .procname = "fullnat_toa_entry",
+	 .data = &sysctl_ip_vs_toa_entry,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = &proc_dointvec_minmax,
+	 .strategy = &sysctl_intvec,
+	 .extra1 = &ip_vs_entry_min,
+	 .extra2 = &ip_vs_entry_max,
+	 },
+	{
+	 .procname = "fullnat_lport_max",
+	 .data = &sysctl_ip_vs_lport_max,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = &proc_dointvec_minmax,
+	 .strategy = &sysctl_intvec,
+	 .extra1 = &ip_vs_port_min,
+	 .extra2 = &ip_vs_port_max,
+	 },
+	{
+	 .procname = "fullnat_lport_min",
+	 .data = &sysctl_ip_vs_lport_min,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = &proc_dointvec_minmax,
+	 .strategy = &sysctl_intvec,
+	 .extra1 = &ip_vs_port_min,
+	 .extra2 = &ip_vs_port_max,
+	 },
+	{
+	 .procname = "fullnat_lport_tries",
+	 .data = &sysctl_ip_vs_lport_tries,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = &proc_dointvec_minmax,
+	 .strategy = &sysctl_intvec,
+	 .extra1 = &ip_vs_port_try_min,
+	 .extra2 = &ip_vs_port_try_max,
+	 },
+	/* syn-proxy sysctl variables */
+	{
+	 .procname = "synproxy_init_mss",
+	 .data = &sysctl_ip_vs_synproxy_init_mss,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = &proc_dointvec_minmax,
+	 .extra1 = &ip_vs_synproxy_init_mss_min,
+	 .extra2 = &ip_vs_synproxy_init_mss_max,
+	 .strategy = &sysctl_intvec,
+	 },
+	{
+	 .procname = "synproxy_sack",
+	 .data = &sysctl_ip_vs_synproxy_sack,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = &proc_dointvec_minmax,
+	 .extra1 = &ip_vs_synproxy_switch_min,
+	 .extra2 = &ip_vs_synproxy_switch_max,
+	 .strategy = &sysctl_intvec,
+	 },
+	{
+	 .procname = "synproxy_wscale",
+	 .data = &sysctl_ip_vs_synproxy_wscale,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = &proc_dointvec_minmax,
+	 .extra1 = &ip_vs_synproxy_wscale_min,
+	 .extra2 = &ip_vs_synproxy_wscale_max,
+	 .strategy = &sysctl_intvec,
+	 },
+	{
+	 .procname = "synproxy_timestamp",
+	 .data = &sysctl_ip_vs_synproxy_timestamp,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = &proc_dointvec_minmax,
+	 .extra1 = &ip_vs_synproxy_switch_min,
+	 .extra2 = &ip_vs_synproxy_switch_max,
+	 .strategy = &sysctl_intvec,
+	 },
+	{
+	 .procname = "synproxy_synack_ttl",
+	 .data = &sysctl_ip_vs_synproxy_synack_ttl,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = &proc_dointvec_minmax,
+	 .extra1 = &ip_vs_synproxy_synack_ttl_min,
+	 .extra2 = &ip_vs_synproxy_synack_ttl_max,
+	 .strategy = &sysctl_intvec,
+	 },
+	{
+	 .procname = "synproxy_defer",
+	 .data = &sysctl_ip_vs_synproxy_defer,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = &proc_dointvec_minmax,
+	 .extra1 = &ip_vs_synproxy_switch_min,
+	 .extra2 = &ip_vs_synproxy_switch_max,
+	 .strategy = &sysctl_intvec,
+	 },
+	{
+	 .procname = "synproxy_conn_reuse",
+	 .data = &sysctl_ip_vs_synproxy_conn_reuse,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = &proc_dointvec_minmax,
+	 .extra1 = &ip_vs_synproxy_switch_min,
+	 .extra2 = &ip_vs_synproxy_switch_max,
+	 .strategy = &sysctl_intvec,
+	 },
+	{
+	 .procname = "synproxy_conn_reuse_close",
+	 .data = &sysctl_ip_vs_synproxy_conn_reuse_cl,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = &proc_dointvec_minmax,
+	 .extra1 = &ip_vs_synproxy_switch_min,
+	 .extra2 = &ip_vs_synproxy_switch_max,
+	 .strategy = &sysctl_intvec,
+	 },
+	{
+	 .procname = "synproxy_conn_reuse_time_wait",
+	 .data = &sysctl_ip_vs_synproxy_conn_reuse_tw,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = &proc_dointvec_minmax,
+	 .extra1 = &ip_vs_synproxy_switch_min,
+	 .extra2 = &ip_vs_synproxy_switch_max,
+	 .strategy = &sysctl_intvec,
+	 },
+	{
+	 .procname = "synproxy_conn_reuse_fin_wait",
+	 .data = &sysctl_ip_vs_synproxy_conn_reuse_fw,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = &proc_dointvec_minmax,
+	 .extra1 = &ip_vs_synproxy_switch_min,
+	 .extra2 = &ip_vs_synproxy_switch_max,
+	 .strategy = &sysctl_intvec,
+	 },
+	{
+	 .procname = "synproxy_conn_reuse_close_wait",
+	 .data = &sysctl_ip_vs_synproxy_conn_reuse_cw,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = &proc_dointvec_minmax,
+	 .extra1 = &ip_vs_synproxy_switch_min,
+	 .extra2 = &ip_vs_synproxy_switch_max,
+	 .strategy = &sysctl_intvec,
+	 },
+	{
+	 .procname = "synproxy_conn_reuse_last_ack",
+	 .data = &sysctl_ip_vs_synproxy_conn_reuse_la,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = &proc_dointvec_minmax,
+	 .extra1 = &ip_vs_synproxy_switch_min,
+	 .extra2 = &ip_vs_synproxy_switch_max,
+	 .strategy = &sysctl_intvec,
+	 },
+	{
+	 .procname = "synproxy_ack_skb_store_thresh",
+	 .data = &sysctl_ip_vs_synproxy_skb_store_thresh,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = &proc_dointvec_minmax,
+	 .extra1 = &ip_vs_synproxy_skb_store_thresh_min,
+	 .extra2 = &ip_vs_synproxy_skb_store_thresh_max,
+	 .strategy = &sysctl_intvec,
+	 },
+	{
+	 .procname = "synproxy_ack_storm_thresh",
+	 .data = &sysctl_ip_vs_synproxy_dup_ack_thresh,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = &proc_dointvec_minmax,
+	 .extra1 = &ip_vs_synproxy_dup_ack_cnt_min,
+	 .extra2 = &ip_vs_synproxy_dup_ack_cnt_max,
+	 .strategy = &sysctl_intvec,
+	 },
+	{
+	 .procname = "synproxy_syn_retry",
+	 .data = &sysctl_ip_vs_synproxy_syn_retry,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = &proc_dointvec_minmax,
+	 .extra1 = &ip_vs_synproxy_syn_retry_min,
+	 .extra2 = &ip_vs_synproxy_syn_retry_max,
+	 .strategy = &sysctl_intvec,
+	 },
+	/* attack-defence sysctl variables */
+	{
+	 .procname = "defence_tcp_drop",
+	 .data = &sysctl_ip_vs_tcp_drop_entry,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = &proc_dointvec_minmax,
+	 .strategy = &sysctl_intvec,
+	 .extra1 = &ip_vs_entry_min,
+	 .extra2 = &ip_vs_entry_max,
+	 },
+	{
+	 .procname = "defence_udp_drop",
+	 .data = &sysctl_ip_vs_udp_drop_entry,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = &proc_dointvec_minmax,
+	 .strategy = &sysctl_intvec,
+	 .extra1 = &ip_vs_entry_min,
+	 .extra2 = &ip_vs_entry_max,
+	 },
+	{
+	 .procname = "defence_frag_drop",
+	 .data = &sysctl_ip_vs_frag_drop_entry,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = &proc_dointvec_minmax,
+	 .strategy = &sysctl_intvec,
+	 .extra1 = &ip_vs_entry_min,
+	 .extra2 = &ip_vs_entry_max,
+	 },
+	/* send rst sysctl variables */
+	{
+	 .procname = "conn_expire_tcp_rst",
+	 .data = &sysctl_ip_vs_conn_expire_tcp_rst,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = &proc_dointvec_minmax,
+	 .strategy = &sysctl_intvec,
+	 .extra1 = &ip_vs_entry_min,	/* zero */
+	 .extra2 = &ip_vs_entry_max,	/* one */
+	 },
+	{
+	 .procname = "fast_response_xmit",
+	 .data = &sysctl_ip_vs_fast_xmit,
+	 .maxlen = sizeof(int),
+	 .mode = 0644,
+	 .proc_handler = &proc_dointvec_minmax,
+	 .strategy = &sysctl_intvec,
+	 .extra1 = &ip_vs_entry_min,	/* zero */
+	 .extra2 = &ip_vs_entry_max,	/* one */
 	},
 	{ .ctl_name = 0 }
 };
@@ -1739,6 +2425,8 @@ static inline const char *ip_vs_fwd_name(unsigned flags)
 		return "Tunnel";
 	case IP_VS_CONN_F_DROUTE:
 		return "Route";
+	case IP_VS_CONN_F_FULLNAT:
+		return "FullNat";
 	default:
 		return "Masq";
 	}
@@ -1938,36 +2626,27 @@ static const struct file_operations ip_vs_info_fops = {
 
 #endif
 
-struct ip_vs_stats ip_vs_stats = {
-	.lock = __SPIN_LOCK_UNLOCKED(ip_vs_stats.lock),
-};
+struct ip_vs_stats *ip_vs_stats;
 
 #ifdef CONFIG_PROC_FS
 static int ip_vs_stats_show(struct seq_file *seq, void *v)
 {
+	int i = 0;
 
-/*               01234567 01234567 01234567 0123456701234567 0123456701234567 */
 	seq_puts(seq,
-		 "   Total Incoming Outgoing         Incoming         Outgoing\n");
-	seq_printf(seq,
-		   "   Conns  Packets  Packets            Bytes            Bytes\n");
-
-	spin_lock_bh(&ip_vs_stats.lock);
-	seq_printf(seq, "%8X %8X %8X %16LX %16LX\n\n", ip_vs_stats.ustats.conns,
-		   ip_vs_stats.ustats.inpkts, ip_vs_stats.ustats.outpkts,
-		   (unsigned long long) ip_vs_stats.ustats.inbytes,
-		   (unsigned long long) ip_vs_stats.ustats.outbytes);
-
-/*                 01234567 01234567 01234567 0123456701234567 0123456701234567 */
+	       /* ++++01234567890123456++++01234567890123456++++01234567890123456++++01234567890123456++++01234567890123456*/
+		"	          Total             Incoming             Outgoing             Incoming             Outgoing\n");
 	seq_puts(seq,
-		   " Conns/s   Pkts/s   Pkts/s          Bytes/s          Bytes/s\n");
-	seq_printf(seq,"%8X %8X %8X %16X %16X\n",
-			ip_vs_stats.ustats.cps,
-			ip_vs_stats.ustats.inpps,
-			ip_vs_stats.ustats.outpps,
-			ip_vs_stats.ustats.inbps,
-			ip_vs_stats.ustats.outbps);
-	spin_unlock_bh(&ip_vs_stats.lock);
+		"	          Conns	             Packets		  Packets                Bytes                Bytes\n");
+
+	for_each_online_cpu(i) {
+		seq_printf(seq, "CPU%2d:%17Ld    %17Ld    %17Ld    %17Ld    %17Ld\n", i,
+			ip_vs_stats_cpu(ip_vs_stats, i).conns,
+			ip_vs_stats_cpu(ip_vs_stats, i).inpkts,
+			ip_vs_stats_cpu(ip_vs_stats, i).outpkts,
+			ip_vs_stats_cpu(ip_vs_stats, i).inbytes,
+			ip_vs_stats_cpu(ip_vs_stats, i).outbytes);
+	}
 
 	return 0;
 }
@@ -1987,6 +2666,116 @@ static const struct file_operations ip_vs_stats_fops = {
 
 #endif
 
+#ifdef CONFIG_PROC_FS
+/*
+ * Statistics for FULLNAT and SYNPROXY
+ * in /proc/net/ip_vs_ext_stats
+ */
+
+struct ip_vs_estats_mib *ip_vs_esmib;
+
+static struct ip_vs_estats_entry ext_stats[] = {
+	IP_VS_ESTATS_ITEM("fullnat_add_toa_ok", FULLNAT_ADD_TOA_OK),
+	IP_VS_ESTATS_ITEM("fullnat_add_toa_fail_len", FULLNAT_ADD_TOA_FAIL_LEN),
+	IP_VS_ESTATS_ITEM("fullnat_add_toa_head_full", FULLNAT_ADD_TOA_HEAD_FULL),
+	IP_VS_ESTATS_ITEM("fullnat_add_toa_fail_mem", FULLNAT_ADD_TOA_FAIL_MEM),
+	IP_VS_ESTATS_ITEM("fullnat_add_toa_fail_proto",
+			  FULLNAT_ADD_TOA_FAIL_PROTO),
+	IP_VS_ESTATS_ITEM("fullnat_conn_reused", FULLNAT_CONN_REUSED),
+	IP_VS_ESTATS_ITEM("fullnat_conn_reused_close",
+			  FULLNAT_CONN_REUSED_CLOSE),
+	IP_VS_ESTATS_ITEM("fullnat_conn_reused_timewait",
+			  FULLNAT_CONN_REUSED_TIMEWAIT),
+	IP_VS_ESTATS_ITEM("fullnat_conn_reused_finwait",
+			  FULLNAT_CONN_REUSED_FINWAIT),
+	IP_VS_ESTATS_ITEM("fullnat_conn_reused_closewait",
+			  FULLNAT_CONN_REUSED_CLOSEWAIT),
+	IP_VS_ESTATS_ITEM("fullnat_conn_reused_lastack",
+			  FULLNAT_CONN_REUSED_LASTACK),
+	IP_VS_ESTATS_ITEM("fullnat_conn_reused_estab",
+			  FULLNAT_CONN_REUSED_ESTAB),
+	IP_VS_ESTATS_ITEM("synproxy_rs_error", SYNPROXY_RS_ERROR),
+	IP_VS_ESTATS_ITEM("synproxy_null_ack", SYNPROXY_NULL_ACK),
+	IP_VS_ESTATS_ITEM("synproxy_bad_ack", SYNPROXY_BAD_ACK),
+	IP_VS_ESTATS_ITEM("synproxy_ok_ack", SYNPROXY_OK_ACK),
+	IP_VS_ESTATS_ITEM("synproxy_syn_cnt", SYNPROXY_SYN_CNT),
+	IP_VS_ESTATS_ITEM("synproxy_ackstorm", SYNPROXY_ACK_STORM),
+	IP_VS_ESTATS_ITEM("synproxy_synsend_qlen", SYNPROXY_SYNSEND_QLEN),
+	IP_VS_ESTATS_ITEM("synproxy_conn_reused", SYNPROXY_CONN_REUSED),
+	IP_VS_ESTATS_ITEM("synproxy_conn_reused_close",
+			  SYNPROXY_CONN_REUSED_CLOSE),
+	IP_VS_ESTATS_ITEM("synproxy_conn_reused_timewait",
+			  SYNPROXY_CONN_REUSED_TIMEWAIT),
+	IP_VS_ESTATS_ITEM("synproxy_conn_reused_finwait",
+			  SYNPROXY_CONN_REUSED_FINWAIT),
+	IP_VS_ESTATS_ITEM("synproxy_conn_reused_closewait",
+			  SYNPROXY_CONN_REUSED_CLOSEWAIT),
+	IP_VS_ESTATS_ITEM("synproxy_conn_reused_lastack",
+			  SYNPROXY_CONN_REUSED_LASTACK),
+	IP_VS_ESTATS_ITEM("defence_ip_frag_drop", DEFENCE_IP_FRAG_DROP),
+	IP_VS_ESTATS_ITEM("defence_ip_frag_gather", DEFENCE_IP_FRAG_GATHER),
+	IP_VS_ESTATS_ITEM("defence_tcp_drop", DEFENCE_TCP_DROP),
+	IP_VS_ESTATS_ITEM("defence_udp_drop", DEFENCE_UDP_DROP),
+	IP_VS_ESTATS_ITEM("fast_xmit_reject", FAST_XMIT_REJECT),
+	IP_VS_ESTATS_ITEM("fast_xmit_pass", FAST_XMIT_PASS),
+	IP_VS_ESTATS_ITEM("fast_xmit_skb_copy", FAST_XMIT_SKB_COPY),
+	IP_VS_ESTATS_ITEM("fast_xmit_no_mac", FAST_XMIT_NO_MAC),
+	IP_VS_ESTATS_ITEM("fast_xmit_synproxy_save", FAST_XMIT_SYNPROXY_SAVE),
+	IP_VS_ESTATS_ITEM("fast_xmit_dev_lost", FAST_XMIT_DEV_LOST),
+	IP_VS_ESTATS_ITEM("rst_in_syn_sent", RST_IN_SYN_SENT),
+	IP_VS_ESTATS_ITEM("rst_out_syn_sent", RST_OUT_SYN_SENT),
+	IP_VS_ESTATS_ITEM("rst_in_established", RST_IN_ESTABLISHED),
+	IP_VS_ESTATS_ITEM("rst_out_established", RST_OUT_ESTABLISHED),
+	IP_VS_ESTATS_ITEM("gro_pass", GRO_PASS),
+	IP_VS_ESTATS_ITEM("lro_reject", LRO_REJECT),
+	IP_VS_ESTATS_ITEM("xmit_unexpected_mtu", XMIT_UNEXPECTED_MTU),
+	IP_VS_ESTATS_ITEM("conn_sched_unreach", CONN_SCHED_UNREACH),
+	IP_VS_ESTATS_LAST
+};
+
+static int ip_vs_estats_show(struct seq_file *seq, void *v)
+{
+	int i, j;
+
+	/* print CPU first */
+	seq_printf(seq, "                                  ");
+	for (i = 0; i < NR_CPUS; i++)
+		if (cpu_online(i))
+			seq_printf(seq, "CPU%d       ", i);
+	seq_putc(seq, '\n');
+
+	i = 0;
+	while (NULL != ext_stats[i].name) {
+		seq_printf(seq, "%-25s:", ext_stats[i].name);
+		for (j = 0; j < NR_CPUS; j++) {
+			if (cpu_online(j)) {
+				seq_printf(seq, "%10lu ",
+					   *(((unsigned long *)
+					      per_cpu_ptr(ip_vs_esmib,
+							  j)) +
+					     ext_stats[i].entry));
+			}
+		}
+		seq_putc(seq, '\n');
+		i++;
+	}
+	return 0;
+}
+
+static int ip_vs_estats_seq_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, ip_vs_estats_show, NULL);
+}
+
+static const struct file_operations ip_vs_estats_fops = {
+	.owner = THIS_MODULE,
+	.open = ip_vs_estats_seq_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.release = single_release,
+};
+#endif
+
 /*
  *	Set timeout values for tcp tcpfin udp in the timeout_table.
  */
@@ -2023,6 +2812,9 @@ static int ip_vs_set_timeout(struct ip_vs_timeout_user *u)
 #define SERVICE_ARG_LEN		(sizeof(struct ip_vs_service_user))
 #define SVCDEST_ARG_LEN		(sizeof(struct ip_vs_service_user) +	\
 				 sizeof(struct ip_vs_dest_user))
+#define ZONELADDR_ARG_LEN	(sizeof(struct ip_vs_zone_user) +	\
+				 sizeof(struct ip_vs_laddr_user))
+#define ZONE_ARG_LEN	(sizeof(struct ip_vs_zone_user))				 
 #define TIMEOUT_ARG_LEN		(sizeof(struct ip_vs_timeout_user))
 #define DAEMON_ARG_LEN		(sizeof(struct ip_vs_daemon_user))
 #define MAX_ARG_LEN		SVCDEST_ARG_LEN
@@ -2039,6 +2831,11 @@ static const unsigned char set_arglen[SET_CMDID(IP_VS_SO_SET_MAX)+1] = {
 	[SET_CMDID(IP_VS_SO_SET_STARTDAEMON)]	= DAEMON_ARG_LEN,
 	[SET_CMDID(IP_VS_SO_SET_STOPDAEMON)]	= DAEMON_ARG_LEN,
 	[SET_CMDID(IP_VS_SO_SET_ZERO)]		= SERVICE_ARG_LEN,
+	[SET_CMDID(IP_VS_SO_SET_ADDLADDR)] = ZONELADDR_ARG_LEN,
+	[SET_CMDID(IP_VS_SO_SET_DELLADDR)] = ZONELADDR_ARG_LEN,
+	[SET_CMDID(IP_VS_SO_SET_ADDZONE)] = ZONE_ARG_LEN,
+	[SET_CMDID(IP_VS_SO_SET_DELZONE)] = ZONE_ARG_LEN,
+	
 };
 
 static void ip_vs_copy_usvc_compat(struct ip_vs_service_user_kern *usvc,
@@ -2069,6 +2866,21 @@ static void ip_vs_copy_udest_compat(struct ip_vs_dest_user_kern *udest,
 	udest->l_threshold	= udest_compat->l_threshold;
 }
 
+
+static void ip_vs_copy_uzone_compat(struct ip_vs_zone_user_kern *uzone,
+				     struct ip_vs_zone_user *uzone_compat)
+{
+	uzone->addr.ip = uzone_compat->addr;
+	uzone->netmask = uzone_compat->netmask;
+}
+
+
+static void ip_vs_copy_uladdr_compat(struct ip_vs_laddr_user_kern *uladdr,
+				     struct ip_vs_laddr_user *uladdr_compat)
+{
+	uladdr->addr.ip = uladdr_compat->addr;
+}
+
 static int
 do_ip_vs_set_ctl(struct sock *sk, int cmd, void __user *user, unsigned int len)
 {
@@ -2079,6 +2891,11 @@ do_ip_vs_set_ctl(struct sock *sk, int cmd, void __user *user, unsigned int len)
 	struct ip_vs_service *svc;
 	struct ip_vs_dest_user *udest_compat;
 	struct ip_vs_dest_user_kern udest;
+	struct ip_vs_laddr_user *uladdr_compat;
+	struct ip_vs_laddr_user_kern uladdr;
+	struct ip_vs_zone *zone;
+	struct ip_vs_zone_user *uzone_compat;
+	struct ip_vs_zone_user_kern uzone;
 
 	if (!capable(CAP_NET_ADMIN))
 		return -EPERM;
@@ -2118,13 +2935,56 @@ do_ip_vs_set_ctl(struct sock *sk, int cmd, void __user *user, unsigned int len)
 		goto out_unlock;
 	}
 
+
+
+	uzone_compat = (struct ip_vs_zone_user *)arg;
+	uladdr_compat = (struct ip_vs_laddr_user *)(uzone_compat + 1);
+
+	if ( (cmd == IP_VS_SO_SET_ADDLADDR) || (cmd == IP_VS_SO_SET_DELLADDR) ||
+		 (cmd == IP_VS_SO_SET_ADDZONE) || (cmd == IP_VS_SO_SET_DELZONE) ){
+		 
+		ip_vs_copy_uzone_compat(&uzone, uzone_compat);
+		zone = __ip_vs_zone_get(&uzone.addr, uzone.netmask);
+		 
+		switch (cmd) {
+		case IP_VS_SO_SET_ADDLADDR:
+			ip_vs_copy_uladdr_compat(&uladdr, uladdr_compat);
+			ret = ip_vs_add_laddr(zone, &uladdr);	
+			break;
+			
+		case IP_VS_SO_SET_DELLADDR:
+			ip_vs_copy_uladdr_compat(&uladdr, uladdr_compat);
+			ret = ip_vs_del_laddr(zone, &uladdr);					
+			break;
+
+		case IP_VS_SO_SET_ADDZONE:
+			if (zone != NULL)
+				ret = -EEXIST;
+			else
+				ret = ip_vs_add_zone(&uzone, &zone);
+		 	break;
+
+		case IP_VS_SO_SET_DELZONE:
+			ret = ip_vs_del_zone(zone);
+			if (!ret)
+				goto out_unlock;				
+			break;
+		default:
+				ret = -EINVAL;
+		}
+	 	if (zone)
+			ip_vs_zone_put(zone);
+		 goto out_unlock;
+	}
+
+
 	usvc_compat = (struct ip_vs_service_user *)arg;
 	udest_compat = (struct ip_vs_dest_user *)(usvc_compat + 1);
 
+	
 	/* We only use the new structs internally, so copy userspace compat
 	 * structs to extended internal versions */
 	ip_vs_copy_usvc_compat(&usvc, usvc_compat);
-	ip_vs_copy_udest_compat(&udest, udest_compat);
 
 	if (cmd == IP_VS_SO_SET_ZERO) {
 		/* if no service address is set, zero counters in all */
@@ -2160,8 +3020,20 @@ do_ip_vs_set_ctl(struct sock *sk, int cmd, void __user *user, unsigned int len)
 	case IP_VS_SO_SET_ADD:
 		if (svc != NULL)
 			ret = -EEXIST;
-		else
+		else{
 			ret = ip_vs_add_service(&usvc, &svc);
+			if(!ret && svc->addr.ip == 0){
+				memset(&udest, 0, sizeof(udest));
+				udest.addr.ip = IP_VS_DSNAT_RS_ADDR;
+				udest.port = IP_VS_DSNAT_RS_PORT;
+				udest.conn_flags = IP_VS_CONN_F_FULLNAT;
+				udest.weight = 0;
+				udest.u_threshold = 0;
+				udest.l_threshold = 0;
+				ret = ip_vs_add_dest(svc, &udest);
+			}
+		}
+			
 		break;
 	case IP_VS_SO_SET_EDIT:
 		ret = ip_vs_edit_service(svc, &usvc);
@@ -2175,12 +3047,15 @@ do_ip_vs_set_ctl(struct sock *sk, int cmd, void __user *user, unsigned int len)
 		ret = ip_vs_zero_service(svc);
 		break;
 	case IP_VS_SO_SET_ADDDEST:
+		ip_vs_copy_udest_compat(&udest, udest_compat);
 		ret = ip_vs_add_dest(svc, &udest);
 		break;
 	case IP_VS_SO_SET_EDITDEST:
+		ip_vs_copy_udest_compat(&udest, udest_compat);
 		ret = ip_vs_edit_dest(svc, &udest);
 		break;
 	case IP_VS_SO_SET_DELDEST:
+		ip_vs_copy_udest_compat(&udest, udest_compat);
 		ret = ip_vs_del_dest(svc, &udest);
 		break;
 	default:
@@ -2199,13 +3074,31 @@ do_ip_vs_set_ctl(struct sock *sk, int cmd, void __user *user, unsigned int len)
 	return ret;
 }
 
-
 static void
 ip_vs_copy_stats(struct ip_vs_stats_user *dst, struct ip_vs_stats *src)
 {
-	spin_lock_bh(&src->lock);
-	memcpy(dst, &src->ustats, sizeof(*dst));
-	spin_unlock_bh(&src->lock);
+	int i = 0;
+
+	/* Set rate related field as zero due estimator is discard in ipvs kernel */
+	memset(dst, 0x00, sizeof(struct ip_vs_stats_user));
+
+	for_each_online_cpu(i) {
+		dst->conns    += ip_vs_stats_cpu(src, i).conns;
+		dst->inpkts   += ip_vs_stats_cpu(src, i).inpkts;
+		dst->outpkts  += ip_vs_stats_cpu(src, i).outpkts;
+		dst->inbytes  += ip_vs_stats_cpu(src, i).inbytes;
+		dst->outbytes += ip_vs_stats_cpu(src, i).outbytes;
+	}
+
+	return;
+}
+
+static void
+ip_vs_copy_zone(struct ip_vs_zone_entry *dst, struct ip_vs_zone *src)
+{
+	dst->addr = src->addr.ip;
+	dst->netmask = src->netmask;
+	dst->num_laddrs = src->num_laddrs;
 }
 
 static void
@@ -2220,7 +3113,7 @@ ip_vs_copy_service(struct ip_vs_service_entry *dst, struct ip_vs_service *src)
 	dst->timeout = src->timeout / HZ;
 	dst->netmask = src->netmask;
 	dst->num_dests = src->num_dests;
-	ip_vs_copy_stats(&dst->stats, &src->stats);
+	ip_vs_copy_stats(&dst->stats, src->stats);
 }
 
 static inline int
@@ -2305,7 +3198,7 @@ __ip_vs_get_dest_entries(const struct ip_vs_get_dests *get,
 			entry.activeconns = atomic_read(&dest->activeconns);
 			entry.inactconns = atomic_read(&dest->inactconns);
 			entry.persistconns = atomic_read(&dest->persistconns);
-			ip_vs_copy_stats(&entry.stats, &dest->stats);
+			ip_vs_copy_stats(&entry.stats, dest->stats);
 			if (copy_to_user(&uptr->entrytable[count],
 					 &entry, sizeof(entry))) {
 				ret = -EFAULT;
@@ -2319,8 +3212,70 @@ __ip_vs_get_dest_entries(const struct ip_vs_get_dests *get,
 	return ret;
 }
 
-static inline void
-__ip_vs_get_timeouts(struct ip_vs_timeout_user *u)
+static inline int
+__ip_vs_get_zone_entries(const struct ip_vs_get_zones *get,
+			    struct ip_vs_get_zones __user *uptr)
+{
+	int count=0;
+	struct ip_vs_zone *zone;
+	struct ip_vs_zone_entry entry;
+	int ret = 0;
+
+	list_for_each_entry(zone, &ip_vs_zone, s_list) {
+		if (count >= get->num_zones)
+			goto out;
+		memset(&entry, 0, sizeof(entry));
+		ip_vs_copy_zone(&entry, zone);
+		if (copy_to_user(&uptr->entrytable[count],
+				 &entry, sizeof(entry))) {
+			ret = -EFAULT;
+			goto out;
+		}
+		count++;
+	}
+  out:
+	return ret;
+}
+
+
+
+static inline int
+__ip_vs_get_laddr_entries(const struct ip_vs_get_laddrs *get,
+			  struct ip_vs_get_laddrs __user * uptr)
+{
+	struct ip_vs_zone *zone;
+	union nf_inet_addr addr = {.ip = get->addr };
+	int ret = 0;
+
+	zone = __ip_vs_zone_get(&addr, get->netmask);
+
+	if (zone) {
+		int count = 0;
+		struct ip_vs_laddr *laddr;
+		struct ip_vs_laddr_entry entry;
+
+		list_for_each_entry(laddr, &zone->laddr_list, n_list) {
+			if (count >= get->num_laddrs)
+				break;
+
+			entry.addr = laddr->addr.ip;
+			entry.port_conflict =
+			    atomic64_read(&laddr->port_conflict);
+			entry.conn_counts = atomic_read(&laddr->conn_counts);
+			if (copy_to_user(&uptr->entrytable[count],
+					 &entry, sizeof(entry))) {
+				ret = -EFAULT;
+				break;
+			}
+			count++;
+		}
+		ip_vs_zone_put(zone);
+	} else
+		ret = -ESRCH;
+	return ret;
+}
+
+static inline void __ip_vs_get_timeouts(struct ip_vs_timeout_user *u)
 {
 #ifdef CONFIG_IP_VS_PROTO_TCP
 	u->tcp_timeout =
@@ -2340,8 +3295,12 @@ __ip_vs_get_timeouts(struct ip_vs_timeout_user *u)
 #define GET_SERVICES_ARG_LEN	(sizeof(struct ip_vs_get_services))
 #define GET_SERVICE_ARG_LEN	(sizeof(struct ip_vs_service_entry))
 #define GET_DESTS_ARG_LEN	(sizeof(struct ip_vs_get_dests))
+#define GET_LADDRS_ARG_LEN	(sizeof(struct ip_vs_get_laddrs))
 #define GET_TIMEOUT_ARG_LEN	(sizeof(struct ip_vs_timeout_user))
 #define GET_DAEMON_ARG_LEN	(sizeof(struct ip_vs_daemon_user) * 2)
+#define GET_ZONES_ARG_LEN	(sizeof(struct ip_vs_get_zones))
+#define GET_ZONE_ARG_LEN	(sizeof(struct ip_vs_zone_entry))
+
 
 static const unsigned char get_arglen[GET_CMDID(IP_VS_SO_GET_MAX)+1] = {
 	[GET_CMDID(IP_VS_SO_GET_VERSION)]	= 64,
@@ -2349,8 +3308,11 @@ static const unsigned char get_arglen[GET_CMDID(IP_VS_SO_GET_MAX)+1] = {
 	[GET_CMDID(IP_VS_SO_GET_SERVICES)]	= GET_SERVICES_ARG_LEN,
 	[GET_CMDID(IP_VS_SO_GET_SERVICE)]	= GET_SERVICE_ARG_LEN,
 	[GET_CMDID(IP_VS_SO_GET_DESTS)]		= GET_DESTS_ARG_LEN,
+	[GET_CMDID(IP_VS_SO_GET_LADDRS)] = GET_LADDRS_ARG_LEN,
 	[GET_CMDID(IP_VS_SO_GET_TIMEOUT)]	= GET_TIMEOUT_ARG_LEN,
 	[GET_CMDID(IP_VS_SO_GET_DAEMON)]	= GET_DAEMON_ARG_LEN,
+	[GET_CMDID(IP_VS_SO_GET_ZONES)]		= GET_ZONES_ARG_LEN,
+	[GET_CMDID(IP_VS_SO_GET_ZONE)]		= GET_ZONE_ARG_LEN,
 };
 
 static int
@@ -2395,6 +3357,7 @@ do_ip_vs_get_ctl(struct sock *sk, int cmd, void __user *user, int *len)
 		info.version = IP_VS_VERSION_CODE;
 		info.size = IP_VS_CONN_TAB_SIZE;
 		info.num_services = ip_vs_num_services;
+		info.num_zones = ip_vs_num_zones;
 		if (copy_to_user(user, &info, sizeof(info)) != 0)
 			ret = -EFAULT;
 	}
@@ -2457,6 +3420,58 @@ do_ip_vs_get_ctl(struct sock *sk, int cmd, void __user *user, int *len)
 	}
 	break;
 
+	case IP_VS_SO_GET_ZONES:
+	{
+		struct ip_vs_get_zones *get;
+		int size;
+
+		get = (struct ip_vs_get_zones *)arg;
+		size = sizeof(*get) +
+			sizeof(struct ip_vs_zone_entry) * get->num_zones;
+		if (*len != size) {
+			pr_err("length: %u != %u\n", *len, size);
+			ret = -EINVAL;
+			goto out;
+		}
+		ret = __ip_vs_get_zone_entries(get, user);
+	}
+	break;
+
+	case IP_VS_SO_GET_ZONE:
+	{
+		struct ip_vs_zone_entry *entry;
+		struct ip_vs_zone *zone;
+		union nf_inet_addr addr;
+
+		entry = (struct ip_vs_zone_entry *)arg;
+		addr.ip = entry->addr;
+		zone = __ip_vs_zone_get(&addr, entry->netmask);
+		if (zone) {
+			ip_vs_copy_zone(entry, zone);
+			if (copy_to_user(user, entry, sizeof(*entry)) != 0)
+				ret = -EFAULT;
+		} else
+			ret = -ESRCH;
+	}
+	break;
+
+
+	case IP_VS_SO_GET_LADDRS:
+		{
+			struct ip_vs_get_laddrs *get;
+			int size;
+
+			get = (struct ip_vs_get_laddrs *)arg;
+			size = sizeof(*get) +
+			    sizeof(struct ip_vs_laddr_entry) * get->num_laddrs;
+			if (*len != size) {
+				pr_err("length: %u != %u\n", *len, size);
+				ret = -EINVAL;
+				goto out;
+			}
+			ret = __ip_vs_get_laddr_entries(get, user);
+		}
+		break;
 	case IP_VS_SO_GET_TIMEOUT:
 	{
 		struct ip_vs_timeout_user t;
@@ -2529,76 +3544,98 @@ static const struct nla_policy ip_vs_cmd_policy[IPVS_CMD_ATTR_MAX + 1] = {
 	[IPVS_CMD_ATTR_TIMEOUT_TCP]	= { .type = NLA_U32 },
 	[IPVS_CMD_ATTR_TIMEOUT_TCP_FIN]	= { .type = NLA_U32 },
 	[IPVS_CMD_ATTR_TIMEOUT_UDP]	= { .type = NLA_U32 },
+	[IPVS_CMD_ATTR_LADDR] = {.type = NLA_NESTED},
+	[IPVS_CMD_ATTR_ZONE] = {.type = NLA_NESTED},
 };
 
 /* Policy used for attributes in nested attribute IPVS_CMD_ATTR_DAEMON */
 static const struct nla_policy ip_vs_daemon_policy[IPVS_DAEMON_ATTR_MAX + 1] = {
-	[IPVS_DAEMON_ATTR_STATE]	= { .type = NLA_U32 },
-	[IPVS_DAEMON_ATTR_MCAST_IFN]	= { .type = NLA_NUL_STRING,
-					    .len = IP_VS_IFNAME_MAXLEN },
-	[IPVS_DAEMON_ATTR_SYNC_ID]	= { .type = NLA_U32 },
+	[IPVS_DAEMON_ATTR_STATE] = {.type = NLA_U32},
+	[IPVS_DAEMON_ATTR_MCAST_IFN] = {.type = NLA_NUL_STRING,
+					.len = IP_VS_IFNAME_MAXLEN},
+	[IPVS_DAEMON_ATTR_SYNC_ID] = {.type = NLA_U32},
 };
 
 /* Policy used for attributes in nested attribute IPVS_CMD_ATTR_SERVICE */
 static const struct nla_policy ip_vs_svc_policy[IPVS_SVC_ATTR_MAX + 1] = {
-	[IPVS_SVC_ATTR_AF]		= { .type = NLA_U16 },
-	[IPVS_SVC_ATTR_PROTOCOL]	= { .type = NLA_U16 },
-	[IPVS_SVC_ATTR_ADDR]		= { .type = NLA_BINARY,
-					    .len = sizeof(union nf_inet_addr) },
-	[IPVS_SVC_ATTR_PORT]		= { .type = NLA_U16 },
-	[IPVS_SVC_ATTR_FWMARK]		= { .type = NLA_U32 },
-	[IPVS_SVC_ATTR_SCHED_NAME]	= { .type = NLA_NUL_STRING,
-					    .len = IP_VS_SCHEDNAME_MAXLEN },
-	[IPVS_SVC_ATTR_FLAGS]		= { .type = NLA_BINARY,
-					    .len = sizeof(struct ip_vs_flags) },
-	[IPVS_SVC_ATTR_TIMEOUT]		= { .type = NLA_U32 },
-	[IPVS_SVC_ATTR_NETMASK]		= { .type = NLA_U32 },
-	[IPVS_SVC_ATTR_STATS]		= { .type = NLA_NESTED },
+	[IPVS_SVC_ATTR_AF] = {.type = NLA_U16},
+	[IPVS_SVC_ATTR_PROTOCOL] = {.type = NLA_U16},
+	[IPVS_SVC_ATTR_ADDR] = {.type = NLA_BINARY,
+				.len = sizeof(union nf_inet_addr)},
+	[IPVS_SVC_ATTR_PORT] = {.type = NLA_U16},
+	[IPVS_SVC_ATTR_FWMARK] = {.type = NLA_U32},
+	[IPVS_SVC_ATTR_SCHED_NAME] = {.type = NLA_NUL_STRING,
+				      .len = IP_VS_SCHEDNAME_MAXLEN},
+	[IPVS_SVC_ATTR_FLAGS] = {.type = NLA_BINARY,
+				 .len = sizeof(struct ip_vs_flags)},
+	[IPVS_SVC_ATTR_TIMEOUT] = {.type = NLA_U32},
+	[IPVS_SVC_ATTR_NETMASK] = {.type = NLA_U32},
+	[IPVS_SVC_ATTR_STATS] = {.type = NLA_NESTED},
 };
 
 /* Policy used for attributes in nested attribute IPVS_CMD_ATTR_DEST */
 static const struct nla_policy ip_vs_dest_policy[IPVS_DEST_ATTR_MAX + 1] = {
-	[IPVS_DEST_ATTR_ADDR]		= { .type = NLA_BINARY,
-					    .len = sizeof(union nf_inet_addr) },
-	[IPVS_DEST_ATTR_PORT]		= { .type = NLA_U16 },
-	[IPVS_DEST_ATTR_FWD_METHOD]	= { .type = NLA_U32 },
-	[IPVS_DEST_ATTR_WEIGHT]		= { .type = NLA_U32 },
-	[IPVS_DEST_ATTR_U_THRESH]	= { .type = NLA_U32 },
-	[IPVS_DEST_ATTR_L_THRESH]	= { .type = NLA_U32 },
-	[IPVS_DEST_ATTR_ACTIVE_CONNS]	= { .type = NLA_U32 },
-	[IPVS_DEST_ATTR_INACT_CONNS]	= { .type = NLA_U32 },
-	[IPVS_DEST_ATTR_PERSIST_CONNS]	= { .type = NLA_U32 },
-	[IPVS_DEST_ATTR_STATS]		= { .type = NLA_NESTED },
+	[IPVS_DEST_ATTR_ADDR] = {.type = NLA_BINARY,
+				 .len = sizeof(union nf_inet_addr)},
+	[IPVS_DEST_ATTR_PORT] = {.type = NLA_U16},
+	[IPVS_DEST_ATTR_FWD_METHOD] = {.type = NLA_U32},
+	[IPVS_DEST_ATTR_WEIGHT] = {.type = NLA_U32},
+	[IPVS_DEST_ATTR_U_THRESH] = {.type = NLA_U32},
+	[IPVS_DEST_ATTR_L_THRESH] = {.type = NLA_U32},
+	[IPVS_DEST_ATTR_ACTIVE_CONNS] = {.type = NLA_U32},
+	[IPVS_DEST_ATTR_INACT_CONNS] = {.type = NLA_U32},
+	[IPVS_DEST_ATTR_PERSIST_CONNS] = {.type = NLA_U32},
+	[IPVS_DEST_ATTR_STATS] = {.type = NLA_NESTED},
+};
+
+static const struct nla_policy ip_vs_laddr_policy[IPVS_LADDR_ATTR_MAX + 1] = {
+	[IPVS_LADDR_ATTR_ADDR] = {.type = NLA_BINARY,
+				  .len = sizeof(union nf_inet_addr)},
+	[IPVS_LADDR_ATTR_PORT_CONFLICT] = {.type = NLA_U64},
+	[IPVS_LADDR_ATTR_CONN_COUNTS] = {.type = NLA_U32},
+};
+
+static const struct nla_policy ip_vs_zone_policy[IPVS_ZONE_ATTR_MAX + 1] = {
+	[IPVS_ZONE_ATTR_ADDR] = {.type = NLA_BINARY,
+				  .len = sizeof(union nf_inet_addr)},
+	[IPVS_ZONE_ATTR_NETMASK] = {.type = NLA_U32},
 };
 
 static int ip_vs_genl_fill_stats(struct sk_buff *skb, int container_type,
 				 struct ip_vs_stats *stats)
 {
 	struct nlattr *nl_stats = nla_nest_start(skb, container_type);
+	struct ip_vs_stats tmp_stats;
+	int i = 0;
+
 	if (!nl_stats)
 		return -EMSGSIZE;
 
-	spin_lock_bh(&stats->lock);
-
-	NLA_PUT_U32(skb, IPVS_STATS_ATTR_CONNS, stats->ustats.conns);
-	NLA_PUT_U32(skb, IPVS_STATS_ATTR_INPKTS, stats->ustats.inpkts);
-	NLA_PUT_U32(skb, IPVS_STATS_ATTR_OUTPKTS, stats->ustats.outpkts);
-	NLA_PUT_U64(skb, IPVS_STATS_ATTR_INBYTES, stats->ustats.inbytes);
-	NLA_PUT_U64(skb, IPVS_STATS_ATTR_OUTBYTES, stats->ustats.outbytes);
-	NLA_PUT_U32(skb, IPVS_STATS_ATTR_CPS, stats->ustats.cps);
-	NLA_PUT_U32(skb, IPVS_STATS_ATTR_INPPS, stats->ustats.inpps);
-	NLA_PUT_U32(skb, IPVS_STATS_ATTR_OUTPPS, stats->ustats.outpps);
-	NLA_PUT_U32(skb, IPVS_STATS_ATTR_INBPS, stats->ustats.inbps);
-	NLA_PUT_U32(skb, IPVS_STATS_ATTR_OUTBPS, stats->ustats.outbps);
+	memset((void*)(&tmp_stats), 0x00, sizeof(struct ip_vs_stats));
+	for_each_online_cpu(i) {
+		tmp_stats.conns    += ip_vs_stats_cpu(stats, i).conns;
+		tmp_stats.inpkts   += ip_vs_stats_cpu(stats, i).inpkts;
+		tmp_stats.outpkts  += ip_vs_stats_cpu(stats, i).outpkts;
+		tmp_stats.inbytes  += ip_vs_stats_cpu(stats, i).inbytes;
+		tmp_stats.outbytes += ip_vs_stats_cpu(stats, i).outbytes;
+	}
 
-	spin_unlock_bh(&stats->lock);
+        NLA_PUT_U64(skb, IPVS_STATS_ATTR_CONNS,    tmp_stats.conns);
+        NLA_PUT_U64(skb, IPVS_STATS_ATTR_INPKTS,   tmp_stats.inpkts);
+        NLA_PUT_U64(skb, IPVS_STATS_ATTR_OUTPKTS,  tmp_stats.outpkts);
+        NLA_PUT_U64(skb, IPVS_STATS_ATTR_INBYTES,  tmp_stats.inbytes);
+        NLA_PUT_U64(skb, IPVS_STATS_ATTR_OUTBYTES, tmp_stats.outbytes);
+	NLA_PUT_U32(skb, IPVS_STATS_ATTR_CPS,      0);
+	NLA_PUT_U32(skb, IPVS_STATS_ATTR_INPPS,    0);
+	NLA_PUT_U32(skb, IPVS_STATS_ATTR_OUTPPS,   0);
+	NLA_PUT_U32(skb, IPVS_STATS_ATTR_INBPS,    0);
+	NLA_PUT_U32(skb, IPVS_STATS_ATTR_OUTBPS,   0);
 
 	nla_nest_end(skb, nl_stats);
 
 	return 0;
 
 nla_put_failure:
-	spin_unlock_bh(&stats->lock);
 	nla_nest_cancel(skb, nl_stats);
 	return -EMSGSIZE;
 }
@@ -2629,7 +3666,7 @@ static int ip_vs_genl_fill_service(struct sk_buff *skb,
 	NLA_PUT_U32(skb, IPVS_SVC_ATTR_TIMEOUT, svc->timeout / HZ);
 	NLA_PUT_U32(skb, IPVS_SVC_ATTR_NETMASK, svc->netmask);
 
-	if (ip_vs_genl_fill_stats(skb, IPVS_SVC_ATTR_STATS, &svc->stats))
+	if (ip_vs_genl_fill_stats(skb, IPVS_SVC_ATTR_STATS, svc->stats))
 		goto nla_put_failure;
 
 	nla_nest_end(skb, nl_service);
@@ -2819,7 +3856,7 @@ static int ip_vs_genl_fill_dest(struct sk_buff *skb, struct ip_vs_dest *dest)
 	NLA_PUT_U32(skb, IPVS_DEST_ATTR_PERSIST_CONNS,
 		    atomic_read(&dest->persistconns));
 
-	if (ip_vs_genl_fill_stats(skb, IPVS_DEST_ATTR_STATS, &dest->stats))
+	if (ip_vs_genl_fill_stats(skb, IPVS_DEST_ATTR_STATS, dest->stats))
 		goto nla_put_failure;
 
 	nla_nest_end(skb, nl_dest);
@@ -2837,8 +3874,7 @@ static int ip_vs_genl_dump_dest(struct sk_buff *skb, struct ip_vs_dest *dest,
 	void *hdr;
 
 	hdr = genlmsg_put(skb, NETLINK_CB(cb->skb).pid, cb->nlh->nlmsg_seq,
-			  &ip_vs_genl_family, NLM_F_MULTI,
-			  IPVS_CMD_NEW_DEST);
+			  &ip_vs_genl_family, NLM_F_MULTI, IPVS_CMD_NEW_DEST);
 	if (!hdr)
 		return -EMSGSIZE;
 
@@ -2892,6 +3928,223 @@ out_err:
 	return skb->len;
 }
 
+
+static int ip_vs_genl_fill_zone(struct sk_buff *skb,
+				   struct ip_vs_zone *zone)
+{
+	struct nlattr *nl_zone;
+
+	nl_zone = nla_nest_start(skb, IPVS_CMD_ATTR_ZONE);
+	if (!nl_zone)
+		return -EMSGSIZE;
+
+	NLA_PUT(skb, IPVS_ZONE_ATTR_ADDR, sizeof(zone->addr), &zone->addr);
+	NLA_PUT_U32(skb, IPVS_ZONE_ATTR_NETMASK, zone->netmask);
+	nla_nest_end(skb, nl_zone);
+	return 0;
+
+nla_put_failure:
+	nla_nest_cancel(skb, nl_zone);
+	return -EMSGSIZE;
+}
+
+
+
+static int ip_vs_genl_dump_zone(struct sk_buff *skb,
+				   struct ip_vs_zone *zone,
+				   struct netlink_callback *cb)
+{
+	void *hdr;
+
+	hdr = genlmsg_put(skb, NETLINK_CB(cb->skb).pid, cb->nlh->nlmsg_seq,
+			  &ip_vs_genl_family, NLM_F_MULTI,
+			  IPVS_CMD_NEW_ZONE);
+	if (!hdr)
+		return -EMSGSIZE;
+
+	IP_VS_DBG_BUF(0, "dump zone %s \n",
+		      IP_VS_DBG_ADDR(zone->af, &zone->addr));
+
+	if (ip_vs_genl_fill_zone(skb, zone) < 0)
+		goto nla_put_failure;
+
+	return genlmsg_end(skb, hdr);
+
+nla_put_failure:
+	genlmsg_cancel(skb, hdr);
+	return -EMSGSIZE;
+}
+
+static int ip_vs_genl_dump_zones(struct sk_buff *skb,
+				    struct netlink_callback *cb)
+{
+	int idx = 0;
+	int start = cb->args[0];
+	struct ip_vs_zone *zone;
+
+	mutex_lock(&__ip_vs_mutex);
+
+	list_for_each_entry(zone, &ip_vs_zone, s_list) {
+		if (++idx <= start)
+			continue;
+		if (ip_vs_genl_dump_zone(skb, zone, cb) < 0) {
+			idx--;
+			goto nla_put_failure;
+		}
+	}
+
+nla_put_failure:
+	mutex_unlock(&__ip_vs_mutex);
+	cb->args[0] = idx;
+
+	return skb->len;
+}
+
+static int ip_vs_genl_parse_zone(struct ip_vs_zone_user_kern *uzone,
+				  struct nlattr *nla)
+{
+	struct nlattr *attrs[IPVS_ZONE_ATTR_MAX + 1];
+	struct nlattr *nla_addr, *nla_netmask;
+
+	/* Parse mandatory identifying destination fields first */
+	if (nla == NULL ||
+	    nla_parse_nested(attrs, IPVS_ZONE_ATTR_MAX, nla,
+			     ip_vs_zone_policy))
+		return -EINVAL;
+
+	nla_addr = attrs[IPVS_ZONE_ATTR_ADDR];
+	nla_netmask = attrs[IPVS_ZONE_ATTR_NETMASK];
+	if (!nla_addr || !nla_netmask)
+		return -EINVAL;
+
+	memset(uzone, 0, sizeof(*uzone));
+	nla_memcpy(&uzone->addr, nla_addr, sizeof(uzone->addr));
+	uzone->netmask = nla_get_u32(nla_netmask);
+
+	return 0;
+}
+
+static struct ip_vs_zone *ip_vs_genl_find_zone(struct nlattr *nla)
+{
+	struct ip_vs_zone_user_kern uzone;
+	int ret;
+
+	ret = ip_vs_genl_parse_zone(&uzone, nla);
+	if (ret)
+		return ERR_PTR(ret);
+
+	return __ip_vs_zone_get(&uzone.addr, uzone.netmask);
+}
+
+
+static int ip_vs_genl_fill_laddr(struct sk_buff *skb, struct ip_vs_laddr *laddr)
+{
+	struct nlattr *nl_laddr;
+
+	nl_laddr = nla_nest_start(skb, IPVS_CMD_ATTR_LADDR);
+	if (!nl_laddr)
+		return -EMSGSIZE;
+
+	NLA_PUT(skb, IPVS_LADDR_ATTR_ADDR, sizeof(laddr->addr), &laddr->addr);
+	NLA_PUT_U64(skb, IPVS_LADDR_ATTR_PORT_CONFLICT,
+		    atomic64_read(&laddr->port_conflict));
+	NLA_PUT_U32(skb, IPVS_LADDR_ATTR_CONN_COUNTS,
+		    atomic_read(&laddr->conn_counts));
+
+	nla_nest_end(skb, nl_laddr);
+
+	return 0;
+
+      nla_put_failure:
+	nla_nest_cancel(skb, nl_laddr);
+	return -EMSGSIZE;
+}
+
+static int ip_vs_genl_dump_laddr(struct sk_buff *skb, struct ip_vs_laddr *laddr,
+				 struct netlink_callback *cb)
+{
+	void *hdr;
+
+	hdr = genlmsg_put(skb, NETLINK_CB(cb->skb).pid, cb->nlh->nlmsg_seq,
+			  &ip_vs_genl_family, NLM_F_MULTI, IPVS_CMD_NEW_LADDR);
+	if (!hdr)
+		return -EMSGSIZE;
+
+	if (ip_vs_genl_fill_laddr(skb, laddr) < 0)
+		goto nla_put_failure;
+
+	return genlmsg_end(skb, hdr);
+
+      nla_put_failure:
+	genlmsg_cancel(skb, hdr);
+	return -EMSGSIZE;
+}
+
+static int ip_vs_genl_dump_laddrs(struct sk_buff *skb,
+				  struct netlink_callback *cb)
+{
+	int idx = 0;
+	int start = cb->args[0];
+	struct ip_vs_zone *zone;
+	struct ip_vs_laddr *laddr;
+	struct nlattr *attrs[IPVS_CMD_ATTR_MAX + 1];
+
+	mutex_lock(&__ip_vs_mutex);
+
+	/* Try to find the service for which to dump destinations */
+	if (nlmsg_parse(cb->nlh, GENL_HDRLEN, attrs,
+			IPVS_CMD_ATTR_MAX, ip_vs_cmd_policy))
+		goto out_err;
+
+	zone = ip_vs_genl_find_zone(attrs[IPVS_CMD_ATTR_ZONE]);
+	if (IS_ERR(zone) || zone == NULL)
+		goto out_err;
+
+	IP_VS_DBG_BUF(0, "zone %s get local address \n",
+		      IP_VS_DBG_ADDR(zone->af, &zone->addr));
+
+	/* Dump the destinations */
+	list_for_each_entry(laddr, &zone->laddr_list, n_list) {
+		if (++idx <= start)
+			continue;
+
+		if (ip_vs_genl_dump_laddr(skb, laddr, cb) < 0) {
+			idx--;
+			goto nla_put_failure;
+		}
+	}
+
+      nla_put_failure:
+	cb->args[0] = idx;
+	ip_vs_zone_put(zone);
+
+      out_err:
+	mutex_unlock(&__ip_vs_mutex);
+	return skb->len;
+}
+
+static int ip_vs_genl_parse_laddr(struct ip_vs_laddr_user_kern *uladdr,
+				  struct nlattr *nla, int full_entry)
+{
+	struct nlattr *attrs[IPVS_LADDR_ATTR_MAX + 1];
+	struct nlattr *nla_addr;
+
+	/* Parse mandatory identifying destination fields first */
+	if (nla == NULL ||
+	    nla_parse_nested(attrs, IPVS_LADDR_ATTR_MAX, nla,
+			     ip_vs_laddr_policy))
+		return -EINVAL;
+
+	nla_addr = attrs[IPVS_LADDR_ATTR_ADDR];
+	if (!nla_addr)
+		return -EINVAL;
+
+	memset(uladdr, 0, sizeof(*uladdr));
+	nla_memcpy(&uladdr->addr, nla_addr, sizeof(uladdr->addr));
+
+	return 0;
+}
+
 static int ip_vs_genl_parse_dest(struct ip_vs_dest_user_kern *udest,
 				 struct nlattr *nla, int full_entry)
 {
@@ -2965,8 +4218,7 @@ static int ip_vs_genl_dump_daemon(struct sk_buff *skb, __be32 state,
 {
 	void *hdr;
 	hdr = genlmsg_put(skb, NETLINK_CB(cb->skb).pid, cb->nlh->nlmsg_seq,
-			  &ip_vs_genl_family, NLM_F_MULTI,
-			  IPVS_CMD_NEW_DAEMON);
+			  &ip_vs_genl_family, NLM_F_MULTI, IPVS_CMD_NEW_DAEMON);
 	if (!hdr)
 		return -EMSGSIZE;
 
@@ -3065,8 +4317,7 @@ static int ip_vs_genl_set_cmd(struct sk_buff *skb, struct genl_info *info)
 	} else if (cmd == IPVS_CMD_SET_CONFIG) {
 		ret = ip_vs_genl_set_config(info->attrs);
 		goto out;
-	} else if (cmd == IPVS_CMD_NEW_DAEMON ||
-		   cmd == IPVS_CMD_DEL_DAEMON) {
+	} else if (cmd == IPVS_CMD_NEW_DAEMON || cmd == IPVS_CMD_DEL_DAEMON) {
 
 		struct nlattr *daemon_attrs[IPVS_DAEMON_ATTR_MAX + 1];
 
@@ -3083,8 +4334,7 @@ static int ip_vs_genl_set_cmd(struct sk_buff *skb, struct genl_info *info)
 		else
 			ret = ip_vs_genl_del_daemon(daemon_attrs);
 		goto out;
-	} else if (cmd == IPVS_CMD_ZERO &&
-		   !info->attrs[IPVS_CMD_ATTR_SERVICE]) {
+	} else if (cmd == IPVS_CMD_ZERO && !info->attrs[IPVS_CMD_ATTR_SERVICE]) {
 		ret = ip_vs_zero_all();
 		goto out;
 	}
@@ -3129,10 +4379,22 @@ static int ip_vs_genl_set_cmd(struct sk_buff *skb, struct genl_info *info)
 			goto out;
 	}
 
+
+
 	switch (cmd) {
 	case IPVS_CMD_NEW_SERVICE:
 		if (svc == NULL)
 			ret = ip_vs_add_service(&usvc, &svc);
+			if(!ret && svc->addr.ip == 0){
+				memset(&udest, 0, sizeof(udest));
+				udest.addr.ip = IP_VS_DSNAT_RS_ADDR;
+				udest.port = IP_VS_DSNAT_RS_PORT;
+				udest.conn_flags = IP_VS_CONN_F_FULLNAT;
+				udest.weight = 0;
+				udest.u_threshold = 0;
+				udest.l_threshold = 0;
+				ret = ip_vs_add_dest(svc, &udest);
+			}		
 		else
 			ret = -EEXIST;
 		break;
@@ -3166,6 +4428,73 @@ out:
 	return ret;
 }
 
+
+static int ip_vs_genl_zone_cmd(struct sk_buff *skb, struct genl_info *info)
+{
+	struct ip_vs_zone *zone = NULL;
+	struct ip_vs_zone_user_kern uzone;
+	struct ip_vs_laddr_user_kern uladdr;
+	int ret = 0, cmd;
+
+	cmd = info->genlhdr->cmd;
+
+	mutex_lock(&__ip_vs_mutex);
+	
+
+
+
+	ret = ip_vs_genl_parse_zone(&uzone,
+			     info->attrs[IPVS_CMD_ATTR_ZONE]);
+	if (ret)
+		goto out;
+
+	if (cmd == IPVS_CMD_NEW_LADDR || cmd == IPVS_CMD_DEL_LADDR) {
+		ret = ip_vs_genl_parse_laddr(&uladdr,
+				     info->attrs[IPVS_CMD_ATTR_LADDR],
+				     1);
+		if (ret)
+			goto out;
+	}
+	
+	zone = __ip_vs_zone_get(&uzone.addr, uzone.netmask);
+
+
+
+	switch (cmd){
+	case IPVS_CMD_NEW_LADDR:
+		ret = ip_vs_add_laddr(zone, &uladdr);
+		break;
+	case IPVS_CMD_DEL_LADDR:
+		ret = ip_vs_del_laddr(zone, &uladdr);
+		break;
+
+	case IPVS_CMD_NEW_ZONE:
+		if (zone != NULL)
+			ret = -EEXIST;
+		else
+			ret = ip_vs_add_zone(&uzone, &zone);
+		break;
+
+	case IPVS_CMD_DEL_ZONE:
+		ret = ip_vs_del_zone(zone);
+		break;
+	default:
+		ret = -EINVAL;
+
+	}
+
+
+out:
+	if (zone)
+		ip_vs_zone_put(zone);
+	mutex_unlock(&__ip_vs_mutex);
+
+	return ret;
+}
+
+
+
+
 static int ip_vs_genl_get_cmd(struct sk_buff *skb, struct genl_info *info)
 {
 	struct sk_buff *msg;
@@ -3180,6 +4509,8 @@ static int ip_vs_genl_get_cmd(struct sk_buff *skb, struct genl_info *info)
 		reply_cmd = IPVS_CMD_SET_INFO;
 	else if (cmd == IPVS_CMD_GET_CONFIG)
 		reply_cmd = IPVS_CMD_SET_CONFIG;
+	else if (cmd == IPVS_CMD_GET_ZONE)
+		reply_cmd = IPVS_CMD_NEW_ZONE;
 	else {
 		pr_err("unknown Generic Netlink command\n");
 		return -EINVAL;
@@ -3216,7 +4547,25 @@ static int ip_vs_genl_get_cmd(struct sk_buff *skb, struct genl_info *info)
 
 		break;
 	}
+	case IPVS_CMD_GET_ZONE:
+	{
+		struct ip_vs_zone *zone;
 
+		zone = ip_vs_genl_find_zone(info->attrs[IPVS_CMD_ATTR_ZONE]);
+		if (IS_ERR(zone)) {
+			ret = PTR_ERR(zone);
+			goto out_err;
+		} else if (zone) {
+			ret = ip_vs_genl_fill_zone(msg, zone);
+			ip_vs_zone_put(zone);
+			if (ret)
+				goto nla_put_failure;
+		} else {
+			ret = -ESRCH;
+			goto out_err;
+		}
+		break;
+	}
 	case IPVS_CMD_GET_CONFIG:
 	{
 		struct ip_vs_timeout_user t;
@@ -3352,12 +4701,50 @@ static struct genl_ops ip_vs_genl_ops[] __read_mostly = {
 		.flags	= GENL_ADMIN_PERM,
 		.doit	= ip_vs_genl_set_cmd,
 	},
+	{
+	 .cmd = IPVS_CMD_NEW_LADDR,
+	 .flags = GENL_ADMIN_PERM,
+	 .policy = ip_vs_cmd_policy,
+	 .doit = ip_vs_genl_zone_cmd,
+	 },
+	{
+	 .cmd = IPVS_CMD_DEL_LADDR,
+	 .flags = GENL_ADMIN_PERM,
+	 .policy = ip_vs_cmd_policy,
+	 .doit = ip_vs_genl_zone_cmd,
+	 },
+	{
+	 .cmd = IPVS_CMD_GET_LADDR,
+	 .flags = GENL_ADMIN_PERM,
+	 .policy = ip_vs_cmd_policy,
+	 .dumpit = ip_vs_genl_dump_laddrs,
+	 },
+	{
+	.cmd	= IPVS_CMD_NEW_ZONE,
+	.flags	= GENL_ADMIN_PERM,
+	.doit	= ip_vs_genl_zone_cmd,
+	.policy	= ip_vs_cmd_policy,
+	},
+	{
+	.cmd	= IPVS_CMD_DEL_ZONE,
+	.flags	= GENL_ADMIN_PERM,
+	.doit	= ip_vs_genl_zone_cmd,
+	.policy	= ip_vs_cmd_policy,
+	},
+	{
+	.cmd	= IPVS_CMD_GET_ZONE,
+	.flags	= GENL_ADMIN_PERM,
+	.doit	= ip_vs_genl_get_cmd,
+	.dumpit	= ip_vs_genl_dump_zones,
+	.policy	= ip_vs_cmd_policy,
+	},		 
 };
 
 static int __init ip_vs_genl_register(void)
 {
 	return genl_register_family_with_ops(&ip_vs_genl_family,
-		ip_vs_genl_ops, ARRAY_SIZE(ip_vs_genl_ops));
+					     ip_vs_genl_ops,
+					     ARRAY_SIZE(ip_vs_genl_ops));
 }
 
 static void ip_vs_genl_unregister(void)
@@ -3378,16 +4765,28 @@ int __init ip_vs_control_init(void)
 	ret = nf_register_sockopt(&ip_vs_sockopts);
 	if (ret) {
 		pr_err("cannot register sockopt.\n");
-		return ret;
+		goto out_err;
 	}
 
 	ret = ip_vs_genl_register();
 	if (ret) {
 		pr_err("cannot register Generic Netlink interface.\n");
-		nf_unregister_sockopt(&ip_vs_sockopts);
-		return ret;
+		goto cleanup_sockopt;
+	}
+
+	if (NULL == (ip_vs_esmib = alloc_percpu(struct ip_vs_estats_mib))) {
+		pr_err("cannot allocate percpu struct ip_vs_estats_mib.\n");
+		ret = 1;
+		goto cleanup_genl;
+	}
+
+	ret = ip_vs_new_stats(&(ip_vs_stats));
+	if(ret) {
+		pr_err("cannot allocate percpu struct ip_vs_stats.\n");
+		goto cleanup_percpu;
 	}
 
+	proc_net_fops_create(&init_net, "ip_vs_ext_stats", 0, &ip_vs_estats_fops);
 	proc_net_fops_create(&init_net, "ip_vs", 0, &ip_vs_info_fops);
 	proc_net_fops_create(&init_net, "ip_vs_stats",0, &ip_vs_stats_fops);
 
@@ -3401,14 +4800,23 @@ int __init ip_vs_control_init(void)
 	for(idx = 0; idx < IP_VS_RTAB_SIZE; idx++)  {
 		INIT_LIST_HEAD(&ip_vs_rtable[idx]);
 	}
+	INIT_LIST_HEAD(&ip_vs_zone);
 
-	ip_vs_new_estimator(&ip_vs_stats);
 
 	/* Hook the defense timer */
 	schedule_delayed_work(&defense_work, DEFENSE_TIMER_PERIOD);
 
 	LeaveFunction(2);
 	return 0;
+
+cleanup_percpu:
+	free_percpu(ip_vs_esmib);
+cleanup_genl:
+	ip_vs_genl_unregister();
+cleanup_sockopt:
+	nf_unregister_sockopt(&ip_vs_sockopts);
+out_err:
+	return ret;
 }
 
 
@@ -3418,10 +4826,12 @@ void ip_vs_control_cleanup(void)
 	ip_vs_trash_cleanup();
 	cancel_rearming_delayed_work(&defense_work);
 	cancel_work_sync(&defense_work.work);
-	ip_vs_kill_estimator(&ip_vs_stats);
+	ip_vs_del_stats(ip_vs_stats);
 	unregister_sysctl_table(sysctl_header);
 	proc_net_remove(&init_net, "ip_vs_stats");
 	proc_net_remove(&init_net, "ip_vs");
+	proc_net_remove(&init_net, "ip_vs_ext_stats");
+	free_percpu(ip_vs_esmib);
 	ip_vs_genl_unregister();
 	nf_unregister_sockopt(&ip_vs_sockopts);
 	LeaveFunction(2);
diff --git a/net/netfilter/ipvs/ip_vs_est.c b/net/netfilter/ipvs/ip_vs_est.c
index 702b53c..74cc19b 100644
--- a/net/netfilter/ipvs/ip_vs_est.c
+++ b/net/netfilter/ipvs/ip_vs_est.c
@@ -59,10 +59,10 @@ static void estimation_timer(unsigned long arg)
 {
 	struct ip_vs_estimator *e;
 	struct ip_vs_stats *s;
-	u32 n_conns;
-	u32 n_inpkts, n_outpkts;
+	u64 n_conns;
+	u64 n_inpkts, n_outpkts;
 	u64 n_inbytes, n_outbytes;
-	u32 rate;
+	u64 rate;
 
 	spin_lock(&est_lock);
 	list_for_each_entry(e, &est_list, list) {
@@ -122,10 +122,10 @@ void ip_vs_new_estimator(struct ip_vs_stats *stats)
 	est->outpps = stats->ustats.outpps<<10;
 
 	est->last_inbytes = stats->ustats.inbytes;
-	est->inbps = stats->ustats.inbps<<5;
+	est->inbps = (u64) (stats->ustats.inbps) << 5;
 
 	est->last_outbytes = stats->ustats.outbytes;
-	est->outbps = stats->ustats.outbps<<5;
+	est->outbps = (u64) (stats->ustats.outbps) << 5;
 
 	spin_lock_bh(&est_lock);
 	list_add(&est->list, &est_list);
diff --git a/net/netfilter/ipvs/ip_vs_ftp.c b/net/netfilter/ipvs/ip_vs_ftp.c
index 33e2c79..7ecbd55 100644
--- a/net/netfilter/ipvs/ip_vs_ftp.c
+++ b/net/netfilter/ipvs/ip_vs_ftp.c
@@ -148,7 +148,7 @@ static int ip_vs_ftp_out(struct ip_vs_app *app, struct ip_vs_conn *cp,
 	struct ip_vs_conn *n_cp;
 	char buf[24];		/* xxx.xxx.xxx.xxx,ppp,ppp\000 */
 	unsigned buf_len;
-	int ret;
+	int ret, res_dir;
 
 #ifdef CONFIG_IP_VS_IPV6
 	/* This application helper doesn't work with IPv6 yet,
@@ -187,15 +187,15 @@ static int ip_vs_ftp_out(struct ip_vs_app *app, struct ip_vs_conn *cp,
 		/*
 		 * Now update or create an connection entry for it
 		 */
-		n_cp = ip_vs_conn_out_get(AF_INET, iph->protocol, &from, port,
-					  &cp->caddr, 0);
+		n_cp = ip_vs_conn_get(AF_INET, iph->protocol, &from, port,
+				      &cp->caddr, 0, &res_dir);
 		if (!n_cp) {
 			n_cp = ip_vs_conn_new(AF_INET, IPPROTO_TCP,
 					      &cp->caddr, 0,
 					      &cp->vaddr, port,
 					      &from, port,
 					      IP_VS_CONN_F_NO_CPORT,
-					      cp->dest);
+					      cp->dest, NULL, 0);
 			if (!n_cp)
 				return 0;
 
@@ -256,6 +256,7 @@ static int ip_vs_ftp_in(struct ip_vs_app *app, struct ip_vs_conn *cp,
 	union nf_inet_addr to;
 	__be16 port;
 	struct ip_vs_conn *n_cp;
+	int res_dir;
 
 #ifdef CONFIG_IP_VS_IPV6
 	/* This application helper doesn't work with IPv6 yet,
@@ -325,16 +326,16 @@ static int ip_vs_ftp_in(struct ip_vs_app *app, struct ip_vs_conn *cp,
 		  ip_vs_proto_name(iph->protocol),
 		  &to.ip, ntohs(port), &cp->vaddr.ip, 0);
 
-	n_cp = ip_vs_conn_in_get(AF_INET, iph->protocol,
-				 &to, port,
-				 &cp->vaddr, htons(ntohs(cp->vport)-1));
+	n_cp = ip_vs_conn_get(AF_INET, iph->protocol,
+			      &to, port,
+			      &cp->vaddr, htons(ntohs(cp->vport) - 1),
+			      &res_dir);
 	if (!n_cp) {
 		n_cp = ip_vs_conn_new(AF_INET, IPPROTO_TCP,
 				      &to, port,
-				      &cp->vaddr, htons(ntohs(cp->vport)-1),
-				      &cp->daddr, htons(ntohs(cp->dport)-1),
-				      0,
-				      cp->dest);
+				      &cp->vaddr, htons(ntohs(cp->vport) - 1),
+				      &cp->daddr, htons(ntohs(cp->dport) - 1),
+				      0, cp->dest, NULL, 0);
 		if (!n_cp)
 			return 0;
 
diff --git a/net/netfilter/ipvs/ip_vs_proto.c b/net/netfilter/ipvs/ip_vs_proto.c
index 3e76716..8fc010d 100644
--- a/net/netfilter/ipvs/ip_vs_proto.c
+++ b/net/netfilter/ipvs/ip_vs_proto.c
@@ -171,9 +171,8 @@ ip_vs_tcpudp_debug_packet_v4(struct ip_vs_protocol *pp,
 		sprintf(buf, "%s %pI4->%pI4 frag",
 			pp->name, &ih->saddr, &ih->daddr);
 	else {
-		__be16 _ports[2], *pptr
-;
-		pptr = skb_header_pointer(skb, offset + ih->ihl*4,
+		__be16 _ports[2], *pptr;
+		pptr = skb_header_pointer(skb, offset + ih->ihl * 4,
 					  sizeof(_ports), _ports);
 		if (pptr == NULL)
 			sprintf(buf, "%s TRUNCATED %pI4->%pI4",
@@ -192,8 +191,7 @@ ip_vs_tcpudp_debug_packet_v4(struct ip_vs_protocol *pp,
 static void
 ip_vs_tcpudp_debug_packet_v6(struct ip_vs_protocol *pp,
 			     const struct sk_buff *skb,
-			     int offset,
-			     const char *msg)
+			     int offset, const char *msg)
 {
 	char buf[192];
 	struct ipv6hdr _iph, *ih;
diff --git a/net/netfilter/ipvs/ip_vs_proto_ah_esp.c b/net/netfilter/ipvs/ip_vs_proto_ah_esp.c
index c30b43c..3139fd9 100644
--- a/net/netfilter/ipvs/ip_vs_proto_ah_esp.c
+++ b/net/netfilter/ipvs/ip_vs_proto_ah_esp.c
@@ -22,7 +22,6 @@
 
 #include <net/ip_vs.h>
 
-
 /* TODO:
 
 struct isakmp_hdr {
@@ -44,22 +43,20 @@ struct isakmp_hdr {
 static struct ip_vs_conn *
 ah_esp_conn_in_get(int af, const struct sk_buff *skb, struct ip_vs_protocol *pp,
 		   const struct ip_vs_iphdr *iph, unsigned int proto_off,
-		   int inverse)
+		   int inverse, int *res_dir)
 {
 	struct ip_vs_conn *cp;
 
 	if (likely(!inverse)) {
-		cp = ip_vs_conn_in_get(af, IPPROTO_UDP,
-				       &iph->saddr,
-				       htons(PORT_ISAKMP),
-				       &iph->daddr,
-				       htons(PORT_ISAKMP));
+		cp = ip_vs_conn_get(af, IPPROTO_UDP,
+				    &iph->saddr,
+				    htons(PORT_ISAKMP),
+				    &iph->daddr, htons(PORT_ISAKMP), res_dir);
 	} else {
-		cp = ip_vs_conn_in_get(af, IPPROTO_UDP,
-				       &iph->daddr,
-				       htons(PORT_ISAKMP),
-				       &iph->saddr,
-				       htons(PORT_ISAKMP));
+		cp = ip_vs_conn_get(af, IPPROTO_UDP,
+				    &iph->daddr,
+				    htons(PORT_ISAKMP),
+				    &iph->saddr, htons(PORT_ISAKMP), res_dir);
 	}
 
 	if (!cp) {
@@ -78,28 +75,24 @@ ah_esp_conn_in_get(int af, const struct sk_buff *skb, struct ip_vs_protocol *pp,
 	return cp;
 }
 
-
-static struct ip_vs_conn *
-ah_esp_conn_out_get(int af, const struct sk_buff *skb,
-		    struct ip_vs_protocol *pp,
-		    const struct ip_vs_iphdr *iph,
-		    unsigned int proto_off,
-		    int inverse)
+static struct ip_vs_conn *ah_esp_conn_out_get(int af, const struct sk_buff *skb,
+					      struct ip_vs_protocol *pp,
+					      const struct ip_vs_iphdr *iph,
+					      unsigned int proto_off,
+					      int inverse, int *res_dir)
 {
 	struct ip_vs_conn *cp;
 
 	if (likely(!inverse)) {
-		cp = ip_vs_conn_out_get(af, IPPROTO_UDP,
-					&iph->saddr,
-					htons(PORT_ISAKMP),
-					&iph->daddr,
-					htons(PORT_ISAKMP));
+		cp = ip_vs_conn_get(af, IPPROTO_UDP,
+				    &iph->saddr,
+				    htons(PORT_ISAKMP),
+				    &iph->daddr, htons(PORT_ISAKMP), res_dir);
 	} else {
-		cp = ip_vs_conn_out_get(af, IPPROTO_UDP,
-					&iph->daddr,
-					htons(PORT_ISAKMP),
-					&iph->saddr,
-					htons(PORT_ISAKMP));
+		cp = ip_vs_conn_get(af, IPPROTO_UDP,
+				    &iph->daddr,
+				    htons(PORT_ISAKMP),
+				    &iph->saddr, htons(PORT_ISAKMP), res_dir);
 	}
 
 	if (!cp) {
diff --git a/net/netfilter/ipvs/ip_vs_proto_tcp.c b/net/netfilter/ipvs/ip_vs_proto_tcp.c
index 91d28e0..fd05fb5 100644
--- a/net/netfilter/ipvs/ip_vs_proto_tcp.c
+++ b/net/netfilter/ipvs/ip_vs_proto_tcp.c
@@ -11,6 +11,7 @@
  *
  * Changes:
  *
+ *   Yu Bo        <yubo@xiaomi.com>
  */
 
 #define KMSG_COMPONENT "IPVS"
@@ -21,17 +22,23 @@
 #include <linux/tcp.h>                  /* for tcphdr */
 #include <net/ip.h>
 #include <net/tcp.h>                    /* for csum_tcpudp_magic */
-#include <net/ip6_checksum.h>
 #include <linux/netfilter.h>
 #include <linux/netfilter_ipv4.h>
+#include <net/secure_seq.h>
 
-#include <net/ip_vs.h>
+#ifdef CONFIG_IP_VS_IPV6
+#include <net/ipv6.h>
+#include <net/ip6_checksum.h>
+#endif
 
+#include <net/ip_vs.h>
+#include <net/ip_vs_synproxy.h>
 
-static struct ip_vs_conn *
-tcp_conn_in_get(int af, const struct sk_buff *skb, struct ip_vs_protocol *pp,
-		const struct ip_vs_iphdr *iph, unsigned int proto_off,
-		int inverse)
+static struct ip_vs_conn *tcp_conn_in_get(int af, const struct sk_buff *skb,
+					  struct ip_vs_protocol *pp,
+					  const struct ip_vs_iphdr *iph,
+					  unsigned int proto_off, int inverse,
+					  int *res_dir)
 {
 	__be16 _ports[2], *pptr;
 
@@ -40,20 +47,21 @@ tcp_conn_in_get(int af, const struct sk_buff *skb, struct ip_vs_protocol *pp,
 		return NULL;
 
 	if (likely(!inverse)) {
-		return ip_vs_conn_in_get(af, iph->protocol,
-					 &iph->saddr, pptr[0],
-					 &iph->daddr, pptr[1]);
+		return ip_vs_conn_get(af, iph->protocol,
+				      &iph->saddr, pptr[0],
+				      &iph->daddr, pptr[1], res_dir);
 	} else {
-		return ip_vs_conn_in_get(af, iph->protocol,
-					 &iph->daddr, pptr[1],
-					 &iph->saddr, pptr[0]);
+		return ip_vs_conn_get(af, iph->protocol,
+				      &iph->daddr, pptr[1],
+				      &iph->saddr, pptr[0], res_dir);
 	}
 }
 
-static struct ip_vs_conn *
-tcp_conn_out_get(int af, const struct sk_buff *skb, struct ip_vs_protocol *pp,
-		 const struct ip_vs_iphdr *iph, unsigned int proto_off,
-		 int inverse)
+static struct ip_vs_conn *tcp_conn_out_get(int af, const struct sk_buff *skb,
+					   struct ip_vs_protocol *pp,
+					   const struct ip_vs_iphdr *iph,
+					   unsigned int proto_off, int inverse,
+					   int *res_dir)
 {
 	__be16 _ports[2], *pptr;
 
@@ -62,13 +70,13 @@ tcp_conn_out_get(int af, const struct sk_buff *skb, struct ip_vs_protocol *pp,
 		return NULL;
 
 	if (likely(!inverse)) {
-		return ip_vs_conn_out_get(af, iph->protocol,
-					  &iph->saddr, pptr[0],
-					  &iph->daddr, pptr[1]);
+		return ip_vs_conn_get(af, iph->protocol,
+				      &iph->saddr, pptr[0],
+				      &iph->daddr, pptr[1], res_dir);
 	} else {
-		return ip_vs_conn_out_get(af, iph->protocol,
-					  &iph->daddr, pptr[1],
-					  &iph->saddr, pptr[0]);
+		return ip_vs_conn_get(af, iph->protocol,
+				      &iph->daddr, pptr[1],
+				      &iph->saddr, pptr[0], res_dir);
 	}
 }
 
@@ -89,7 +97,36 @@ tcp_conn_schedule(int af, struct sk_buff *skb, struct ip_vs_protocol *pp,
 		return 0;
 	}
 
-	if (th->syn &&
+	/*
+	 * Syn-proxy step 2 logic: receive client's
+	 * 3-handshake Ack packet
+	 */
+	if (ip_vs_synproxy_ack_rcv(af, skb, th, pp, cpp, &iph, verdict) == 0) {
+		return 0;
+	}
+
+
+	if( af & IP_VS_CONN_F_DSNAT ){
+		if (th->syn &&
+			(svc = ip_vs_service_get(af,
+			skb->mark, iph.protocol, &iph.daddr,th->dest))) {
+			if (ip_vs_todrop()) {
+				ip_vs_service_put(svc);
+				*verdict = NF_DROP;
+				return 0;
+			}
+			*cpp = ip_vs_schedule(svc, skb, 0);
+			if (!*cpp) {
+				*verdict = ip_vs_leave(svc, skb, pp);
+				return 0;
+			}
+			ip_vs_service_put(svc);
+		}
+		return 1;
+	}
+
+
+	if (th->syn && !th->ack && !th->fin && !th->rst &&
 	    (svc = ip_vs_service_get(af, skb->mark, iph.protocol, &iph.daddr,
 				     th->dest))) {
 		if (ip_vs_todrop()) {
@@ -106,13 +143,23 @@ tcp_conn_schedule(int af, struct sk_buff *skb, struct ip_vs_protocol *pp,
 		 * Let the virtual server select a real server for the
 		 * incoming connection, and create a connection entry.
 		 */
-		*cpp = ip_vs_schedule(svc, skb);
+		*cpp = ip_vs_schedule(svc, skb, 0);
 		if (!*cpp) {
 			*verdict = ip_vs_leave(svc, skb, pp);
 			return 0;
 		}
 		ip_vs_service_put(svc);
+		return 1;
 	}
+
+	/* drop tcp packet which send to vip and !vport */
+	if (sysctl_ip_vs_tcp_drop_entry &&
+	    (svc = ip_vs_lookup_vip(af, iph.protocol, &iph.daddr))) {
+		IP_VS_INC_ESTATS(ip_vs_esmib, DEFENCE_TCP_DROP);
+		*verdict = NF_DROP;
+		return 0;
+	}
+
 	return 1;
 }
 
@@ -126,18 +173,23 @@ tcp_fast_csum_update(int af, struct tcphdr *tcph,
 #ifdef CONFIG_IP_VS_IPV6
 	if (af == AF_INET6)
 		tcph->check =
-			csum_fold(ip_vs_check_diff16(oldip->ip6, newip->ip6,
-					 ip_vs_check_diff2(oldport, newport,
-						~csum_unfold(tcph->check))));
+		    csum_fold(ip_vs_check_diff16(oldip->ip6, newip->ip6,
+						 ip_vs_check_diff2(oldport,
+								   newport,
+								   ~csum_unfold
+								   (tcph->
+								    check))));
 	else
 #endif
-	tcph->check =
-		csum_fold(ip_vs_check_diff4(oldip->ip, newip->ip,
-				 ip_vs_check_diff2(oldport, newport,
-						~csum_unfold(tcph->check))));
+		tcph->check =
+		    csum_fold(ip_vs_check_diff4(oldip->ip, newip->ip,
+						ip_vs_check_diff2(oldport,
+								  newport,
+								  ~csum_unfold
+								  (tcph->
+								   check))));
 }
 
-
 static inline void
 tcp_partial_csum_update(int af, struct tcphdr *tcph,
 		     const union nf_inet_addr *oldip,
@@ -147,17 +199,203 @@ tcp_partial_csum_update(int af, struct tcphdr *tcph,
 #ifdef CONFIG_IP_VS_IPV6
 	if (af == AF_INET6)
 		tcph->check =
-			csum_fold(ip_vs_check_diff16(oldip->ip6, newip->ip6,
-					 ip_vs_check_diff2(oldlen, newlen,
-						~csum_unfold(tcph->check))));
+			~csum_fold(ip_vs_check_diff16(oldip->ip6, newip->ip6,
+					ip_vs_check_diff2(oldlen, newlen,
+						csum_unfold(tcph->check))));
 	else
 #endif
-	tcph->check =
-		csum_fold(ip_vs_check_diff4(oldip->ip, newip->ip,
-				ip_vs_check_diff2(oldlen, newlen,
-						~csum_unfold(tcph->check))));
+		tcph->check =
+			~csum_fold(ip_vs_check_diff4(oldip->ip, newip->ip,
+					ip_vs_check_diff2(oldlen, newlen,
+						csum_unfold(tcph->check))));
+}
+
+/* Calculate TCP checksum, only for PARTICAL */
+static inline void
+tcp_partial_csum_reset(int af, int len, struct tcphdr *tcph,
+				const union nf_inet_addr *saddr,
+				const union nf_inet_addr *daddr)
+{
+#ifdef CONFIG_IP_VS_IPV6
+	if (af == AF_INET6)
+		tcph->check = ~csum_ipv6_magic(&saddr->in6, &daddr->in6,
+							len, IPPROTO_TCP, 0);
+        else
+#endif
+		tcph->check = ~tcp_v4_check(len, saddr->ip, daddr->ip, 0);
+}
+
+/* adjust tcp opt mss, sub TCPOLEN_CIP */
+static void tcp_opt_adjust_mss(int af, struct tcphdr *tcph)
+{
+	unsigned char *ptr;
+	int length;
+
+	if (sysctl_ip_vs_mss_adjust_entry == 0)
+		return;
+
+	ptr = (unsigned char *)(tcph + 1);
+	length = (tcph->doff * 4) - sizeof(struct tcphdr);
+
+	while (length > 0) {
+		int opcode = *ptr++;
+		int opsize;
+
+		switch (opcode) {
+		case TCPOPT_EOL:
+			return;
+		case TCPOPT_NOP:	/* Ref: RFC 793 section 3.1 */
+			length--;
+			continue;
+		default:
+			opsize = *ptr++;
+			if (opsize < 2)	/* "silly options" */
+				return;
+			if (opsize > length)
+				return;	/* don't parse partial options */
+			if ((opcode == TCPOPT_MSS) && (opsize == TCPOLEN_MSS)) {
+				__be16 old = *(__be16 *) ptr;
+				__u16 in_mss = ntohs(*(__be16 *) ptr);
+#ifdef CONFIG_IP_VS_IPV6
+				if (af == AF_INET6)
+					in_mss -= TCPOLEN_ADDR_V6;
+				else
+#endif
+					in_mss -= TCPOLEN_ADDR;
+				*((__be16 *) ptr) = htons(in_mss);/* set mss, 16bit */
+				tcph->check = csum_fold(ip_vs_check_diff2(old,
+								*(__be16 *) ptr,
+						~csum_unfold(tcph->check)));
+				return;
+			}
+
+			ptr += opsize - 2;
+			length -= opsize;
+		}
+	}
 }
 
+/* save tcp sequense for fullnat/nat, INside to OUTside */
+static void
+tcp_save_out_seq(struct sk_buff *skb, struct ip_vs_conn *cp,
+		 struct tcphdr *th, int ihl)
+{
+	if (unlikely(th == NULL) || unlikely(cp == NULL) ||
+	    unlikely(skb == NULL))
+		return;
+
+	if (sysctl_ip_vs_conn_expire_tcp_rst && !th->rst) {
+
+		/* seq out of order. just skip */
+		if (before(ntohl(th->ack_seq), ntohl(cp->rs_ack_seq)) &&
+							(cp->rs_ack_seq != 0))
+			return;
+
+		if (th->syn && th->ack)
+			cp->rs_end_seq = htonl(ntohl(th->seq) + 1);
+		else
+			cp->rs_end_seq = htonl(ntohl(th->seq) + skb->len
+					       - ihl - (th->doff << 2));
+		cp->rs_ack_seq = th->ack_seq;
+		IP_VS_DBG_RL("packet from RS, seq:%u ack_seq:%u.",
+			     ntohl(th->seq), ntohl(th->ack_seq));
+		IP_VS_DBG_RL("port:%u->%u", ntohs(th->source), ntohs(th->dest));
+	}
+}
+
+/*
+ * 1. adjust tcp ack/sack sequence for FULL-NAT, INside to OUTside
+ * 2. adjust tcp sequence for SYNPROXY, OUTside to INside
+ */
+static int tcp_out_adjust_seq(struct ip_vs_conn *cp, struct tcphdr *tcph)
+{
+	__u8 i;
+	__u8 *ptr;
+	int length;
+	__be32 old_seq;
+
+	/*
+	 * Syn-proxy seq change, include tcp hdr and
+	 * check ack storm.
+	 */
+	if (ip_vs_synproxy_snat_handler(tcph, cp) == 0) {
+		return 0;
+	}
+
+	/*
+	 * FULLNAT ack-seq change
+	 */
+
+	old_seq = tcph->ack_seq;
+	/* adjust ack sequence */
+	tcph->ack_seq = htonl(ntohl(tcph->ack_seq) - cp->fnat_seq.delta);
+	/* update checksum */
+	tcph->check = csum_fold(ip_vs_check_diff4(old_seq, tcph->ack_seq,
+						~csum_unfold(tcph->check)));
+
+	/* adjust sack sequence */
+	ptr = (__u8 *) (tcph + 1);
+	length = (tcph->doff * 4) - sizeof(struct tcphdr);
+
+	/* Fast path for timestamp-only option */
+	if (length == TCPOLEN_TSTAMP_ALIGNED &&
+		*(__be32 *) ptr == htonl((TCPOPT_NOP << 24) |
+					(TCPOPT_NOP << 16) |
+					(TCPOPT_TIMESTAMP << 8) |
+					TCPOLEN_TIMESTAMP))
+		return 1;
+
+	while (length > 0) {
+		int opcode = *ptr++;
+		int opsize;
+
+		switch (opcode) {
+		case TCPOPT_EOL:
+			return 1;
+		case TCPOPT_NOP:	/* Ref: RFC 793 section 3.1 */
+			length--;
+			continue;
+		default:
+			opsize = *ptr++;
+			if (opsize < 2)	/* "silly options" */
+				return 1;
+			if (opsize > length)
+				return 1;	/* don't parse partial options */
+			if ((opcode == TCPOPT_SACK) &&
+			(opsize >= (TCPOLEN_SACK_BASE + TCPOLEN_SACK_PERBLOCK))
+			&& !((opsize - TCPOLEN_SACK_BASE) %
+						TCPOLEN_SACK_PERBLOCK)) {
+				for (i = 0; i < opsize - TCPOLEN_SACK_BASE;
+						i += TCPOLEN_SACK_PERBLOCK) {
+					__be32 *tmp = (__be32 *) (ptr + i);
+					old_seq = *tmp;
+					*tmp = htonl(ntohl(*tmp) -
+							cp->fnat_seq.delta);
+					tcph->check =
+						csum_fold(ip_vs_check_diff4(
+								old_seq, *tmp,
+						~csum_unfold(tcph->check)));
+
+					tmp++;
+
+					old_seq = *tmp;
+					*tmp = htonl(ntohl(*tmp) -
+							cp->fnat_seq.delta);
+					tcph->check =
+						csum_fold(ip_vs_check_diff4(
+								old_seq, *tmp,
+						~csum_unfold(tcph->check)));
+				}
+				return 1;
+			}
+
+			ptr += opsize - 2;
+			length -= opsize;
+		}
+	}
+
+	return 1;
+}
 
 static int
 tcp_snat_handler(struct sk_buff *skb,
@@ -190,13 +428,21 @@ tcp_snat_handler(struct sk_buff *skb,
 	}
 
 	tcph = (void *)skb_network_header(skb) + tcphoff;
+	tcp_save_out_seq(skb, cp, tcph, tcphoff);
 	tcph->source = cp->vport;
 
+	/*
+	 * Syn-proxy seq change, include tcp hdr and
+	 * check ack storm.
+	 */
+	if (ip_vs_synproxy_snat_handler(tcph, cp) == 0) {
+		return 0;
+	}
+
 	/* Adjust TCP checksums */
 	if (skb->ip_summed == CHECKSUM_PARTIAL) {
-		tcp_partial_csum_update(cp->af, tcph, &cp->daddr, &cp->vaddr,
-					htons(oldlen),
-					htons(skb->len - tcphoff));
+		tcp_partial_csum_reset(cp->af, (skb->len - tcphoff),
+					tcph, &cp->vaddr, &cp->caddr);
 	} else if (!cp->app) {
 		/* Only port and addr are changed, do fast csum update */
 		tcp_fast_csum_update(cp->af, tcph, &cp->daddr, &cp->vaddr,
@@ -228,6 +474,455 @@ tcp_snat_handler(struct sk_buff *skb,
 	return 1;
 }
 
+/*
+ * init first data sequence, INside to OUTside;
+ */
+static inline void
+tcp_out_init_seq(struct ip_vs_conn *cp, struct tcphdr *tcph)
+{
+	cp->fnat_seq.fdata_seq = ntohl(tcph->seq) + 1;
+}
+
+
+static int
+tcp_fnat_out_handler(struct sk_buff *skb,
+		     struct ip_vs_protocol *pp, struct ip_vs_conn *cp)
+{
+	struct tcphdr *tcph;
+	unsigned int tcphoff;
+	int oldlen;
+
+#ifdef CONFIG_IP_VS_IPV6
+	if (cp->af == AF_INET6)
+		tcphoff = sizeof(struct ipv6hdr);
+	else
+#endif
+		tcphoff = ip_hdrlen(skb);
+	oldlen = skb->len - tcphoff;
+
+	/* csum_check requires unshared skb */
+	if (!skb_make_writable(skb, tcphoff + sizeof(*tcph)))
+		return 0;
+
+	if (unlikely(cp->app != NULL)) {
+		/* Some checks before mangling */
+		if (pp->csum_check && !pp->csum_check(cp->af, skb, pp))
+			return 0;
+
+		/* Call application helper if needed */
+		if (!ip_vs_app_pkt_out(cp, skb))
+			return 0;
+	}
+
+	tcph = (void *)skb_network_header(skb) + tcphoff;
+	tcp_save_out_seq(skb, cp, tcph, tcphoff);
+	tcph->source = cp->vport;
+	tcph->dest = cp->cport;
+
+	/*
+	 * for syn_ack
+	 * 1. adjust tcp opt mss in rs->client
+	 */
+	if (tcph->syn && tcph->ack) {
+		tcp_opt_adjust_mss(cp->af, tcph);
+	}
+
+	/* adjust tcp ack/sack sequence */
+	if (tcp_out_adjust_seq(cp, tcph) == 0) {
+		return 0;
+	}
+
+	/*
+	 * for syn_ack
+	 * 2. init sequence
+	 */
+	if (tcph->syn && tcph->ack) {
+		tcp_out_init_seq(cp, tcph);
+	}
+
+	/* Adjust TCP checksums */
+	if (skb->ip_summed == CHECKSUM_PARTIAL) {
+		tcp_partial_csum_reset(cp->af, (skb->len - tcphoff),
+					tcph, &cp->vaddr, &cp->caddr);
+	} else if (!cp->app) {
+		/* Only port and addr are changed, do fast csum update */
+		tcp_fast_csum_update(cp->af, tcph, &cp->daddr, &cp->vaddr,
+				     cp->dport, cp->vport);
+		tcp_fast_csum_update(cp->af, tcph, &cp->laddr, &cp->caddr,
+				     cp->lport, cp->cport);
+		if (skb->ip_summed == CHECKSUM_COMPLETE)
+			skb->ip_summed = CHECKSUM_NONE;
+	} else {
+		/* full checksum calculation */
+		tcph->check = 0;
+		skb->csum = skb_checksum(skb, tcphoff, skb->len - tcphoff, 0);
+#ifdef CONFIG_IP_VS_IPV6
+		if (cp->af == AF_INET6)
+			tcph->check = csum_ipv6_magic(&cp->vaddr.in6,
+						      &cp->caddr.in6,
+						      skb->len - tcphoff,
+						      cp->protocol, skb->csum);
+		else
+#endif
+			tcph->check = csum_tcpudp_magic(cp->vaddr.ip,
+							cp->caddr.ip,
+							skb->len - tcphoff,
+							cp->protocol, skb->csum);
+
+		IP_VS_DBG(11, "O-pkt: %s O-csum=%d (+%zd)\n",
+			pp->name, tcph->check,
+			(char *)&(tcph->check) - (char *)tcph);
+	}
+	return 1;
+}
+
+/*
+ * remove tcp timestamp opt in one packet, just set it to TCPOPT_NOP
+ * reference to tcp_parse_options in tcp_input.c
+ */
+static void tcp_opt_remove_timestamp(struct tcphdr *tcph)
+{
+	unsigned char *ptr;
+	__be32 old[4], new[4];
+	int length;
+	int i;
+
+	if (sysctl_ip_vs_timestamp_remove_entry == 0)
+		return;
+
+	ptr = (unsigned char *)(tcph + 1);
+	length = (tcph->doff * 4) - sizeof(struct tcphdr);
+
+	while (length > 0) {
+		int opcode = *ptr++;
+		int opsize;
+
+		switch (opcode) {
+		case TCPOPT_EOL:
+			return;
+		case TCPOPT_NOP:	/* Ref: RFC 793 section 3.1 */
+			length--;
+			continue;
+		default:
+			opsize = *ptr++;
+			if (opsize < 2)	/* "silly options" */
+				return;
+			if (opsize > length)
+				return;	/* don't parse partial options */
+			if ((opcode == TCPOPT_TIMESTAMP)
+			    && (opsize == TCPOLEN_TIMESTAMP)) {
+				/* the length of buf is 16Byte,
+				 * but data is 10Byte. zero the buf
+				 */
+				memset((__u8*)old, 0, sizeof(old));
+				memset((__u8*)new, 0, sizeof(new));
+				memcpy((__u8*)old, ptr - 2, TCPOLEN_TIMESTAMP);
+				for (i = 0; i < TCPOLEN_TIMESTAMP; i++) {
+					*(ptr - 2 + i) = TCPOPT_NOP;	/* TCPOPT_NOP replace timestamp opt */
+				}
+				memcpy((__u8*)new, ptr - 2, TCPOLEN_TIMESTAMP);
+				tcph->check = csum_fold(ip_vs_check_diff16(
+								old, new,
+						~csum_unfold(tcph->check)));
+				return;
+			}
+
+			ptr += opsize - 2;
+			length -= opsize;
+		}
+	}
+}
+
+/*
+ * recompute tcp sequence, OUTside to INside;
+ */
+static void
+tcp_in_init_seq(struct ip_vs_conn *cp, struct sk_buff *skb, struct tcphdr *tcph)
+{
+	struct ip_vs_seq *fseq = &(cp->fnat_seq);
+	__u32 seq = ntohl(tcph->seq);
+	int conn_reused_entry;
+
+	if ((fseq->delta == fseq->init_seq - seq) && (fseq->init_seq != 0)) {
+		/* retransmit */
+		return;
+	}
+
+	/* init syn seq, lvs2rs */
+	conn_reused_entry = (sysctl_ip_vs_conn_reused_entry == 1)
+	    && (fseq->init_seq != 0)
+	    && ((cp->state == IP_VS_TCP_S_SYN_RECV)
+		|| (cp->state == IP_VS_TCP_S_SYN_SENT));
+	if ((fseq->init_seq == 0) || conn_reused_entry) {
+#ifdef CONFIG_IP_VS_IPV6
+		if (cp->af == AF_INET6)
+			fseq->init_seq =
+			    secure_tcpv6_sequence_number(cp->laddr.ip6,
+							 cp->daddr.ip6,
+							 cp->lport, cp->dport);
+		else
+#endif
+			fseq->init_seq =
+			    secure_tcp_sequence_number(cp->laddr.ip,
+						       cp->daddr.ip, cp->lport,
+						       cp->dport);
+		fseq->delta = fseq->init_seq - seq;
+
+		if (conn_reused_entry) {
+			IP_VS_INC_ESTATS(ip_vs_esmib, FULLNAT_CONN_REUSED);
+			switch (cp->old_state) {
+			case IP_VS_TCP_S_CLOSE:
+				IP_VS_INC_ESTATS(ip_vs_esmib,
+						 FULLNAT_CONN_REUSED_CLOSE);
+				break;
+			case IP_VS_TCP_S_TIME_WAIT:
+				IP_VS_INC_ESTATS(ip_vs_esmib,
+						 FULLNAT_CONN_REUSED_TIMEWAIT);
+				break;
+			case IP_VS_TCP_S_FIN_WAIT:
+				IP_VS_INC_ESTATS(ip_vs_esmib,
+						 FULLNAT_CONN_REUSED_FINWAIT);
+				break;
+			case IP_VS_TCP_S_CLOSE_WAIT:
+				IP_VS_INC_ESTATS(ip_vs_esmib,
+						 FULLNAT_CONN_REUSED_CLOSEWAIT);
+				break;
+			case IP_VS_TCP_S_LAST_ACK:
+				IP_VS_INC_ESTATS(ip_vs_esmib,
+						 FULLNAT_CONN_REUSED_LASTACK);
+				break;
+			case IP_VS_TCP_S_ESTABLISHED:
+				IP_VS_INC_ESTATS(ip_vs_esmib,
+						 FULLNAT_CONN_REUSED_ESTAB);
+				break;
+			}
+		}
+	}
+}
+
+/* adjust tcp sequence, OUTside to INside */
+static void tcp_in_adjust_seq(struct ip_vs_conn *cp, struct tcphdr *tcph)
+{
+	__be32 old_seq = tcph->seq;
+	/* adjust seq for FULLNAT */
+	tcph->seq = htonl(ntohl(tcph->seq) + cp->fnat_seq.delta);
+	/* update checksum */
+	tcph->check = csum_fold(ip_vs_check_diff4(old_seq, tcph->seq,
+					~csum_unfold(tcph->check)));
+
+	/* adjust ack_seq for SYNPROXY, include tcp hdr and sack opt */
+	ip_vs_synproxy_dnat_handler(tcph, &cp->syn_proxy_seq);
+}
+
+/*
+ * add client address in tcp option
+ * alloc a new skb, and free the old skb
+ * return new skb
+ */
+static struct sk_buff *tcp_opt_add_toa(struct ip_vs_conn *cp,
+				       struct sk_buff *old_skb,
+				       struct tcphdr **tcph)
+{
+	__u32 mtu;
+	struct sk_buff *new_skb = NULL;
+	struct ip_vs_tcpo_addr *toa;
+	unsigned int tcphoff;
+	struct tcphdr *th;
+	__u8 *p, *q;
+
+	/* now only process IPV4 */
+	if (cp->af != AF_INET) {
+		IP_VS_INC_ESTATS(ip_vs_esmib, FULLNAT_ADD_TOA_FAIL_PROTO);
+		return old_skb;
+	}
+
+	/* skb length and tcp option length checking */
+	mtu = dst_mtu((struct dst_entry *)old_skb->_skb_dst);
+	if (old_skb->len > (mtu - sizeof(struct ip_vs_tcpo_addr))) {
+		IP_VS_INC_ESTATS(ip_vs_esmib, FULLNAT_ADD_TOA_FAIL_LEN);
+		return old_skb;
+	}
+
+	/* the maximum length of TCP head is 60 bytes, so only 40 bytes for options */
+	if ((60 - ((*tcph)->doff << 2)) < sizeof(struct ip_vs_tcpo_addr)) {
+		IP_VS_INC_ESTATS(ip_vs_esmib, FULLNAT_ADD_TOA_HEAD_FULL);
+		return old_skb;
+	}
+
+	/* copy all skb, plus ttm space , new skb is linear */
+	new_skb = skb_copy_expand(old_skb,
+				  skb_headroom(old_skb),
+				  skb_tailroom(old_skb) +
+				  sizeof(struct ip_vs_tcpo_addr), GFP_ATOMIC);
+	if (new_skb == NULL) {
+		IP_VS_INC_ESTATS(ip_vs_esmib, FULLNAT_ADD_TOA_FAIL_MEM);
+		return old_skb;
+	}
+
+	/* free old skb */
+	kfree_skb(old_skb);
+
+	/*
+	 * add client ip
+	 */
+	tcphoff = ip_hdrlen(new_skb);
+	/* get new tcp header */
+	*tcph = th =
+	    (struct tcphdr *)((void *)skb_network_header(new_skb) + tcphoff);
+
+	/* ptr to old opts */
+	p = skb_tail_pointer(new_skb) - 1;
+	q = p + sizeof(struct ip_vs_tcpo_addr);
+
+	/* move data down, offset is sizeof(struct ip_vs_tcpo_addr) */
+	while (p >= ((__u8 *) th + sizeof(struct tcphdr))) {
+		*q = *p;
+		p--;
+		q--;
+	}
+
+	/* move tail to new postion */
+	new_skb->tail += sizeof(struct ip_vs_tcpo_addr);
+
+	/* put client ip opt , ptr point to opts */
+	toa = (struct ip_vs_tcpo_addr *)(th + 1);
+	toa->opcode = TCPOPT_ADDR;
+	toa->opsize = TCPOLEN_ADDR;
+	toa->port = cp->cport;
+	toa->addr = cp->caddr.ip;
+
+	/* reset tcp header length */
+	th->doff += sizeof(struct ip_vs_tcpo_addr) / 4;
+	/* reset ip header totoal length */
+	ip_hdr(new_skb)->tot_len =
+	    htons(ntohs(ip_hdr(new_skb)->tot_len) +
+		  sizeof(struct ip_vs_tcpo_addr));
+	/* reset skb length */
+	new_skb->len += sizeof(struct ip_vs_tcpo_addr);
+
+	/* re-calculate tcp csum */
+	th->check = 0;
+	new_skb->csum = skb_checksum(new_skb, tcphoff,
+					new_skb->len - tcphoff, 0);
+	th->check = csum_tcpudp_magic(cp->caddr.ip,
+					cp->vaddr.ip,
+					new_skb->len - tcphoff,
+					cp->protocol, new_skb->csum);
+
+	/* re-calculate ip head csum, tot_len has been adjusted */
+	ip_send_check(ip_hdr(new_skb));
+
+	if(new_skb->ip_summed == CHECKSUM_PARTIAL) {
+		new_skb->ip_summed = CHECKSUM_COMPLETE;
+		skb_shinfo(new_skb)->gso_size = 0;
+	}
+
+	IP_VS_INC_ESTATS(ip_vs_esmib, FULLNAT_ADD_TOA_OK);
+
+	return new_skb;
+}
+
+#ifdef CONFIG_IP_VS_IPV6
+static struct sk_buff *tcp_opt_add_toa_v6(struct ip_vs_conn *cp,
+				       struct sk_buff *old_skb,
+				       struct tcphdr **tcph)
+{
+	__u32 mtu;
+	struct sk_buff *new_skb = NULL;
+	struct ip_vs_tcpo_addr_v6 *toa;
+	unsigned int tcphoff;
+	struct tcphdr *th;
+	__u8 *p, *q;
+
+	/* IPV6 */
+	if (cp->af != AF_INET6) {
+		IP_VS_INC_ESTATS(ip_vs_esmib, FULLNAT_ADD_TOA_FAIL_PROTO);
+		return old_skb;
+	}
+
+	/* skb length and tcph length checking */
+	mtu = dst_mtu((struct dst_entry *)old_skb->_skb_dst);
+	if (old_skb->len > (mtu - sizeof(struct ip_vs_tcpo_addr_v6))) {
+		IP_VS_INC_ESTATS(ip_vs_esmib, FULLNAT_ADD_TOA_FAIL_LEN);
+		return old_skb;
+	}
+
+	/* the maximum length of TCP head is 60 bytes, so only 40 bytes for options */
+	if ((60 - ((*tcph)->doff << 2)) < sizeof(struct ip_vs_tcpo_addr_v6)) {
+		IP_VS_INC_ESTATS(ip_vs_esmib, FULLNAT_ADD_TOA_HEAD_FULL);
+		return old_skb;
+	}
+
+	/* copy all skb, plus ttm space , new skb is linear */
+	new_skb = skb_copy_expand(old_skb,
+				  skb_headroom(old_skb),
+				  skb_tailroom(old_skb) +
+				  sizeof(struct ip_vs_tcpo_addr_v6), GFP_ATOMIC);
+	if (new_skb == NULL) {
+		IP_VS_INC_ESTATS(ip_vs_esmib, FULLNAT_ADD_TOA_FAIL_MEM);
+		return old_skb;
+	}
+
+	/* free old skb */
+	kfree_skb(old_skb);
+
+	/*
+	 * add client ip
+	 */
+	tcphoff = sizeof(struct ipv6hdr);
+	/* get new tcp header */
+	*tcph = th =
+	    (struct tcphdr *)((void *)skb_network_header(new_skb) + tcphoff);
+
+	/* ptr to old opts */
+	p = skb_tail_pointer(new_skb) - 1;
+	q = p + sizeof(struct ip_vs_tcpo_addr_v6);
+
+	/* move data down, offset is sizeof(struct ip_vs_tcpo_addr) */
+	while (p >= ((__u8 *) th + sizeof(struct tcphdr))) {
+		*q = *p;
+		p--;
+		q--;
+	}
+
+	/* move tail to new postion */
+	new_skb->tail += sizeof(struct ip_vs_tcpo_addr_v6);
+
+	/* put client ip opt , ptr point to opts */
+	toa = (struct ip_vs_tcpo_addr_v6 *)(th + 1);
+	toa->opcode = TCPOPT_ADDR_V6;
+	toa->opsize = TCPOLEN_ADDR_V6;
+	toa->port = cp->cport;
+	toa->addr = cp->caddr.in6;
+
+	/* reset tcp header length */
+	th->doff += sizeof(struct ip_vs_tcpo_addr_v6) >> 2;
+	/* reset ip header totoal length */
+	ipv6_hdr(new_skb)->payload_len =
+	    htons(ntohs(ipv6_hdr(new_skb)->payload_len) +
+		  sizeof(struct ip_vs_tcpo_addr_v6));
+	/* reset skb length */
+	new_skb->len += sizeof(struct ip_vs_tcpo_addr_v6);
+
+	/* re-calculate tcp csum */
+	th->check = 0;
+	new_skb->csum = skb_checksum(new_skb, tcphoff,
+					new_skb->len - tcphoff, 0);
+	th->check = csum_ipv6_magic(&cp->caddr.in6,
+					&cp->vaddr.in6,
+					new_skb->len - tcphoff,
+					cp->protocol, new_skb->csum);
+
+	if(new_skb->ip_summed == CHECKSUM_PARTIAL) {
+		new_skb->ip_summed = CHECKSUM_COMPLETE;
+		skb_shinfo(new_skb)->gso_size = 0;
+	}
+
+	IP_VS_INC_ESTATS(ip_vs_esmib, FULLNAT_ADD_TOA_OK);
+
+	return new_skb;
+}
+#endif
 
 static int
 tcp_dnat_handler(struct sk_buff *skb,
@@ -266,12 +961,16 @@ tcp_dnat_handler(struct sk_buff *skb,
 	tcph->dest = cp->dport;
 
 	/*
-	 *	Adjust TCP checksums
+	 * Syn-proxy ack_seq change, include tcp hdr and sack opt.
+	 */
+	ip_vs_synproxy_dnat_handler(tcph, &cp->syn_proxy_seq);
+
+	/*
+	 *      Adjust TCP checksums
 	 */
 	if (skb->ip_summed == CHECKSUM_PARTIAL) {
-		tcp_partial_csum_update(cp->af, tcph, &cp->daddr, &cp->vaddr,
-					htons(oldlen),
-					htons(skb->len - tcphoff));
+		tcp_partial_csum_reset(cp->af, (skb->len - tcphoff),
+					tcph, &cp->caddr, &cp->daddr);
 	} else if (!cp->app) {
 		/* Only port and addr are changed, do fast csum update */
 		tcp_fast_csum_update(cp->af, tcph, &cp->vaddr, &cp->daddr,
@@ -300,6 +999,360 @@ tcp_dnat_handler(struct sk_buff *skb,
 	return 1;
 }
 
+static int
+tcp_fnat_in_handler(struct sk_buff **skb_p,
+		    struct ip_vs_protocol *pp, struct ip_vs_conn *cp)
+{
+	struct tcphdr *tcph;
+	unsigned int tcphoff;
+	int oldlen;
+	struct sk_buff *skb = *skb_p;
+
+#ifdef CONFIG_IP_VS_IPV6
+	if (cp->af == AF_INET6)
+		tcphoff = sizeof(struct ipv6hdr);
+	else
+#endif
+		tcphoff = ip_hdrlen(skb);
+	oldlen = skb->len - tcphoff;
+
+	/* csum_check requires unshared skb */
+	if (!skb_make_writable(skb, tcphoff + sizeof(*tcph)))
+		return 0;
+
+	if (unlikely(cp->app != NULL)) {
+		/* Some checks before mangling */
+		if (pp->csum_check && !pp->csum_check(cp->af, skb, pp))
+			return 0;
+
+		/*
+		 *      Attempt ip_vs_app call.
+		 *      It will fix ip_vs_conn and iph ack_seq stuff
+		 */
+		if (!ip_vs_app_pkt_in(cp, skb))
+			return 0;
+	}
+
+	tcph = (void *)skb_network_header(skb) + tcphoff;
+	/*
+	 * for syn packet
+	 * 1. remove tcp timestamp opt,
+	 *    because local address with diffrent client have the diffrent timestamp;
+	 * 2. recompute tcp sequence
+	 * 3. add toa
+	 */
+	if (tcph->syn & !tcph->ack) {
+		tcp_opt_remove_timestamp(tcph);
+		tcp_in_init_seq(cp, skb, tcph);
+#ifdef CONFIG_IP_VS_IPV6
+		if (cp->af == AF_INET6)
+			skb = *skb_p = tcp_opt_add_toa_v6(cp, skb, &tcph);
+		else
+#endif
+			skb = *skb_p = tcp_opt_add_toa(cp, skb, &tcph);
+	}
+
+	/* TOA: add client ip */
+	if ((sysctl_ip_vs_toa_entry == 1)
+	    && (ntohl(tcph->ack_seq) == cp->fnat_seq.fdata_seq)
+	    && !tcph->syn && !tcph->rst && !tcph->fin) {
+#ifdef CONFIG_IP_VS_IPV6
+		if (cp->af == AF_INET6)
+			skb = *skb_p = tcp_opt_add_toa_v6(cp, skb, &tcph);
+		else
+#endif
+			skb = *skb_p = tcp_opt_add_toa(cp, skb, &tcph);
+	}
+
+	/*
+	 * adjust tcp sequence, becase
+	 * 1. FULLNAT: local address with diffrent client have the diffrent sequence
+	 * 2. SYNPROXY: dont know rs->client synack sequence
+	 */
+	tcp_in_adjust_seq(cp, tcph);
+
+	/* adjust src/dst port */
+	tcph->source = cp->lport;
+	tcph->dest = cp->dport;
+
+	/* Adjust TCP checksums */
+	if (skb->ip_summed == CHECKSUM_PARTIAL) {
+		tcp_partial_csum_reset(cp->af, (skb->len - tcphoff),
+					tcph, &cp->laddr, &cp->daddr);
+	} else if (!cp->app) {
+		/* Only port and addr are changed, do fast csum update */
+		tcp_fast_csum_update(cp->af, tcph, &cp->vaddr, &cp->daddr,
+				     cp->vport, cp->dport);
+		tcp_fast_csum_update(cp->af, tcph, &cp->caddr, &cp->laddr,
+				     cp->cport, cp->lport);
+		if (skb->ip_summed == CHECKSUM_COMPLETE)
+			skb->ip_summed = CHECKSUM_NONE;
+	} else {
+		/* full checksum calculation */
+		tcph->check = 0;
+		skb->csum = skb_checksum(skb, tcphoff, skb->len - tcphoff, 0);
+#ifdef CONFIG_IP_VS_IPV6
+		if (cp->af == AF_INET6)
+			tcph->check = csum_ipv6_magic(&cp->laddr.in6,
+						      &cp->daddr.in6,
+						      skb->len - tcphoff,
+						      cp->protocol, skb->csum);
+		else
+#endif
+			tcph->check = csum_tcpudp_magic(cp->laddr.ip,
+							cp->daddr.ip,
+							skb->len - tcphoff,
+							cp->protocol, skb->csum);
+		skb->ip_summed = CHECKSUM_UNNECESSARY;
+	}
+	return 1;
+}
+
+/* send reset packet to RS */
+static void tcp_send_rst_in(struct ip_vs_protocol *pp, struct ip_vs_conn *cp)
+{
+	struct sk_buff *skb = NULL;
+	struct sk_buff *tmp_skb = NULL;
+	struct tcphdr *th;
+	unsigned int tcphoff;
+
+	skb = alloc_skb(MAX_TCP_HEADER, GFP_ATOMIC);
+	if (unlikely(skb == NULL)) {
+		IP_VS_ERR_RL("alloc skb failed when send rs RST packet\n");
+		return;
+	}
+
+	skb_reserve(skb, MAX_TCP_HEADER);
+	th = (struct tcphdr *)skb_push(skb, sizeof(struct tcphdr));
+	skb_reset_transport_header(skb);
+	skb->csum = 0;
+
+	/* set tcp head */
+	memset(th, 0, sizeof(struct tcphdr));
+	th->source = cp->cport;
+	th->dest = cp->vport;
+
+	/* set the reset seq of tcp head */
+	if ((cp->state == IP_VS_TCP_S_SYN_SENT) &&
+			((tmp_skb = skb_dequeue(&cp->ack_skb)) != NULL)) {
+		struct tcphdr *tcph;
+#ifdef CONFIG_IP_VS_IPV6
+		if (cp->af == AF_INET6)
+			tcphoff = sizeof(struct ipv6hdr);
+		else
+#endif
+			tcphoff = ip_hdrlen(tmp_skb);
+		tcph = (void *)skb_network_header(tmp_skb) + tcphoff;
+
+		th->seq = tcph->seq;
+		/* put back. Just for sending reset packet to client */
+		skb_queue_head(&cp->ack_skb, tmp_skb);
+		IP_VS_INC_ESTATS(ip_vs_esmib, RST_IN_SYN_SENT);
+	} else if (cp->state == IP_VS_TCP_S_ESTABLISHED) {
+		th->seq = cp->rs_ack_seq;
+		/* Be careful! fullnat */
+		if (cp->flags & IP_VS_CONN_F_FULLNAT)
+			th->seq = htonl(ntohl(th->seq) - cp->fnat_seq.delta);
+
+		IP_VS_INC_ESTATS(ip_vs_esmib, RST_IN_ESTABLISHED);
+	} else {
+		kfree_skb(skb);
+		IP_VS_DBG_RL("IPVS: Is SYN_SENT or ESTABLISHED ?");
+		return;
+	}
+
+	IP_VS_DBG_RL("IPVS: rst to rs seq: %u", htonl(th->seq));
+	th->ack_seq = 0;
+	th->doff = sizeof(struct tcphdr) >> 2;
+	th->rst = 1;
+
+	/*
+	 * Set ip hdr
+	 * Attention: set source and dest addr to ack skb's.
+	 * we rely on packet_xmit func to do NATs thing.
+	 */
+#ifdef CONFIG_IP_VS_IPV6
+	if (cp->af == AF_INET6) {
+		struct ipv6hdr *iph =
+		    (struct ipv6hdr *)skb_push(skb, sizeof(struct ipv6hdr));
+
+		tcphoff = sizeof(struct ipv6hdr);
+		skb_reset_network_header(skb);
+		memcpy(&iph->saddr, &cp->caddr.in6, sizeof(struct in6_addr));
+		memcpy(&iph->daddr, &cp->vaddr.in6, sizeof(struct in6_addr));
+
+		iph->version = 6;
+		iph->nexthdr = NEXTHDR_TCP;
+		iph->hop_limit = IPV6_DEFAULT_HOPLIMIT;
+		iph->payload_len = htons(sizeof(struct tcphdr));
+
+		th->check = 0;
+		skb->csum = skb_checksum(skb, tcphoff, skb->len - tcphoff, 0);
+		th->check = csum_ipv6_magic(&iph->saddr, &iph->daddr,
+					    skb->len - tcphoff,
+					    IPPROTO_TCP, skb->csum);
+	} else
+#endif
+	{
+		struct iphdr *iph =
+		    (struct iphdr *)skb_push(skb, sizeof(struct iphdr));
+
+		tcphoff = sizeof(struct iphdr);
+		skb_reset_network_header(skb);
+		iph->version = 4;
+		iph->ihl = 5;
+		iph->tot_len = htons(skb->len);
+		iph->frag_off = htons(IP_DF);
+		iph->ttl = IPDEFTTL;
+		iph->protocol = IPPROTO_TCP;
+		iph->saddr = cp->caddr.ip;
+		iph->daddr = cp->vaddr.ip;
+
+		ip_send_check(iph);
+
+		th->check = 0;
+		skb->csum = skb_checksum(skb, tcphoff, skb->len - tcphoff, 0);
+		th->check = csum_tcpudp_magic(iph->saddr, iph->daddr,
+					      skb->len - tcphoff,
+					      IPPROTO_TCP, skb->csum);
+	}
+
+	cp->packet_xmit(skb, cp, pp);
+}
+
+/* send reset packet to client */
+static void tcp_send_rst_out(struct ip_vs_protocol *pp, struct ip_vs_conn *cp)
+{
+	struct sk_buff *skb = NULL;
+	struct sk_buff *tmp_skb = NULL;
+	struct tcphdr *th;
+	unsigned int tcphoff;
+
+	skb = alloc_skb(MAX_TCP_HEADER, GFP_ATOMIC);
+	if (unlikely(skb == NULL)) {
+		IP_VS_ERR_RL("alloc skb failed when send client RST packet\n");
+		return;
+	}
+
+	skb_reserve(skb, MAX_TCP_HEADER);
+	th = (struct tcphdr *)skb_push(skb, sizeof(struct tcphdr));
+	skb_reset_transport_header(skb);
+	skb->csum = 0;
+
+	/* set tcp head */
+	memset(th, 0, sizeof(struct tcphdr));
+	th->source = cp->dport;
+	if (cp->flags & IP_VS_CONN_F_FULLNAT)
+		th->dest = cp->lport;
+	else
+		th->dest = cp->cport;
+
+	/* set the reset seq of tcp head*/
+	if ((cp->state == IP_VS_TCP_S_SYN_SENT) &&
+			((tmp_skb = skb_dequeue(&cp->ack_skb)) != NULL)) {
+		struct tcphdr *tcph;
+#ifdef CONFIG_IP_VS_IPV6
+		if (cp->af == AF_INET6)
+			tcphoff = sizeof(struct ipv6hdr);
+		else
+#endif
+			tcphoff = ip_hdrlen(tmp_skb);
+		tcph = (void *)skb_network_header(tmp_skb) + tcphoff;
+		/* Perhaps delta is 0 */
+		th->seq = htonl(ntohl(tcph->ack_seq) - cp->syn_proxy_seq.delta);
+		/* put back. Just for sending reset packet to RS */
+		skb_queue_head(&cp->ack_skb, tmp_skb);
+		IP_VS_INC_ESTATS(ip_vs_esmib, RST_OUT_SYN_SENT);
+	} else if (cp->state == IP_VS_TCP_S_ESTABLISHED) {
+		th->seq = cp->rs_end_seq;
+		IP_VS_INC_ESTATS(ip_vs_esmib, RST_OUT_ESTABLISHED);
+	} else {
+		kfree_skb(skb);
+		IP_VS_DBG_RL("IPVS: Is in SYN_SENT or ESTABLISHED ?");
+		return;
+	}
+
+	IP_VS_DBG_RL("IPVS: rst to client seq: %u", htonl(th->seq));
+	th->ack_seq = 0;
+	th->doff = sizeof(struct tcphdr) >> 2;
+	th->rst = 1;
+
+	/*
+	 * Set ip hdr
+	 * Attention: set source and dest addr to ack skb's.
+	 * we rely on response_xmit func to do NATs thing.
+	 */
+#ifdef CONFIG_IP_VS_IPV6
+	if (cp->af == AF_INET6) {
+		struct ipv6hdr *iph =
+		    (struct ipv6hdr *)skb_push(skb, sizeof(struct ipv6hdr));
+
+		tcphoff = sizeof(struct ipv6hdr);
+		skb_reset_network_header(skb);
+		memcpy(&iph->saddr, &cp->daddr.in6, sizeof(struct in6_addr));
+		memcpy(&iph->daddr, &cp->laddr.in6, sizeof(struct in6_addr));
+
+		iph->version = 6;
+		iph->nexthdr = NEXTHDR_TCP;
+		iph->hop_limit = IPV6_DEFAULT_HOPLIMIT;
+		iph->payload_len = htons(sizeof(struct tcphdr));
+
+		th->check = 0;
+		skb->csum = skb_checksum(skb, tcphoff, skb->len - tcphoff, 0);
+		th->check = csum_ipv6_magic(&iph->saddr, &iph->daddr,
+					    skb->len - tcphoff,
+					    IPPROTO_TCP, skb->csum);
+
+		if (cp->flags & IP_VS_CONN_F_FULLNAT)
+			ip_vs_fnat_response_xmit_v6(skb, pp, cp,
+						    sizeof(struct ipv6hdr));
+		else
+			ip_vs_normal_response_xmit_v6(skb, pp, cp,
+						      sizeof(struct ipv6hdr));
+	} else
+#endif
+	{
+		struct iphdr *iph =
+		    (struct iphdr *)skb_push(skb, sizeof(struct iphdr));
+
+		tcphoff = sizeof(struct iphdr);
+		skb_reset_network_header(skb);
+		iph->version = 4;
+		iph->ihl = 5;
+		iph->tot_len = htons(skb->len);
+		iph->frag_off = htons(IP_DF);
+		iph->ttl = IPDEFTTL;
+		iph->protocol = IPPROTO_TCP;
+		iph->saddr = cp->daddr.ip;
+		iph->daddr = cp->laddr.ip;
+
+		ip_send_check(iph);
+
+		th->check = 0;
+		skb->csum = skb_checksum(skb, tcphoff, skb->len - tcphoff, 0);
+		th->check = csum_tcpudp_magic(iph->saddr, iph->daddr,
+					      skb->len - tcphoff,
+					      IPPROTO_TCP, skb->csum);
+
+		if (cp->flags & IP_VS_CONN_F_FULLNAT)
+			ip_vs_fnat_response_xmit(skb, pp, cp, iph->ihl << 2);
+		else
+			ip_vs_normal_response_xmit(skb, pp, cp, iph->ihl << 2);
+	}
+}
+
+static void
+tcp_conn_expire_handler(struct ip_vs_protocol *pp, struct ip_vs_conn *cp)
+{
+	/* support fullnat and nat */
+	if (sysctl_ip_vs_conn_expire_tcp_rst &&
+	    (cp->flags & (IP_VS_CONN_F_FULLNAT | IP_VS_CONN_F_MASQ))) {
+		/* send reset packet to RS */
+		tcp_send_rst_in(pp, cp);
+		/* send reset packet to client */
+		tcp_send_rst_out(pp, cp);
+	}
+}
 
 static int
 tcp_csum_check(int af, struct sk_buff *skb, struct ip_vs_protocol *pp)
@@ -362,19 +1415,19 @@ static const int tcp_state_off[IP_VS_DIR_LAST] = {
 /*
  *	Timeout table[state]
  */
-static int tcp_timeouts[IP_VS_TCP_S_LAST+1] = {
-	[IP_VS_TCP_S_NONE]		=	2*HZ,
-	[IP_VS_TCP_S_ESTABLISHED]	=	15*60*HZ,
-	[IP_VS_TCP_S_SYN_SENT]		=	2*60*HZ,
-	[IP_VS_TCP_S_SYN_RECV]		=	1*60*HZ,
-	[IP_VS_TCP_S_FIN_WAIT]		=	2*60*HZ,
-	[IP_VS_TCP_S_TIME_WAIT]		=	2*60*HZ,
-	[IP_VS_TCP_S_CLOSE]		=	10*HZ,
-	[IP_VS_TCP_S_CLOSE_WAIT]	=	60*HZ,
-	[IP_VS_TCP_S_LAST_ACK]		=	30*HZ,
-	[IP_VS_TCP_S_LISTEN]		=	2*60*HZ,
-	[IP_VS_TCP_S_SYNACK]		=	120*HZ,
-	[IP_VS_TCP_S_LAST]		=	2*HZ,
+int sysctl_ip_vs_tcp_timeouts[IP_VS_TCP_S_LAST + 1] = {
+	[IP_VS_TCP_S_NONE] = 2 * HZ,
+	[IP_VS_TCP_S_ESTABLISHED] = 90 * HZ,
+	[IP_VS_TCP_S_SYN_SENT] = 3 * HZ,
+	[IP_VS_TCP_S_SYN_RECV] = 30 * HZ,
+	[IP_VS_TCP_S_FIN_WAIT] = 3 * HZ,
+	[IP_VS_TCP_S_TIME_WAIT] = 3 * HZ,
+	[IP_VS_TCP_S_CLOSE] = 3 * HZ,
+	[IP_VS_TCP_S_CLOSE_WAIT] = 3 * HZ,
+	[IP_VS_TCP_S_LAST_ACK] = 3 * HZ,
+	[IP_VS_TCP_S_LISTEN] = 2 * 60 * HZ,
+	[IP_VS_TCP_S_SYNACK] = 30 * HZ,
+	[IP_VS_TCP_S_LAST] = 2 * HZ,
 };
 
 static const char *const tcp_state_name_table[IP_VS_TCP_S_LAST+1] = {
@@ -427,7 +1480,7 @@ static struct tcp_states_t tcp_states [] = {
 /*        sNO, sES, sSS, sSR, sFW, sTW, sCL, sCW, sLA, sLI, sSA	*/
 /*syn*/ {{sSS, sES, sSS, sSR, sSS, sSS, sSS, sSS, sSS, sLI, sSR }},
 /*fin*/ {{sTW, sFW, sSS, sTW, sFW, sTW, sCL, sTW, sLA, sLI, sTW }},
-/*ack*/ {{sES, sES, sSS, sES, sFW, sTW, sCL, sCW, sLA, sES, sES }},
+/*ack*/ {{sES, sES, sES, sES, sFW, sTW, sCL, sCW, sLA, sES, sES }},
 /*rst*/ {{sCL, sCL, sSS, sCL, sCL, sTW, sCL, sCL, sCL, sCL, sCL }},
 
 /*	INPUT-ONLY */
@@ -559,6 +1612,7 @@ set_tcp_state(struct ip_vs_protocol *pp, struct ip_vs_conn *cp,
 		}
 	}
 
+	cp->old_state = cp->state;	// old_state called when connection reused
 	cp->timeout = pp->timeout_table[cp->state = new_state];
 }
 
@@ -701,7 +1755,7 @@ void ip_vs_tcp_conn_listen(struct ip_vs_conn *cp)
 static void ip_vs_tcp_init(struct ip_vs_protocol *pp)
 {
 	IP_VS_INIT_HASH_TABLE(tcp_apps);
-	pp->timeout_table = tcp_timeouts;
+	pp->timeout_table = sysctl_ip_vs_tcp_timeouts;
 }
 
 
@@ -725,6 +1779,8 @@ struct ip_vs_protocol ip_vs_protocol_tcp = {
 	.conn_out_get =		tcp_conn_out_get,
 	.snat_handler =		tcp_snat_handler,
 	.dnat_handler =		tcp_dnat_handler,
+	.fnat_in_handler = tcp_fnat_in_handler,
+	.fnat_out_handler = tcp_fnat_out_handler,
 	.csum_check =		tcp_csum_check,
 	.state_name =		tcp_state_name,
 	.state_transition =	tcp_state_transition,
@@ -732,4 +1788,5 @@ struct ip_vs_protocol ip_vs_protocol_tcp = {
 	.debug_packet =		ip_vs_tcpudp_debug_packet,
 	.timeout_change =	tcp_timeout_change,
 	.set_state_timeout =	tcp_set_state_timeout,
+	.conn_expire_handler = tcp_conn_expire_handler,
 };
diff --git a/net/netfilter/ipvs/ip_vs_proto_udp.c b/net/netfilter/ipvs/ip_vs_proto_udp.c
index e7a6885..8838b34 100644
--- a/net/netfilter/ipvs/ip_vs_proto_udp.c
+++ b/net/netfilter/ipvs/ip_vs_proto_udp.c
@@ -30,7 +30,7 @@
 static struct ip_vs_conn *
 udp_conn_in_get(int af, const struct sk_buff *skb, struct ip_vs_protocol *pp,
 		const struct ip_vs_iphdr *iph, unsigned int proto_off,
-		int inverse)
+		int inverse, int *res_dir)
 {
 	struct ip_vs_conn *cp;
 	__be16 _ports[2], *pptr;
@@ -40,23 +40,23 @@ udp_conn_in_get(int af, const struct sk_buff *skb, struct ip_vs_protocol *pp,
 		return NULL;
 
 	if (likely(!inverse)) {
-		cp = ip_vs_conn_in_get(af, iph->protocol,
-				       &iph->saddr, pptr[0],
-				       &iph->daddr, pptr[1]);
+		cp = ip_vs_conn_get(af, iph->protocol,
+				    &iph->saddr, pptr[0],
+				    &iph->daddr, pptr[1], res_dir);
 	} else {
-		cp = ip_vs_conn_in_get(af, iph->protocol,
-				       &iph->daddr, pptr[1],
-				       &iph->saddr, pptr[0]);
+		cp = ip_vs_conn_get(af, iph->protocol,
+				    &iph->daddr, pptr[1],
+				    &iph->saddr, pptr[0], res_dir);
 	}
 
 	return cp;
 }
 
-
-static struct ip_vs_conn *
-udp_conn_out_get(int af, const struct sk_buff *skb, struct ip_vs_protocol *pp,
-		 const struct ip_vs_iphdr *iph, unsigned int proto_off,
-		 int inverse)
+static struct ip_vs_conn *udp_conn_out_get(int af, const struct sk_buff *skb,
+					   struct ip_vs_protocol *pp,
+					   const struct ip_vs_iphdr *iph,
+					   unsigned int proto_off, int inverse,
+					   int *res_dir)
 {
 	struct ip_vs_conn *cp;
 	__be16 _ports[2], *pptr;
@@ -66,13 +66,13 @@ udp_conn_out_get(int af, const struct sk_buff *skb, struct ip_vs_protocol *pp,
 		return NULL;
 
 	if (likely(!inverse)) {
-		cp = ip_vs_conn_out_get(af, iph->protocol,
-					&iph->saddr, pptr[0],
-					&iph->daddr, pptr[1]);
+		cp = ip_vs_conn_get(af, iph->protocol,
+				    &iph->saddr, pptr[0],
+				    &iph->daddr, pptr[1], res_dir);
 	} else {
-		cp = ip_vs_conn_out_get(af, iph->protocol,
-					&iph->daddr, pptr[1],
-					&iph->saddr, pptr[0]);
+		cp = ip_vs_conn_get(af, iph->protocol,
+				    &iph->daddr, pptr[1],
+				    &iph->saddr, pptr[0], res_dir);
 	}
 
 	return cp;
@@ -112,7 +112,7 @@ udp_conn_schedule(int af, struct sk_buff *skb, struct ip_vs_protocol *pp,
 		 * Let the virtual server select a real server for the
 		 * incoming connection, and create a connection entry.
 		 */
-		*cpp = ip_vs_schedule(svc, skb);
+		*cpp = ip_vs_schedule(svc, skb, 0);
 		if (!*cpp) {
 			*verdict = ip_vs_leave(svc, skb, pp);
 			return 0;
@@ -132,15 +132,21 @@ udp_fast_csum_update(int af, struct udphdr *uhdr,
 #ifdef CONFIG_IP_VS_IPV6
 	if (af == AF_INET6)
 		uhdr->check =
-			csum_fold(ip_vs_check_diff16(oldip->ip6, newip->ip6,
-					 ip_vs_check_diff2(oldport, newport,
-						~csum_unfold(uhdr->check))));
+		    csum_fold(ip_vs_check_diff16(oldip->ip6, newip->ip6,
+						 ip_vs_check_diff2(oldport,
+								   newport,
+								   ~csum_unfold
+								   (uhdr->
+								    check))));
 	else
 #endif
 		uhdr->check =
-			csum_fold(ip_vs_check_diff4(oldip->ip, newip->ip,
-					 ip_vs_check_diff2(oldport, newport,
-						~csum_unfold(uhdr->check))));
+		    csum_fold(ip_vs_check_diff4(oldip->ip, newip->ip,
+						ip_vs_check_diff2(oldport,
+								  newport,
+								  ~csum_unfold
+								  (uhdr->
+								   check))));
 	if (!uhdr->check)
 		uhdr->check = CSUM_MANGLED_0;
 }
@@ -154,17 +160,37 @@ udp_partial_csum_update(int af, struct udphdr *uhdr,
 #ifdef CONFIG_IP_VS_IPV6
 	if (af == AF_INET6)
 		uhdr->check =
-			csum_fold(ip_vs_check_diff16(oldip->ip6, newip->ip6,
-					 ip_vs_check_diff2(oldlen, newlen,
-						~csum_unfold(uhdr->check))));
+		    csum_fold(ip_vs_check_diff16(oldip->ip6, newip->ip6,
+						 ip_vs_check_diff2(oldlen,
+								   newlen,
+								   ~csum_unfold
+								   (uhdr->
+								    check))));
 	else
 #endif
-	uhdr->check =
-		csum_fold(ip_vs_check_diff4(oldip->ip, newip->ip,
-				ip_vs_check_diff2(oldlen, newlen,
-						~csum_unfold(uhdr->check))));
+		uhdr->check =
+		    csum_fold(ip_vs_check_diff4(oldip->ip, newip->ip,
+						ip_vs_check_diff2(oldlen,
+								  newlen,
+								  ~csum_unfold
+								  (uhdr->
+								   check))));
 }
 
+/* Calculate UDP checksum, only for PARTICAL */
+static inline void
+udp_partial_csum_reset(int af, int len, struct udphdr *uhdr,
+				const union nf_inet_addr *saddr,
+				const union nf_inet_addr *daddr)
+{
+#ifdef CONFIG_IP_VS_IPV6
+	if (af == AF_INET6)
+		uhdr->check = ~csum_ipv6_magic(&saddr->in6, &daddr->in6,
+							len, IPPROTO_UDP, 0);
+        else
+#endif
+		uhdr->check = ~csum_tcpudp_magic(saddr->ip, daddr->ip, len, IPPROTO_UDP, 0);
+}
 
 static int
 udp_snat_handler(struct sk_buff *skb,
@@ -200,6 +226,7 @@ udp_snat_handler(struct sk_buff *skb,
 
 	udph = (void *)skb_network_header(skb) + udphoff;
 	udph->source = cp->vport;
+	udph->dest = cp->cport;
 
 	/*
 	 *	Adjust UDP checksums
@@ -208,10 +235,15 @@ udp_snat_handler(struct sk_buff *skb,
 		udp_partial_csum_update(cp->af, udph, &cp->daddr, &cp->vaddr,
 					htons(oldlen),
 					htons(skb->len - udphoff));
+		udp_partial_csum_update(cp->af, udph, &cp->laddr, &cp->caddr,
+					htons(oldlen),
+					htons(skb->len - udphoff));
 	} else if (!cp->app && (udph->check != 0)) {
 		/* Only port and addr are changed, do fast csum update */
 		udp_fast_csum_update(cp->af, udph, &cp->daddr, &cp->vaddr,
 				     cp->dport, cp->vport);
+		udp_fast_csum_update(cp->af, udph, &cp->laddr, &cp->caddr,
+				     cp->lport, cp->cport);
 		if (skb->ip_summed == CHECKSUM_COMPLETE)
 			skb->ip_summed = CHECKSUM_NONE;
 	} else {
@@ -275,19 +307,25 @@ udp_dnat_handler(struct sk_buff *skb,
 	}
 
 	udph = (void *)skb_network_header(skb) + udphoff;
+	udph->source = cp->lport;
 	udph->dest = cp->dport;
 
 	/*
 	 *	Adjust UDP checksums
 	 */
 	if (skb->ip_summed == CHECKSUM_PARTIAL) {
-		udp_partial_csum_update(cp->af, udph, &cp->daddr, &cp->vaddr,
+		udp_partial_csum_update(cp->af, udph, &cp->vaddr, &cp->daddr,
+					htons(oldlen),
+					htons(skb->len - udphoff));
+		udp_partial_csum_update(cp->af, udph, &cp->caddr, &cp->laddr,
 					htons(oldlen),
 					htons(skb->len - udphoff));
 	} else if (!cp->app && (udph->check != 0)) {
 		/* Only port and addr are changed, do fast csum update */
 		udp_fast_csum_update(cp->af, udph, &cp->vaddr, &cp->daddr,
 				     cp->vport, cp->dport);
+		udp_fast_csum_update(cp->af, udph, &cp->caddr, &cp->laddr,
+				     cp->cport, cp->lport);
 		if (skb->ip_summed == CHECKSUM_COMPLETE)
 			skb->ip_summed = CHECKSUM_NONE;
 	} else {
@@ -314,6 +352,149 @@ udp_dnat_handler(struct sk_buff *skb,
 	return 1;
 }
 
+static int
+udp_fnat_in_handler(struct sk_buff **skb_p,
+		    struct ip_vs_protocol *pp, struct ip_vs_conn *cp)
+{
+	struct udphdr *udph;
+	unsigned int udphoff;
+	int oldlen;
+	struct sk_buff *skb = *skb_p;
+
+#ifdef CONFIG_IP_VS_IPV6
+	if (cp->af == AF_INET6)
+		udphoff = sizeof(struct ipv6hdr);
+	else
+#endif
+		udphoff = ip_hdrlen(skb);
+	oldlen = skb->len - udphoff;
+
+	/* csum_check requires unshared skb */
+	if (!skb_make_writable(skb, udphoff + sizeof(*udph)))
+		return 0;
+
+	if (unlikely(cp->app != NULL)) {
+		/* Some checks before mangling */
+		if (pp->csum_check && !pp->csum_check(cp->af, skb, pp))
+			return 0;
+
+		/*
+		 *      Attempt ip_vs_app call.
+		 *      It will fix ip_vs_conn and iph ack_seq stuff
+		 */
+		if (!ip_vs_app_pkt_in(cp, skb))
+			return 0;
+	}
+
+	udph = (void *)skb_network_header(skb) + udphoff;
+
+	/* adjust src/dst port */
+	udph->source = cp->lport;
+	udph->dest = cp->dport;
+
+	/* Adjust UDP checksums */
+	if (skb->ip_summed == CHECKSUM_PARTIAL) {
+		udp_partial_csum_reset(cp->af, (skb->len - udphoff),
+					udph, &cp->laddr, &cp->daddr);
+	} else if (!cp->app) {
+		/* Only port and addr are changed, do fast csum update */
+		udp_fast_csum_update(cp->af, udph, &cp->vaddr, &cp->daddr,
+				     cp->vport, cp->dport);
+		udp_fast_csum_update(cp->af, udph, &cp->caddr, &cp->laddr,
+				     cp->cport, cp->lport);
+		if (skb->ip_summed == CHECKSUM_COMPLETE)
+			skb->ip_summed = CHECKSUM_NONE;
+	} else {
+		/* full checksum calculation */
+		udph->check = 0;
+		skb->csum = skb_checksum(skb, udphoff, skb->len - udphoff, 0);
+#ifdef CONFIG_IP_VS_IPV6
+		if (cp->af == AF_INET6)
+			udph->check = csum_ipv6_magic(&cp->laddr.in6,
+						      &cp->daddr.in6,
+						      skb->len - udphoff,
+						      cp->protocol, skb->csum);
+		else
+#endif
+			udph->check = csum_tcpudp_magic(cp->laddr.ip,
+							cp->daddr.ip,
+							skb->len - udphoff,
+							cp->protocol, skb->csum);
+		skb->ip_summed = CHECKSUM_UNNECESSARY;
+	}
+	return 1;
+}
+
+static int
+udp_fnat_out_handler(struct sk_buff *skb,
+		     struct ip_vs_protocol *pp, struct ip_vs_conn *cp)
+{
+	struct udphdr *udph;
+	unsigned int udphoff;
+	int oldlen;
+
+
+#ifdef CONFIG_IP_VS_IPV6
+	if (cp->af == AF_INET6)
+		udphoff = sizeof(struct ipv6hdr);
+	else
+#endif
+		udphoff = ip_hdrlen(skb);
+	oldlen = skb->len - udphoff;
+
+	/* csum_check requires unshared skb */
+	if (!skb_make_writable(skb, udphoff + sizeof(*udph)))
+		return 0;
+
+	if (unlikely(cp->app != NULL)) {
+		/* Some checks before mangling */
+		if (pp->csum_check && !pp->csum_check(cp->af, skb, pp))
+			return 0;
+
+		/* Call application helper if needed */
+		if (!ip_vs_app_pkt_out(cp, skb))
+			return 0;
+	}
+
+	udph = (void *)skb_network_header(skb) + udphoff;
+	udph->source = cp->vport;
+	udph->dest = cp->cport;
+
+	/* Adjust UDP checksums */
+	if (skb->ip_summed == CHECKSUM_PARTIAL) {
+		udp_partial_csum_reset(cp->af, (skb->len - udphoff),
+					udph, &cp->vaddr, &cp->caddr);
+	} else if (!cp->app) {
+		/* Only port and addr are changed, do fast csum update */
+		udp_fast_csum_update(cp->af, udph, &cp->daddr, &cp->vaddr,
+				     cp->dport, cp->vport);
+		udp_fast_csum_update(cp->af, udph, &cp->laddr, &cp->caddr,
+				     cp->lport, cp->cport);
+		if (skb->ip_summed == CHECKSUM_COMPLETE)
+			skb->ip_summed = CHECKSUM_NONE;
+	} else {
+		/* full checksum calculation */
+		udph->check = 0;
+		skb->csum = skb_checksum(skb, udphoff, skb->len - udphoff, 0);
+#ifdef CONFIG_IP_VS_IPV6
+		if (cp->af == AF_INET6)
+			udph->check = csum_ipv6_magic(&cp->vaddr.in6,
+						      &cp->caddr.in6,
+						      skb->len - udphoff,
+						      cp->protocol, skb->csum);
+		else
+#endif
+			udph->check = csum_tcpudp_magic(cp->vaddr.ip,
+							cp->caddr.ip,
+							skb->len - udphoff,
+							cp->protocol, skb->csum);
+
+		IP_VS_DBG(11, "O-pkt: %s O-csum=%d (+%zd)\n",
+			pp->name, udph->check,
+			(char *)&(udph->check) - (char *)udph);
+	}
+	return 1;
+}
 
 static int
 udp_csum_check(int af, struct sk_buff *skb, struct ip_vs_protocol *pp)
@@ -524,6 +705,8 @@ struct ip_vs_protocol ip_vs_protocol_udp = {
 	.conn_out_get =		udp_conn_out_get,
 	.snat_handler =		udp_snat_handler,
 	.dnat_handler =		udp_dnat_handler,
+	.fnat_in_handler = udp_fnat_in_handler,
+	.fnat_out_handler = udp_fnat_out_handler,
 	.csum_check =		udp_csum_check,
 	.state_transition =	udp_state_transition,
 	.state_name =		udp_state_name,
diff --git a/net/netfilter/ipvs/ip_vs_stats.c b/net/netfilter/ipvs/ip_vs_stats.c
new file mode 100644
index 0000000..3557e5d
--- /dev/null
+++ b/net/netfilter/ipvs/ip_vs_stats.c
@@ -0,0 +1,99 @@
+#include <linux/types.h>
+#include <linux/percpu.h>
+#include <net/ip_vs.h>
+
+
+int ip_vs_new_stats(struct ip_vs_stats **p)
+{
+	if(NULL == p)
+		return -EINVAL;
+
+	*p = alloc_percpu(struct ip_vs_stats);
+	if(NULL == *p) {
+		pr_err("%s: allocate per cpu varible failed \n", __func__);
+		return -ENOMEM;
+	}
+
+	/* Initial stats */
+	ip_vs_zero_stats(*p);
+
+	return 0;
+}
+
+void ip_vs_del_stats(struct ip_vs_stats* p)
+{
+	if(NULL == p)
+		return;
+
+	free_percpu(p);
+
+	return;
+}
+
+void ip_vs_zero_stats(struct ip_vs_stats* stats)
+{
+	int i = 0;
+
+	if(NULL == stats) {
+		pr_err("%s: Invaild point \n", __func__);
+		return;
+	}
+
+	for_each_online_cpu(i) {
+		ip_vs_stats_cpu(stats, i).conns    = 0;
+		ip_vs_stats_cpu(stats, i).inpkts   = 0;
+		ip_vs_stats_cpu(stats, i).outpkts  = 0;
+		ip_vs_stats_cpu(stats, i).inbytes  = 0;
+		ip_vs_stats_cpu(stats, i).outbytes = 0;
+	}
+
+	return;
+}
+
+void ip_vs_in_stats(struct ip_vs_conn *cp, struct sk_buff *skb)
+{
+	struct ip_vs_dest *dest = cp->dest;
+	if (dest && (dest->flags & IP_VS_DEST_F_AVAILABLE)) {
+		ip_vs_stats_this_cpu(dest->stats).inpkts++;
+		ip_vs_stats_this_cpu(dest->stats).inbytes += skb->len;
+
+		ip_vs_stats_this_cpu(dest->svc->stats).inpkts++;
+		ip_vs_stats_this_cpu(dest->svc->stats).inbytes += skb->len;
+
+		ip_vs_stats_this_cpu(ip_vs_stats).inpkts++;
+		ip_vs_stats_this_cpu(ip_vs_stats).inbytes += skb->len;
+	}
+
+	return;
+}
+
+void ip_vs_out_stats(struct ip_vs_conn *cp, struct sk_buff *skb)
+{
+	struct ip_vs_dest *dest = cp->dest;
+	if (dest && (dest->flags & IP_VS_DEST_F_AVAILABLE)) {
+		ip_vs_stats_this_cpu(dest->stats).outpkts++;
+		ip_vs_stats_this_cpu(dest->stats).outbytes += skb->len;
+
+		ip_vs_stats_this_cpu(dest->svc->stats).outpkts++;
+		ip_vs_stats_this_cpu(dest->svc->stats).outbytes += skb->len;
+
+		ip_vs_stats_this_cpu(ip_vs_stats).outpkts++;
+		ip_vs_stats_this_cpu(ip_vs_stats).outbytes += skb->len;
+	}
+	return;
+}
+
+void ip_vs_conn_stats(struct ip_vs_conn *cp, struct ip_vs_service *svc)
+{
+	struct ip_vs_dest *dest = cp->dest;
+	if(dest) {
+		ip_vs_stats_this_cpu(dest->stats).conns++;
+
+		ip_vs_stats_this_cpu(dest->svc->stats).conns++;
+
+		ip_vs_stats_this_cpu(ip_vs_stats).conns++;
+	}
+
+	return;
+}
+
diff --git a/net/netfilter/ipvs/ip_vs_sync.c b/net/netfilter/ipvs/ip_vs_sync.c
index e177f0d..3fa40b5 100644
--- a/net/netfilter/ipvs/ip_vs_sync.c
+++ b/net/netfilter/ipvs/ip_vs_sync.c
@@ -303,6 +303,7 @@ static void ip_vs_process_message(const char *buffer, const size_t buflen)
 	struct ip_vs_dest *dest;
 	char *p;
 	int i;
+	int res_dir;
 
 	if (buflen < sizeof(struct ip_vs_sync_mesg)) {
 		IP_VS_ERR_RL("sync message header too short\n");
@@ -371,11 +372,11 @@ static void ip_vs_process_message(const char *buffer, const size_t buflen)
 		}
 
 		if (!(flags & IP_VS_CONN_F_TEMPLATE))
-			cp = ip_vs_conn_in_get(AF_INET, s->protocol,
-					       (union nf_inet_addr *)&s->caddr,
-					       s->cport,
-					       (union nf_inet_addr *)&s->vaddr,
-					       s->vport);
+			cp = ip_vs_conn_get(AF_INET, s->protocol,
+					    (union nf_inet_addr *)&s->caddr,
+					    s->cport,
+					    (union nf_inet_addr *)&s->vaddr,
+					    s->vport, &res_dir);
 		else
 			cp = ip_vs_ct_in_get(AF_INET, s->protocol,
 					     (union nf_inet_addr *)&s->caddr,
@@ -407,8 +408,7 @@ static void ip_vs_process_message(const char *buffer, const size_t buflen)
 					    (union nf_inet_addr *)&s->vaddr,
 					    s->vport,
 					    (union nf_inet_addr *)&s->daddr,
-					    s->dport,
-					    flags, dest);
+					    s->dport, flags, dest, NULL, 0);
 			if (dest)
 				atomic_dec(&dest->refcnt);
 			if (!cp) {
diff --git a/net/netfilter/ipvs/ip_vs_synproxy.c b/net/netfilter/ipvs/ip_vs_synproxy.c
new file mode 100644
index 0000000..83b1d50
--- /dev/null
+++ b/net/netfilter/ipvs/ip_vs_synproxy.c
@@ -0,0 +1,1138 @@
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/tcp.h>
+#include <linux/if_arp.h>
+
+#include <net/ip.h>
+#include <net/tcp.h>
+#include <net/udp.h>
+#include <net/icmp.h>		/* for icmp_send */
+#include <net/route.h>
+
+#include <linux/netfilter.h>
+#include <linux/netfilter_ipv4.h>
+
+#ifdef CONFIG_IP_VS_IPV6
+#include <net/ipv6.h>
+#include <linux/netfilter_ipv6.h>
+#endif
+
+#include <net/ip_vs.h>
+#include <net/ip_vs_synproxy.h>
+
+static inline void
+syn_proxy_seq_csum_update(struct tcphdr *tcph, __u32 old_seq, __u32 new_seq)
+{
+	tcph->check = csum_fold(ip_vs_check_diff4(old_seq, new_seq,
+						  ~csum_unfold(tcph->check)));
+}
+
+/*
+ * Replace tcp options in tcp header, called by syn_proxy_reuse_skb()
+ *
+ */
+static void
+syn_proxy_parse_set_opts(struct sk_buff *skb, struct tcphdr *th,
+			 struct ip_vs_synproxy_opt *opt)
+{
+	/* mss in received packet */
+	__u16 in_mss;
+	__u32 *tmp;
+	unsigned char *ptr;
+	int length = (th->doff * 4) - sizeof(struct tcphdr);
+	/*tcp_sk(sk)->user_mss. set from proc */
+	__u16 user_mss = sysctl_ip_vs_synproxy_init_mss;
+
+	memset(opt, '\0', sizeof(struct ip_vs_synproxy_opt));
+	opt->mss_clamp = 536;
+	ptr = (unsigned char *)(th + 1);
+
+	while (length > 0) {
+		unsigned char *tmp_opcode = ptr;
+		int opcode = *ptr++;
+		int opsize;
+
+		switch (opcode) {
+		case TCPOPT_EOL:
+			return;
+		case TCPOPT_NOP:
+			length--;
+			continue;
+		default:
+			opsize = *ptr++;
+			if (opsize < 2)	/* "silly options" */
+				return;
+			if (opsize > length)
+				return;	/* don't parse partial options */
+			switch (opcode) {
+			case TCPOPT_MSS:
+				if (opsize == TCPOLEN_MSS) {
+					in_mss = ntohs(*(__u16 *) ptr);
+					if (in_mss) {
+						if (user_mss < in_mss) {
+							in_mss = user_mss;
+						}
+						opt->mss_clamp = in_mss;
+					}
+					*(__u16 *) ptr = htons(opt->mss_clamp);
+				}
+				break;
+			case TCPOPT_WINDOW:
+				if (opsize == TCPOLEN_WINDOW) {
+					if (sysctl_ip_vs_synproxy_wscale) {
+						opt->wscale_ok = 1;
+						opt->snd_wscale = *(__u8 *) ptr;
+						if (opt->snd_wscale >
+						    IP_VS_SYNPROXY_WSCALE_MAX) {
+							IP_VS_DBG(6,
+								  "tcp_parse_options: Illegal window "
+								  "scaling value %d > %d received.",
+								  opt->
+								  snd_wscale,
+								  IP_VS_SYNPROXY_WSCALE_MAX);
+							opt->snd_wscale =
+							    IP_VS_SYNPROXY_WSCALE_MAX;
+						}
+						*(__u8 *) ptr = (__u8)
+						    sysctl_ip_vs_synproxy_wscale;
+					} else {
+						memset(tmp_opcode, TCPOPT_NOP,
+						       TCPOLEN_WINDOW);
+					}
+				}
+				break;
+			case TCPOPT_TIMESTAMP:
+				if (opsize == TCPOLEN_TIMESTAMP) {
+					if (sysctl_ip_vs_synproxy_timestamp) {
+						opt->tstamp_ok = 1;
+						tmp = (__u32 *) ptr;
+						*(tmp + 1) = *tmp;
+						*tmp = htonl(tcp_time_stamp);
+					} else {
+						memset(tmp_opcode, TCPOPT_NOP,
+						       TCPOLEN_TIMESTAMP);
+					}
+				}
+				break;
+			case TCPOPT_SACK_PERM:
+				if (opsize == TCPOLEN_SACK_PERM) {
+					if (sysctl_ip_vs_synproxy_sack) {
+						opt->sack_ok = 1;
+					} else {
+						memset(tmp_opcode, TCPOPT_NOP,
+						       TCPOLEN_SACK_PERM);
+					}
+				}
+				break;
+			}
+			ptr += opsize - 2;
+			length -= opsize;
+		}
+	}
+}
+
+/*
+ * Reuse skb for syn proxy, called by syn_proxy_syn_rcv().
+ * do following things:
+ * 1) set tcp options;
+ * 2) compute seq with cookie func.
+ * 3) set tcp seq and ack_seq;
+ * 4) exchange ip addr and tcp port;
+ * 5) compute iphdr and tcp check.
+ *
+ */
+static void
+syn_proxy_reuse_skb(int af, struct sk_buff *skb, struct ip_vs_synproxy_opt *opt)
+{
+	__u32 isn;
+	unsigned short tmpport;
+	unsigned int tcphoff;
+	struct tcphdr *th;
+	af &= ~IP_VS_CONN_F_DSNAT;
+
+#ifdef CONFIG_IP_VS_IPV6
+	if (af == AF_INET6)
+		tcphoff = sizeof(struct ipv6hdr);
+	else
+#endif
+		tcphoff = ip_hdrlen(skb);
+
+	th = (void *)skb_network_header(skb) + tcphoff;
+
+	/* deal with tcp options */
+	syn_proxy_parse_set_opts(skb, th, opt);
+
+	/* get cookie */
+	skb_set_transport_header(skb, tcphoff);
+#ifdef CONFIG_IP_VS_IPV6
+	if (af == AF_INET6)
+		isn = ip_vs_synproxy_cookie_v6_init_sequence(skb, opt);
+	else
+#endif
+		isn = ip_vs_synproxy_cookie_v4_init_sequence(skb, opt);
+
+	/* Set syn-ack flag
+	 * the tcp opt in syn/ack packet : 00010010 = 0x12
+	 */
+	((u_int8_t *) th)[13] = 0x12;
+
+	/* Exchange ports */
+	tmpport = th->dest;
+	th->dest = th->source;
+	th->source = tmpport;
+
+	/* Set seq(cookie) and ack_seq */
+	th->ack_seq = htonl(ntohl(th->seq) + 1);
+	th->seq = htonl(isn);
+
+	/* Exchange addresses and compute checksums */
+#ifdef CONFIG_IP_VS_IPV6
+	if (af == AF_INET6) {
+		struct ipv6hdr *iph = ipv6_hdr(skb);
+		struct in6_addr tmpAddr;
+
+		memcpy(&tmpAddr, &iph->saddr, sizeof(struct in6_addr));
+		memcpy(&iph->saddr, &iph->daddr, sizeof(struct in6_addr));
+		memcpy(&iph->daddr, &tmpAddr, sizeof(struct in6_addr));
+
+		iph->hop_limit = sysctl_ip_vs_synproxy_synack_ttl;
+
+		th->check = 0;
+		skb->csum = skb_checksum(skb, tcphoff, skb->len - tcphoff, 0);
+		th->check = csum_ipv6_magic(&iph->saddr, &iph->daddr,
+					    skb->len - tcphoff,
+					    IPPROTO_TCP, skb->csum);
+	} else
+#endif
+	{
+		struct iphdr *iph = ip_hdr(skb);
+		__be32 tmpAddr;
+
+		tmpAddr = iph->saddr;
+		iph->saddr = iph->daddr;
+		iph->daddr = tmpAddr;
+
+		iph->ttl = sysctl_ip_vs_synproxy_synack_ttl;
+		iph->tos = 0;
+
+		ip_send_check(iph);
+
+		th->check = 0;
+		skb->csum = skb_checksum(skb, tcphoff, skb->len - tcphoff, 0);
+		th->check = csum_tcpudp_magic(iph->saddr, iph->daddr,
+					      skb->len - tcphoff,
+					      IPPROTO_TCP, skb->csum);
+	}
+}
+
+/*
+ *  syn-proxy step 1 logic:
+ *  Check if synproxy is enabled for this skb, and
+ *  send Syn/Ack back.
+ *
+ *  Synproxy is enabled when:
+ *  1) skb is a Syn packet.
+ *  2) And the service is synproxy-enable.
+ *  3) And ip_vs_todrop return false.
+ *
+ *  @return 0 means the caller should return at once and use
+ *   verdict as return value, return 1 for nothing.
+ */
+int
+ip_vs_synproxy_syn_rcv(int af, struct sk_buff *skb,
+		       struct ip_vs_iphdr *iph, int *verdict)
+{
+	struct ip_vs_service *svc = NULL;
+	struct tcphdr _tcph, *th;
+	struct ip_vs_synproxy_opt tcp_opt;
+
+	th = skb_header_pointer(skb, iph->len, sizeof(_tcph), &_tcph);
+	if (unlikely(th == NULL)) {
+		goto syn_rcv_out;
+	}
+
+	if (th->syn && !th->ack && !th->rst && !th->fin &&
+	    (svc =
+	     ip_vs_service_get(af, skb->mark, iph->protocol, &iph->daddr,
+			       th->dest))
+	    && (svc->flags & IP_VS_CONN_F_SYNPROXY)) {
+		// release service here, because don't use it any all.
+		ip_vs_service_put(svc);
+
+		if (ip_vs_todrop()) {
+			/*
+			 * It seems that we are very loaded.
+			 * We have to drop this packet :(
+			 */
+			goto syn_rcv_out;
+		}
+	} else {
+		/*
+		 * release service.
+		 */
+		if (svc != NULL) {
+			ip_vs_service_put(svc);
+		}
+		return 1;
+	}
+
+	/* update statistics */
+	IP_VS_INC_ESTATS(ip_vs_esmib, SYNPROXY_SYN_CNT);
+
+	/* Try to reuse skb if possible */
+	if (unlikely(skb_shared(skb) || skb_cloned(skb))) {
+		struct sk_buff *new_skb = skb_copy(skb, GFP_ATOMIC);
+		if (unlikely(new_skb == NULL)) {
+			goto syn_rcv_out;
+		}
+		/* Drop old skb */
+		kfree_skb(skb);
+		skb = new_skb;
+	}
+
+	/* reuse skb here: deal with tcp options, exchage ip, port. */
+	syn_proxy_reuse_skb(af, skb, &tcp_opt);
+
+	if (unlikely(skb->dev == NULL)) {
+		IP_VS_ERR_RL("%s: skb->dev is null !!!\n", __func__);
+		goto syn_rcv_out;
+	}
+
+	/* Send the packet out */
+	if (likely(skb->dev->type == ARPHRD_ETHER)) {
+		unsigned char t_hwaddr[ETH_ALEN];
+
+		/* Move the data pointer to point to the link layer header */
+		struct ethhdr *eth = (struct ethhdr *)skb_mac_header(skb);
+		skb->data = (unsigned char *)skb_mac_header(skb);
+		skb->len += ETH_HLEN;	//sizeof(skb->mac.ethernet);
+
+		memcpy(t_hwaddr, (eth->h_dest), ETH_ALEN);
+		memcpy((eth->h_dest), (eth->h_source), ETH_ALEN);
+		memcpy((eth->h_source), t_hwaddr, ETH_ALEN);
+		skb->pkt_type = PACKET_OUTGOING;
+	} else if (skb->dev->type == ARPHRD_LOOPBACK) {
+		/* set link layer */
+		if (likely(skb_mac_header_was_set(skb))) {
+			skb->data = skb_mac_header(skb);
+			skb->len += sizeof(struct ethhdr);
+		} else {
+			skb_push(skb, sizeof(struct ethhdr));
+			skb_reset_mac_header(skb);
+		}
+	}
+
+	dev_queue_xmit(skb);
+	*verdict = NF_STOLEN;
+	return 0;
+syn_rcv_out:
+	/* Drop the packet when all things are right also,
+	 * then we needn't to kfree_skb() */
+	*verdict = NF_DROP;
+	return 0;
+}
+
+/*
+ * Check if skb has user data.
+ * Attention: decrease iph len also.
+ */
+static inline int
+syn_proxy_ack_has_data(struct sk_buff *skb, struct ip_vs_iphdr *iph,
+		       struct tcphdr *th)
+{
+	IP_VS_DBG(6, "tot_len = %u, iph_len = %u, tcph_len = %u\n",
+		  skb->len, iph->len, th->doff * 4);
+	return (skb->len - iph->len - th->doff * 4) != 0;
+}
+
+static inline void
+syn_proxy_syn_build_options(__be32 * ptr, struct ip_vs_synproxy_opt *opt)
+{
+	*ptr++ =
+	    htonl((TCPOPT_MSS << 24) | (TCPOLEN_MSS << 16) | opt->mss_clamp);
+	if (opt->tstamp_ok) {
+		if (opt->sack_ok)
+			*ptr++ = htonl((TCPOPT_SACK_PERM << 24) |
+				       (TCPOLEN_SACK_PERM << 16) |
+				       (TCPOPT_TIMESTAMP << 8) |
+				       TCPOLEN_TIMESTAMP);
+		else
+			*ptr++ = htonl((TCPOPT_NOP << 24) |
+				       (TCPOPT_NOP << 16) |
+				       (TCPOPT_TIMESTAMP << 8) |
+				       TCPOLEN_TIMESTAMP);
+		*ptr++ = htonl(tcp_time_stamp);	/* TSVAL */
+		*ptr++ = 0;	/* TSECR */
+	} else if (opt->sack_ok)
+		*ptr++ = htonl((TCPOPT_NOP << 24) |
+			       (TCPOPT_NOP << 16) |
+			       (TCPOPT_SACK_PERM << 8) | TCPOLEN_SACK_PERM);
+	if (opt->wscale_ok)
+		*ptr++ = htonl((TCPOPT_NOP << 24) |
+			       (TCPOPT_WINDOW << 16) |
+			       (TCPOLEN_WINDOW << 8) | (opt->snd_wscale));
+}
+
+/*
+ * Create syn packet and send it to rs.
+ * ATTENTION: we also store syn skb in cp if syn retransimition
+ * is tured on.
+ */
+static int
+syn_proxy_send_rs_syn(int af, const struct tcphdr *th,
+		      struct ip_vs_conn *cp, struct sk_buff *skb,
+		      struct ip_vs_protocol *pp, struct ip_vs_synproxy_opt *opt)
+{
+	struct sk_buff *syn_skb;
+	int tcp_hdr_size;
+	__u8 tcp_flags = TCPCB_FLAG_SYN;
+	unsigned int tcphoff;
+	struct tcphdr *new_th;
+
+	if (!cp->packet_xmit) {
+		IP_VS_ERR_RL("warning: packet_xmit is null");
+		return 0;
+	}
+
+	syn_skb = alloc_skb(MAX_TCP_HEADER + 15, GFP_ATOMIC);
+	if (unlikely(syn_skb == NULL)) {
+		IP_VS_ERR_RL("alloc skb failed when send rs syn packet\n");
+		return 0;
+	}
+
+	/* Reserve space for headers */
+	skb_reserve(syn_skb, MAX_TCP_HEADER);
+	tcp_hdr_size = (sizeof(struct tcphdr) + TCPOLEN_MSS +
+			(opt->tstamp_ok ? TCPOLEN_TSTAMP_ALIGNED : 0) +
+			(opt->wscale_ok ? TCPOLEN_WSCALE_ALIGNED : 0) +
+			/* SACK_PERM is in the place of NOP NOP of TS */
+			((opt->sack_ok
+			  && !opt->tstamp_ok) ? TCPOLEN_SACKPERM_ALIGNED : 0));
+
+	new_th = (struct tcphdr *)skb_push(syn_skb, tcp_hdr_size);
+	/* Compose tcp header */
+	skb_reset_transport_header(syn_skb);
+	syn_skb->csum = 0;
+
+	/* Set tcp hdr */
+	new_th->source = th->source;
+	new_th->dest = th->dest;
+	new_th->seq = htonl(ntohl(th->seq) - 1);
+	new_th->ack_seq = 0;
+	*(((__u16 *) new_th) + 6) =
+	    htons(((tcp_hdr_size >> 2) << 12) | tcp_flags);
+	/* FIX_ME: what window should we use */
+	new_th->window = htons(5000);
+	new_th->check = 0;
+	new_th->urg_ptr = 0;
+	new_th->urg = 0;
+	new_th->ece = 0;
+	new_th->cwr = 0;
+
+	syn_proxy_syn_build_options((__be32 *) (new_th + 1), opt);
+
+	/*
+	 * Set ip hdr
+	 * Attention: set source and dest addr to ack skb's.
+	 * we rely on packet_xmit func to do NATs thing.
+	 */
+#ifdef CONFIG_IP_VS_IPV6
+	if (af == AF_INET6) {
+		struct ipv6hdr *ack_iph = ipv6_hdr(skb);
+		struct ipv6hdr *iph =
+		    (struct ipv6hdr *)skb_push(syn_skb, sizeof(struct ipv6hdr));
+
+		tcphoff = sizeof(struct ipv6hdr);
+		skb_reset_network_header(syn_skb);
+		memcpy(&iph->saddr, &ack_iph->saddr, sizeof(struct in6_addr));
+		memcpy(&iph->daddr, &ack_iph->daddr, sizeof(struct in6_addr));
+
+		iph->version = 6;
+		iph->nexthdr = NEXTHDR_TCP;
+		iph->payload_len = htons(tcp_hdr_size);
+		iph->hop_limit = IPV6_DEFAULT_HOPLIMIT;
+
+		new_th->check = 0;
+		syn_skb->csum =
+		    skb_checksum(syn_skb, tcphoff, syn_skb->len - tcphoff, 0);
+		new_th->check =
+		    csum_ipv6_magic(&iph->saddr, &iph->daddr,
+				    syn_skb->len - tcphoff, IPPROTO_TCP,
+				    syn_skb->csum);
+	} else
+#endif
+	{
+		struct iphdr *ack_iph = ip_hdr(skb);
+		u32 rtos = RT_TOS(ack_iph->tos);
+		struct iphdr *iph =
+		    (struct iphdr *)skb_push(syn_skb, sizeof(struct iphdr));
+
+		tcphoff = sizeof(struct iphdr);
+		skb_reset_network_header(syn_skb);
+		*((__u16 *) iph) = htons((4 << 12) | (5 << 8) | (rtos & 0xff));
+		iph->tot_len = htons(syn_skb->len);
+		iph->frag_off = htons(IP_DF);
+		/* FIX_ME: what ttl shoule we use */
+		iph->ttl = IPDEFTTL;
+		iph->protocol = IPPROTO_TCP;
+		iph->saddr = ack_iph->saddr;
+		iph->daddr = ack_iph->daddr;
+
+		ip_send_check(iph);
+
+		new_th->check = 0;
+		syn_skb->csum =
+		    skb_checksum(syn_skb, tcphoff, syn_skb->len - tcphoff, 0);
+		new_th->check =
+		    csum_tcpudp_magic(iph->saddr, iph->daddr,
+				      syn_skb->len - tcphoff, IPPROTO_TCP,
+				      syn_skb->csum);
+	}
+
+	/* Save syn_skb if syn retransmission is on  */
+	if (sysctl_ip_vs_synproxy_syn_retry > 0) {
+		cp->syn_skb = skb_copy(syn_skb, GFP_ATOMIC);
+		atomic_set(&cp->syn_retry_max, sysctl_ip_vs_synproxy_syn_retry);
+	}
+
+	/* Save info for fast_response_xmit */
+	if(sysctl_ip_vs_fast_xmit && skb->dev &&
+				likely(skb->dev->type == ARPHRD_ETHER) &&
+				skb_mac_header_was_set(skb)) {
+		struct ethhdr *eth = (struct ethhdr *)skb_mac_header(skb);
+
+		if(likely(cp->indev == NULL)) {
+			cp->indev = skb->dev;
+			dev_hold(cp->indev);
+		}
+
+		if (unlikely(cp->indev != skb->dev)) {
+			dev_put(cp->indev);
+			cp->indev = skb->dev;
+			dev_hold(cp->indev);
+		}
+
+		memcpy(cp->src_hwaddr, eth->h_source, ETH_ALEN);
+		memcpy(cp->dst_hwaddr, eth->h_dest, ETH_ALEN);
+		IP_VS_INC_ESTATS(ip_vs_esmib, FAST_XMIT_SYNPROXY_SAVE);
+		IP_VS_DBG_RL("syn_proxy_send_rs_syn netdevice:%s\n",
+						netdev_name(skb->dev));
+	}
+
+	/* count in the syn packet */
+	ip_vs_in_stats(cp, skb);
+
+	/* If xmit failed, syn_skb will be freed correctly. */
+	cp->packet_xmit(syn_skb, cp, pp);
+
+	return 1;
+}
+
+/*
+ * Syn-proxy step 2 logic
+ * Receive client's 3-handshakes  Ack packet, do cookie check
+ * and then send syn to rs after creating a session.
+ *
+ */
+int
+ip_vs_synproxy_ack_rcv(int af, struct sk_buff *skb, struct tcphdr *th,
+		       struct ip_vs_protocol *pp, struct ip_vs_conn **cpp,
+		       struct ip_vs_iphdr *iph, int *verdict)
+{
+	struct ip_vs_synproxy_opt opt;
+	struct ip_vs_service *svc;
+	int res_cookie_check;
+	int dsnat;
+	dsnat = af & IP_VS_CONN_F_DSNAT;
+	af &= ~IP_VS_CONN_F_DSNAT;
+
+	/*
+	 * Don't check svc syn-proxy flag, as it may
+	 * be changed after syn-proxy step 1.
+	 */
+	if (!th->syn && th->ack && !th->rst && !th->fin &&
+	    (svc =
+	     ip_vs_service_get(af|dsnat, skb->mark, iph->protocol, &iph->daddr,
+			       th->dest))) {
+		if (ip_vs_todrop()) {
+			/*
+			 * It seems that we are very loaded.
+			 * We have to drop this packet :(
+			 */
+			ip_vs_service_put(svc);
+			*verdict = NF_DROP;
+			return 0;
+		}
+
+		if (sysctl_ip_vs_synproxy_defer &&
+		    !syn_proxy_ack_has_data(skb, iph, th)) {
+			/* update statistics */
+			IP_VS_INC_ESTATS(ip_vs_esmib, SYNPROXY_NULL_ACK);
+			/*
+			 * When expecting ack packet with payload,
+			 * we get a pure ack, so have to drop it.
+			 */
+			ip_vs_service_put(svc);
+			*verdict = NF_DROP;
+			return 0;
+		}
+
+		/*
+		 * Import: set tcp hdr before cookie check, as it
+		 * will be used in cookie_check funcs.
+		 */
+		skb_set_transport_header(skb, iph->len);
+#ifdef CONFIG_IP_VS_IPV6
+		if (af == AF_INET6) {
+			res_cookie_check = ip_vs_synproxy_v6_cookie_check(skb,
+									  ntohl
+									  (th->
+									   ack_seq)
+									  - 1,
+									  &opt);
+		} else
+#endif
+		{
+			res_cookie_check = ip_vs_synproxy_v4_cookie_check(skb,
+									  ntohl
+									  (th->
+									   ack_seq)
+									  - 1,
+									  &opt);
+		}
+
+		if (!res_cookie_check) {
+			/* update statistics */
+			IP_VS_INC_ESTATS(ip_vs_esmib, SYNPROXY_BAD_ACK);
+			/*
+			 * Cookie check fail, drop it.
+			 */
+			IP_VS_DBG(6, "syn_cookie check failed seq=%u\n",
+				  ntohl(th->ack_seq) - 1);
+			ip_vs_service_put(svc);
+			*verdict = NF_DROP;
+			return 0;
+		}
+
+		/* update statistics */
+		IP_VS_INC_ESTATS(ip_vs_esmib, SYNPROXY_OK_ACK);
+
+		/*
+		 * Let the virtual server select a real server for the
+		 * incoming connection, and create a connection entry.
+		 */
+		*cpp = ip_vs_schedule(svc, skb, 1);
+		if (!*cpp) {
+			IP_VS_DBG(6, "ip_vs_schedule failed\n");
+			*verdict = ip_vs_leave(svc, skb, pp);
+			return 0;
+		}
+
+		/*
+		 * Release service, we don't need it any more.
+		 */
+		ip_vs_service_put(svc);
+
+		/*
+		 * Do anything but print a error msg when fail.
+		 * Because session will be correctly freed in ip_vs_conn_expire.
+		 */
+		if (!syn_proxy_send_rs_syn(af, th, *cpp, skb, pp, &opt)) {
+			IP_VS_ERR_RL("syn_proxy_send_rs_syn failed!\n");
+		}
+
+		/* count in the ack packet (STOLEN by synproxy) */
+		ip_vs_in_stats(*cpp, skb);
+
+		/*
+		 * Active sesion timer, and dec refcnt.
+		 * Also stole the skb, and let caller return immediately.
+		 */
+		ip_vs_conn_put(*cpp);
+		*verdict = NF_STOLEN;
+		return 0;
+	}
+
+	return 1;
+}
+
+/*
+ * Update out-in sack seqs, and also correct th->check
+ */
+static inline void
+syn_proxy_filter_opt_outin(struct tcphdr *th, struct ip_vs_seq *sp_seq)
+{
+	unsigned char *ptr;
+	int length = (th->doff * 4) - sizeof(struct tcphdr);
+	__be32 *tmp;
+	__u32 old_ack_seq;
+
+	if (!length)
+		return;
+
+	ptr = (unsigned char *)(th + 1);
+
+	/* Fast path for timestamp-only option */
+	if (length == TCPOLEN_TSTAMP_ALIGNED
+	    && *(__be32 *) ptr == __constant_htonl((TCPOPT_NOP << 24)
+						   | (TCPOPT_NOP << 16)
+						   | (TCPOPT_TIMESTAMP << 8) |
+						   TCPOLEN_TIMESTAMP))
+		return;
+
+	while (length > 0) {
+		int opcode = *ptr++;
+		int opsize, i;
+
+		switch (opcode) {
+		case TCPOPT_EOL:
+			return;
+		case TCPOPT_NOP:	/* Ref: RFC 793 section 3.1 */
+			length--;
+			continue;
+		default:
+			opsize = *ptr++;
+			if (opsize < 2)	/* "silly options" */
+				return;
+			if (opsize > length)
+				break;	/* don't parse partial options */
+
+			if (opcode == TCPOPT_SACK
+			    && opsize >= (TCPOLEN_SACK_BASE
+					  + TCPOLEN_SACK_PERBLOCK)
+			    && !((opsize - TCPOLEN_SACK_BASE) %
+				 TCPOLEN_SACK_PERBLOCK)) {
+				for (i = 0; i < (opsize - TCPOLEN_SACK_BASE);
+				     i += TCPOLEN_SACK_PERBLOCK) {
+					tmp = (__be32 *) (ptr + i);
+					old_ack_seq = ntohl(*tmp);
+					*tmp = htonl((__u32)
+						     (old_ack_seq -
+						      sp_seq->delta));
+					syn_proxy_seq_csum_update(th,
+								  htonl
+								  (old_ack_seq),
+								  *tmp);
+					IP_VS_DBG(6,
+						  "syn_proxy_filter_opt_outin: sack_left_seq %u => %u, delta = %u \n",
+						  old_ack_seq, ntohl(*tmp),
+						  sp_seq->delta);
+					tmp++;
+					old_ack_seq = ntohl(*tmp);
+					*tmp = htonl((__u32)
+						     (old_ack_seq -
+						      sp_seq->delta));
+					syn_proxy_seq_csum_update(th,
+								  htonl
+								  (old_ack_seq),
+								  *tmp);
+					IP_VS_DBG(6,
+						  "syn_proxy_filter_opt_outin: sack_right_seq %u => %u, delta = %u \n",
+						  old_ack_seq, ntohl(*tmp),
+						  sp_seq->delta);
+				}
+				return;
+			}
+			ptr += opsize - 2;
+			length -= opsize;
+		}
+	}
+}
+
+/*
+ * Update out-in ack_seqs: include th->ack_seq, sack opt
+ * and also correct tcph->check.
+ */
+void ip_vs_synproxy_dnat_handler(struct tcphdr *tcph, struct ip_vs_seq *sp_seq)
+{
+	__u32 old_ack_seq;
+
+	if (sp_seq->delta != 0) {
+		old_ack_seq = ntohl(tcph->ack_seq);
+		tcph->ack_seq = htonl((__u32) (old_ack_seq - sp_seq->delta));
+		syn_proxy_seq_csum_update(tcph, htonl(old_ack_seq),
+					  tcph->ack_seq);
+		syn_proxy_filter_opt_outin(tcph, sp_seq);
+		IP_VS_DBG(6,
+			  "tcp_dnat_handler: tcph->ack_seq %u => %u, delta = %u \n",
+			  old_ack_seq, htonl(tcph->ack_seq), sp_seq->delta);
+	}
+}
+
+/*
+ * Syn-proxy step 3 logic: receive syn-ack from rs
+ * Update syn_proxy_seq.delta and send stored ack skbs
+ * to rs.
+ */
+int
+ip_vs_synproxy_synack_rcv(struct sk_buff *skb, struct ip_vs_conn *cp,
+			  struct ip_vs_protocol *pp, int ihl, int *verdict)
+{
+	struct tcphdr _tcph, *th;
+	struct sk_buff_head save_skb;
+	struct sk_buff *tmp_skb = NULL;
+	struct ip_vs_dest *dest = cp->dest;
+
+	th = skb_header_pointer(skb, ihl, sizeof(_tcph), &_tcph);
+	if (th == NULL) {
+		*verdict = NF_DROP;
+		return 0;
+	}
+
+	IP_VS_DBG(6, "in syn_proxy_synack_rcv, "
+		  "seq = %u ack_seq = %u %c%c%c cp->is_synproxy = %u cp->state = %u\n",
+		  ntohl(th->seq),
+		  ntohl(th->ack_seq),
+		  (th->syn) ? 'S' : '-',
+		  (th->ack) ? 'A' : '-',
+		  (th->rst) ? 'R' : '-',
+		  cp->flags & IP_VS_CONN_F_SYNPROXY, cp->state);
+
+	skb_queue_head_init(&save_skb);
+	spin_lock(&cp->lock);
+	if ((th->syn) && (th->ack) && (!th->rst) &&
+	    (cp->flags & IP_VS_CONN_F_SYNPROXY) &&
+	    cp->state == IP_VS_TCP_S_SYN_SENT) {
+		cp->syn_proxy_seq.delta =
+		    htonl(cp->syn_proxy_seq.init_seq) - htonl(th->seq);
+		cp->timeout = pp->timeout_table[cp->state =
+						IP_VS_TCP_S_ESTABLISHED];
+		if (dest) {
+			atomic_inc(&dest->activeconns);
+			atomic_dec(&dest->inactconns);
+			cp->flags &= ~IP_VS_CONN_F_INACTIVE;
+		}
+
+		/* save tcp sequense for fullnat/nat, INside to OUTside */
+		if (sysctl_ip_vs_conn_expire_tcp_rst == 1) {
+			cp->rs_end_seq = htonl(ntohl(th->seq) + 1);
+			cp->rs_ack_seq = th->ack_seq;
+			IP_VS_DBG_RL("packet from RS, seq:%u ack_seq:%u.",
+				     ntohl(th->seq), ntohl(th->ack_seq));
+			IP_VS_DBG_RL("port:%u->%u", ntohs(th->source),
+				     ntohs(th->dest));
+		}
+
+		/* First: free stored syn skb */
+		if ((tmp_skb = xchg(&cp->syn_skb, NULL)) != NULL) {
+			kfree_skb(tmp_skb);
+			tmp_skb = NULL;
+		}
+
+		if (skb_queue_len(&cp->ack_skb) <= 0) {
+			/*
+			 * FIXME: maybe a bug here, print err msg and go.
+			 * Attention: cp->state has been changed and we
+			 * should still DROP the Syn/Ack skb.
+			 */
+			IP_VS_ERR_RL
+			    ("Got ack_skb NULL pointer in syn_proxy_synack_rcv\n");
+			spin_unlock(&cp->lock);
+			*verdict = NF_DROP;
+			return 0;
+		}
+
+		while ((tmp_skb = skb_dequeue(&cp->ack_skb)) != NULL) {
+			skb_queue_tail(&save_skb, tmp_skb);
+		}
+
+		/*
+		 * Release the lock, because we don't
+		 * touch session any more.
+		 */
+		spin_unlock(&cp->lock);
+
+		while ((tmp_skb = skb_dequeue(&save_skb)) != NULL) {
+			/* If xmit failed, syn_skb will be freed correctly. */
+			cp->packet_xmit(tmp_skb, cp, pp);
+		}
+
+		*verdict = NF_DROP;
+		return 0;
+	} else if ((th->rst) &&
+		   (cp->flags & IP_VS_CONN_F_SYNPROXY) &&
+		   cp->state == IP_VS_TCP_S_SYN_SENT) {
+		__u32 temp_seq;
+		temp_seq = ntohl(th->seq);
+		IP_VS_DBG(6, "get rst from rs, seq = %u ack_seq= %u\n",
+			  ntohl(th->seq), ntohl(th->ack_seq));
+		/* coute the delta of seq */
+		cp->syn_proxy_seq.delta =
+		    ntohl(cp->syn_proxy_seq.init_seq) - ntohl(th->seq);
+		cp->timeout = pp->timeout_table[cp->state = IP_VS_TCP_S_CLOSE];
+		spin_unlock(&cp->lock);
+		th->seq = htonl(ntohl(th->seq) + 1);
+		syn_proxy_seq_csum_update(th, htonl(temp_seq), th->seq);
+
+		return 1;
+	}
+	spin_unlock(&cp->lock);
+
+	return 1;
+}
+
+static inline void
+__syn_proxy_reuse_conn(struct ip_vs_conn *cp,
+		       struct sk_buff *ack_skb,
+		       struct tcphdr *th, struct ip_vs_protocol *pp)
+{
+	struct sk_buff *tmp_skb = NULL;
+
+	/* Free stored ack packet */
+	while ((tmp_skb = skb_dequeue(&cp->ack_skb)) != NULL) {
+		kfree_skb(tmp_skb);
+		tmp_skb = NULL;
+	}
+
+	/* Free stored syn skb */
+	if ((tmp_skb = xchg(&cp->syn_skb, NULL)) != NULL) {
+		kfree_skb(tmp_skb);
+		tmp_skb = NULL;
+	}
+
+	/* Store new ack_skb */
+	skb_queue_head_init(&cp->ack_skb);
+	skb_queue_tail(&cp->ack_skb, ack_skb);
+
+	/* Save ack_seq - 1 */
+	cp->syn_proxy_seq.init_seq = htonl((__u32) ((htonl(th->ack_seq) - 1)));
+	/* don't change delta here, so original flow can still be valid */
+
+	/* Save ack_seq */
+	cp->fnat_seq.fdata_seq = ntohl(th->ack_seq);
+
+	cp->fnat_seq.init_seq = 0;
+
+	/* Clean dup ack cnt */
+	atomic_set(&cp->dup_ack_cnt, 0);
+
+	/* Set timeout value */
+	cp->timeout = pp->timeout_table[cp->state = IP_VS_TCP_S_SYN_SENT];
+}
+
+/*
+ * Syn-proxy session reuse function.
+ * Update syn_proxy_seq struct and clean syn-proxy related
+ * members.
+ */
+int
+ip_vs_synproxy_reuse_conn(int af, struct sk_buff *skb,
+			  struct ip_vs_conn *cp,
+			  struct ip_vs_protocol *pp,
+			  struct ip_vs_iphdr *iph, int *verdict)
+{
+	struct tcphdr _tcph, *th = NULL;
+	struct ip_vs_synproxy_opt opt;
+	int res_cookie_check;
+	u32 tcp_conn_reuse_states = 0;
+	af &= ~IP_VS_CONN_F_DSNAT;
+
+	th = skb_header_pointer(skb, iph->len, sizeof(_tcph), &_tcph);
+	if (unlikely(NULL == th)) {
+		IP_VS_ERR_RL("skb has a invalid tcp header\n");
+		*verdict = NF_DROP;
+		return 0;
+	}
+
+	tcp_conn_reuse_states =
+	    ((sysctl_ip_vs_synproxy_conn_reuse_cl << IP_VS_TCP_S_CLOSE) |
+	     (sysctl_ip_vs_synproxy_conn_reuse_tw << IP_VS_TCP_S_TIME_WAIT) |
+	     (sysctl_ip_vs_synproxy_conn_reuse_fw << IP_VS_TCP_S_FIN_WAIT) |
+	     (sysctl_ip_vs_synproxy_conn_reuse_cw << IP_VS_TCP_S_CLOSE_WAIT) |
+	     (sysctl_ip_vs_synproxy_conn_reuse_la << IP_VS_TCP_S_LAST_ACK));
+
+	if (((1 << (cp->state)) & tcp_conn_reuse_states) &&
+	    (cp->flags & IP_VS_CONN_F_SYNPROXY) &&
+	    (!th->syn && th->ack && !th->rst && !th->fin) &&
+	    (cp->syn_proxy_seq.init_seq !=
+	     htonl((__u32) ((ntohl(th->ack_seq) - 1))))) {
+		/*
+		 * Import: set tcp hdr before cookie check, as it
+		 * will be used in cookie_check funcs.
+		 */
+		skb_set_transport_header(skb, iph->len);
+#ifdef CONFIG_IP_VS_IPV6
+		if (af == AF_INET6) {
+			res_cookie_check = ip_vs_synproxy_v6_cookie_check(skb,
+									  ntohl
+									  (th->
+									   ack_seq)
+									  - 1,
+									  &opt);
+		} else
+#endif
+		{
+			res_cookie_check = ip_vs_synproxy_v4_cookie_check(skb,
+									  ntohl
+									  (th->
+									   ack_seq)
+									  - 1,
+									  &opt);
+		}
+
+		if (!res_cookie_check) {
+			/* update statistics */
+			IP_VS_INC_ESTATS(ip_vs_esmib, SYNPROXY_BAD_ACK);
+			/*
+			 * Cookie check fail, let it go.
+			 */
+			return 1;
+		}
+
+		/* update statistics */
+		IP_VS_INC_ESTATS(ip_vs_esmib, SYNPROXY_OK_ACK);
+		IP_VS_INC_ESTATS(ip_vs_esmib, SYNPROXY_CONN_REUSED);
+		switch (cp->old_state) {
+		case IP_VS_TCP_S_CLOSE:
+			IP_VS_INC_ESTATS(ip_vs_esmib,
+					 SYNPROXY_CONN_REUSED_CLOSE);
+			break;
+		case IP_VS_TCP_S_TIME_WAIT:
+			IP_VS_INC_ESTATS(ip_vs_esmib,
+					 SYNPROXY_CONN_REUSED_TIMEWAIT);
+			break;
+		case IP_VS_TCP_S_FIN_WAIT:
+			IP_VS_INC_ESTATS(ip_vs_esmib,
+					 SYNPROXY_CONN_REUSED_FINWAIT);
+			break;
+		case IP_VS_TCP_S_CLOSE_WAIT:
+			IP_VS_INC_ESTATS(ip_vs_esmib,
+					 SYNPROXY_CONN_REUSED_CLOSEWAIT);
+			break;
+		case IP_VS_TCP_S_LAST_ACK:
+			IP_VS_INC_ESTATS(ip_vs_esmib,
+					 SYNPROXY_CONN_REUSED_LASTACK);
+			break;
+		}
+
+		spin_lock(&cp->lock);
+		__syn_proxy_reuse_conn(cp, skb, th, pp);
+		spin_unlock(&cp->lock);
+
+		if (unlikely(!syn_proxy_send_rs_syn(af, th, cp, skb, pp, &opt))) {
+			IP_VS_ERR_RL
+			    ("syn_proxy_send_rs_syn failed when reuse conn!\n");
+			/* release conn immediately */
+			spin_lock(&cp->lock);
+			cp->timeout = 0;
+			spin_unlock(&cp->lock);
+		}
+
+		*verdict = NF_STOLEN;
+		return 0;
+	}
+
+	return 1;
+}
+
+/*
+ * Check and stop ack storm.
+ * Return 0 if ack storm is found.
+ */
+static int syn_proxy_is_ack_storm(struct tcphdr *tcph, struct ip_vs_conn *cp)
+{
+	/* only for syn-proxy sessions */
+	if (!(cp->flags & IP_VS_CONN_F_SYNPROXY) || !tcph->ack)
+		return 1;
+
+	if (unlikely(sysctl_ip_vs_synproxy_dup_ack_thresh == 0))
+		return 1;
+
+	if (unlikely(tcph->seq == cp->last_seq &&
+		     tcph->ack_seq == cp->last_ack_seq)) {
+		atomic_inc(&cp->dup_ack_cnt);
+		if (atomic_read(&cp->dup_ack_cnt) >=
+		    sysctl_ip_vs_synproxy_dup_ack_thresh) {
+			atomic_set(&cp->dup_ack_cnt,
+				   sysctl_ip_vs_synproxy_dup_ack_thresh);
+			/* update statistics */
+			IP_VS_INC_ESTATS(ip_vs_esmib, SYNPROXY_ACK_STORM);
+			return 0;
+		}
+
+		return 1;
+	}
+
+	cp->last_seq = tcph->seq;
+	cp->last_ack_seq = tcph->ack_seq;
+	atomic_set(&cp->dup_ack_cnt, 0);
+
+	return 1;
+}
+
+/*
+ * Syn-proxy snat handler:
+ * 1) check and stop ack storm.
+ * 2)Update in-out seqs: include th->seq
+ * and also correct tcph->check.
+ *
+ * Return 0 if ack storm is found and stoped.
+ */
+int ip_vs_synproxy_snat_handler(struct tcphdr *tcph, struct ip_vs_conn *cp)
+{
+	__u32 old_seq;
+
+	if (syn_proxy_is_ack_storm(tcph, cp) == 0) {
+		return 0;
+	}
+
+	if (cp->syn_proxy_seq.delta != 0) {
+		old_seq = ntohl(tcph->seq);
+		tcph->seq = htonl((__u32) (old_seq + cp->syn_proxy_seq.delta));
+		syn_proxy_seq_csum_update(tcph, htonl(old_seq), tcph->seq);
+		IP_VS_DBG(6,
+			  "tcp_snat_handler: tcph->seq %u => %u, delta = %u \n",
+			  old_seq, htonl(tcph->seq), cp->syn_proxy_seq.delta);
+	}
+
+	return 1;
+}
+
+int
+ip_vs_synproxy_filter_ack(struct sk_buff *skb, struct ip_vs_conn *cp,
+			  struct ip_vs_protocol *pp,
+			  struct ip_vs_iphdr *iph, int *verdict)
+{
+	struct tcphdr _tcph, *th;
+
+	th = skb_header_pointer(skb, iph->len, sizeof(_tcph), &_tcph);
+
+	if (unlikely(NULL == th)) {
+		IP_VS_ERR_RL("skb has a invalid tcp header\n");
+		*verdict = NF_DROP;
+		return 0;
+	}
+
+	spin_lock(&cp->lock);
+	if ((cp->flags & IP_VS_CONN_F_SYNPROXY) &&
+	    cp->state == IP_VS_TCP_S_SYN_SENT) {
+		/*
+		 * Not a ack packet, drop it.
+		 */
+		if (!th->ack) {
+			spin_unlock(&cp->lock);
+			*verdict = NF_DROP;
+			return 0;
+		}
+
+		if (sysctl_ip_vs_synproxy_skb_store_thresh <
+		    skb_queue_len(&cp->ack_skb)) {
+			spin_unlock(&cp->lock);
+			/* update statistics */
+			IP_VS_INC_ESTATS(ip_vs_esmib, SYNPROXY_SYNSEND_QLEN);
+			*verdict = NF_DROP;
+			return 0;
+		}
+
+		/*
+		 * Still some space left, store it.
+		 */
+		skb_queue_tail(&cp->ack_skb, skb);
+		spin_unlock(&cp->lock);
+		*verdict = NF_STOLEN;
+		return 0;
+	}
+
+	spin_unlock(&cp->lock);
+	return 1;
+}
diff --git a/net/netfilter/ipvs/ip_vs_xmit.c b/net/netfilter/ipvs/ip_vs_xmit.c
index 30b3189..72a2f86 100644
--- a/net/netfilter/ipvs/ip_vs_xmit.c
+++ b/net/netfilter/ipvs/ip_vs_xmit.c
@@ -28,9 +28,10 @@
 #include <linux/icmpv6.h>
 #include <linux/netfilter.h>
 #include <linux/netfilter_ipv4.h>
+#include <linux/netfilter_ipv6.h>
 
 #include <net/ip_vs.h>
-
+#include <linux/if_arp.h>
 
 /*
  *      Destination cache to speed up outgoing route lookup
@@ -46,8 +47,8 @@ __ip_vs_dst_set(struct ip_vs_dest *dest, u32 rtos, struct dst_entry *dst)
 	dst_release(old_dst);
 }
 
-static inline struct dst_entry *
-__ip_vs_dst_check(struct ip_vs_dest *dest, u32 rtos, u32 cookie)
+static inline struct dst_entry *__ip_vs_dst_check(struct ip_vs_dest *dest,
+						  u32 rtos, u32 cookie)
 {
 	struct dst_entry *dst = dest->dst_cache;
 
@@ -64,13 +65,12 @@ __ip_vs_dst_check(struct ip_vs_dest *dest, u32 rtos, u32 cookie)
 	return dst;
 }
 
-static struct rtable *
-__ip_vs_get_out_rt(struct ip_vs_conn *cp, u32 rtos)
+static struct rtable *__ip_vs_get_out_rt(struct ip_vs_conn *cp, u32 rtos)
 {
 	struct rtable *rt;			/* Route to the other host */
 	struct ip_vs_dest *dest = cp->dest;
 
-	if (dest) {
+	if (dest && dest->addr.ip != IP_VS_DSNAT_RS_ADDR) {
 		spin_lock(&dest->dst_lock);
 		if (!(rt = (struct rtable *)
 		      __ip_vs_dst_check(dest, rtos, 0))) {
@@ -115,6 +115,27 @@ __ip_vs_get_out_rt(struct ip_vs_conn *cp, u32 rtos)
 	return rt;
 }
 
+struct rtable *ip_vs_get_rt(union nf_inet_addr *addr, u32 rtos)
+{
+	struct rtable *rt;	/* Route to the other host */
+
+	struct flowi fl = {
+		.oif = 0,
+		.nl_u = {
+			 .ip4_u = {
+				   .daddr = addr->ip,
+				   .saddr = 0,
+				   .tos = rtos,}},
+	};
+
+	if (ip_route_output_key(&init_net, &rt, &fl)) {
+		IP_VS_DBG_RL("ip_route_output error, dest: %pI4\n", &addr->ip);
+		return NULL;
+	}
+
+	return rt;
+}
+
 #ifdef CONFIG_IP_VS_IPV6
 static struct rt6_info *
 __ip_vs_get_out_rt_v6(struct ip_vs_conn *cp)
@@ -176,6 +197,32 @@ __ip_vs_get_out_rt_v6(struct ip_vs_conn *cp)
 
 	return rt;
 }
+
+struct rt6_info *ip_vs_get_rt_v6(union nf_inet_addr *addr)
+{
+	struct rt6_info *rt;	/* Route to the other host */
+
+	struct flowi fl = {
+		.oif = 0,
+		.nl_u = {
+			 .ip6_u = {
+				   .daddr = addr->in6,
+				   .saddr = {
+					     .s6_addr32 = {0, 0, 0, 0},
+					     },
+				   },
+			 },
+	};
+
+	rt = (struct rt6_info *)ip6_route_output(&init_net, NULL, &fl);
+	if (!rt) {
+		IP_VS_DBG_RL("ip6_route_output error, dest: %pI6\n",
+			     &addr->in6);
+		return NULL;
+	}
+
+	return rt;
+}
 #endif
 
 
@@ -200,6 +247,832 @@ do {							\
 		(rt)->u.dst.dev, dst_output);		\
 } while (0)
 
+/* check if gso can handle the skb */
+static int gso_ok(struct sk_buff *skb, struct net_device *dev)
+{
+	if (skb_is_gso(skb)) {
+		/* LRO check */
+		if (unlikely(skb_shinfo(skb)->gso_type == 0)) {
+			IP_VS_ERR_RL("%s:LRO is enabled."
+					"Cannot be forwarded\n", dev->name);
+			IP_VS_INC_ESTATS(ip_vs_esmib, LRO_REJECT);
+			goto gso_err;
+		}
+
+		/* GRO check */
+		if (net_gso_ok(dev->features, skb_shinfo(skb)->gso_type)) {
+			/* the skb has frag_list, need do sth here */
+			if (skb_has_frags(skb) &&
+					!(dev->features & NETIF_F_FRAGLIST) &&
+							__skb_linearize(skb))
+				goto gso_err;
+
+			IP_VS_DBG_RL("skb length: %d . GSO is ok."
+					"can be forwarded\n", skb->len);
+			IP_VS_INC_ESTATS(ip_vs_esmib, GRO_PASS);
+			goto gso_ok;
+		}
+	}
+
+gso_err:
+	return 0;
+gso_ok:
+	return 1;
+}
+
+/*
+ * Packet has been made sufficiently writable in caller
+ * - inout: 1=in->out, 0=out->in
+ */
+static void ip_vs_nat_icmp(struct sk_buff *skb, struct ip_vs_protocol *pp,
+			   struct ip_vs_conn *cp, int inout)
+{
+	struct iphdr *iph = ip_hdr(skb);
+	unsigned int icmp_offset = iph->ihl * 4;
+	struct icmphdr *icmph = (struct icmphdr *)(skb_network_header(skb) +
+						   icmp_offset);
+	struct iphdr *ciph = (struct iphdr *)(icmph + 1);
+	__u32 fullnat = (IP_VS_FWD_METHOD(cp) == IP_VS_CONN_F_FULLNAT);
+	__u32 dsnat = (IP_VS_FWD_METHOD(cp) == IP_VS_CONN_F_DSNAT);
+
+	if (fullnat | dsnat) {
+		if (inout) {
+			iph->daddr = cp->caddr.ip;
+			ciph->saddr = cp->caddr.ip;
+		} else {
+			iph->saddr = cp->laddr.ip;
+			ciph->daddr = cp->laddr.ip;
+		}
+	}
+
+	if (inout) {
+		iph->saddr = cp->vaddr.ip;
+		ip_send_check(iph);
+		ciph->daddr = cp->vaddr.ip;
+		ip_send_check(ciph);
+	} else {
+		iph->daddr = cp->daddr.ip;
+		ip_send_check(iph);
+		ciph->saddr = cp->daddr.ip;
+		ip_send_check(ciph);
+	}
+
+	/* the TCP/UDP port */
+	if (IPPROTO_TCP == ciph->protocol || IPPROTO_UDP == ciph->protocol) {
+		__be16 *ports = (void *)ciph + ciph->ihl * 4;
+
+		if (fullnat | dsnat) {
+			if (inout) {
+				ports[0] = cp->cport;
+				/* The seq of packet form client
+				 *  has been changed by fullnat.
+				 * we must fix here to
+				 * ensure a valid icmp PKT */
+				if (IPPROTO_TCP == ciph->protocol) {
+					__be32 *seqs = (__be32 *)ports;
+					seqs[1] = htonl(ntohl(seqs[1]) -
+							cp->fnat_seq.delta);
+				}
+			} else
+				ports[1] = cp->lport;
+		}
+
+		if (inout)
+			ports[1] = cp->vport;
+		else {
+			ports[0] = cp->dport;
+			/* synproxy may modify the seq of packet form RS.
+			 * we fix here to ensure a valid icmp PKT*/
+			if (IPPROTO_TCP == ciph->protocol) {
+				__be32 *seqs = (__be32 *)ports;
+				seqs[1] = htonl(ntohl(seqs[1]) -
+						cp->syn_proxy_seq.delta);
+			}
+		}
+	}
+
+	/* And finally the ICMP checksum */
+	icmph->checksum = 0;
+	icmph->checksum = ip_vs_checksum_complete(skb, icmp_offset);
+	skb->ip_summed = CHECKSUM_UNNECESSARY;
+
+	if (inout)
+		IP_VS_DBG_PKT(11, pp, skb, (void *)ciph - (void *)iph,
+			      "Forwarding altered outgoing ICMP");
+	else
+		IP_VS_DBG_PKT(11, pp, skb, (void *)ciph - (void *)iph,
+			      "Forwarding altered incoming ICMP");
+}
+
+#ifdef CONFIG_IP_VS_IPV6
+static void ip_vs_nat_icmp_v6(struct sk_buff *skb, struct ip_vs_protocol *pp,
+			      struct ip_vs_conn *cp, int inout)
+{
+	struct ipv6hdr *iph = ipv6_hdr(skb);
+	unsigned int icmp_offset = sizeof(struct ipv6hdr);
+	struct icmp6hdr *icmph = (struct icmp6hdr *)(skb_network_header(skb) +
+						     icmp_offset);
+	struct ipv6hdr *ciph = (struct ipv6hdr *)(icmph + 1);
+	__u32 fullnat = (IP_VS_FWD_METHOD(cp) == IP_VS_CONN_F_FULLNAT);
+
+	if (fullnat) {
+		if (inout) {
+			iph->daddr = cp->caddr.in6;
+			ciph->saddr = cp->caddr.in6;
+		} else {
+			iph->saddr = cp->laddr.in6;
+			ciph->daddr = cp->laddr.in6;
+		}
+	}
+
+	if (inout) {
+		iph->saddr = cp->vaddr.in6;
+		ciph->daddr = cp->vaddr.in6;
+	} else {
+		iph->daddr = cp->daddr.in6;
+		ciph->saddr = cp->daddr.in6;
+	}
+
+	/* the TCP/UDP port */
+	if (IPPROTO_TCP == ciph->nexthdr || IPPROTO_UDP == ciph->nexthdr) {
+		__be16 *ports = (void *)ciph + sizeof(struct ipv6hdr);
+
+		if (fullnat) {
+			if (inout) {
+				ports[0] = cp->cport;
+				if (IPPROTO_TCP == ciph->nexthdr) {
+					__be32 *seqs = (__be32 *)ports;
+					seqs[1] = htonl(ntohl(seqs[1]) -
+							cp->fnat_seq.delta);
+				}
+			} else
+				ports[1] = cp->lport;
+		}
+
+		if (inout)
+			ports[1] = cp->vport;
+		else {
+			ports[0] = cp->dport;
+			if (IPPROTO_TCP == ciph->nexthdr) {
+				__be32 *seqs = (__be32 *)ports;
+				seqs[1] = htonl(ntohl(seqs[1]) -
+						cp->syn_proxy_seq.delta);
+			}
+		}
+	}
+
+	/* And finally the ICMP checksum */
+	icmph->icmp6_cksum = 0;
+	/* TODO IPv6: is this correct for ICMPv6? */
+	ip_vs_checksum_complete(skb, icmp_offset);
+	skb->ip_summed = CHECKSUM_UNNECESSARY;
+
+	if (inout)
+		IP_VS_DBG_PKT(11, pp, skb, (void *)ciph - (void *)iph,
+			      "Forwarding altered outgoing ICMPv6");
+	else
+		IP_VS_DBG_PKT(11, pp, skb, (void *)ciph - (void *)iph,
+			      "Forwarding altered incoming ICMPv6");
+}
+#endif
+
+/* Response transmit icmp to client
+ * Used for NAT/LOCAL.
+ */
+int
+ip_vs_normal_response_icmp_xmit(struct sk_buff *skb, struct ip_vs_protocol *pp,
+				struct ip_vs_conn *cp, int offset)
+{
+	struct rtable *rt;	/* Route to the other host */
+	int mtu;
+	struct iphdr *iph = ip_hdr(skb);
+
+	if (!skb_make_writable(skb, offset))
+		goto out;
+
+	/* lookup route table */
+	if (!(rt = ip_vs_get_rt(&cp->caddr, RT_TOS(iph->tos))))
+		goto out;
+
+	/* MTU checking */
+	mtu = dst_mtu(&rt->u.dst);
+	if ((skb->len > mtu) && (iph->frag_off & htons(IP_DF))) {
+		ip_rt_put(rt);
+		IP_VS_DBG_RL_PKT(0, pp, skb, 0,
+				 "nat_response_icmp(): frag needed for");
+		goto out;
+	}
+
+	if (skb_cow(skb, rt->u.dst.dev->hard_header_len))
+		goto error_put;
+
+	/* drop old route */
+	skb_dst_drop(skb);
+	skb_dst_set(skb, &rt->u.dst);
+
+	ip_vs_nat_icmp(skb, pp, cp, 1);
+
+	/* Another hack: avoid icmp_send in ip_fragment */
+	skb->local_df = 1;
+
+	IP_VS_XMIT(PF_INET, skb, rt);
+
+	return NF_STOLEN;
+
+error_put:
+	ip_rt_put(rt);
+out:
+	return NF_DROP;
+}
+
+#ifdef CONFIG_IP_VS_IPV6
+
+int
+ip_vs_normal_response_icmp_xmit_v6(struct sk_buff *skb,
+				   struct ip_vs_protocol *pp,
+				   struct ip_vs_conn *cp, int offset)
+{
+	struct rt6_info *rt;	/* Route to the other host */
+	int mtu;
+
+	if (!skb_make_writable(skb, offset))
+		goto out;
+
+	/* lookup route table */
+	if (!(rt = ip_vs_get_rt_v6(&cp->caddr)))
+		goto out;
+
+	/* MTU checking */
+	mtu = dst_mtu(&rt->u.dst);
+	if (skb->len > mtu) {
+		dst_release(&rt->u.dst);
+		IP_VS_DBG_RL("%s(): frag needed\n", __func__);
+		goto out;
+	}
+
+	if (skb_cow(skb, rt->u.dst.dev->hard_header_len))
+		goto error_put;
+
+	/* drop old route */
+	skb_dst_drop(skb);
+	skb_dst_set(skb, &rt->u.dst);
+
+	ip_vs_nat_icmp_v6(skb, pp, cp, 1);
+
+	/* Another hack: avoid icmp_send in ip_fragment */
+	skb->local_df = 1;
+
+	IP_VS_XMIT(PF_INET6, skb, rt);
+
+	return NF_STOLEN;
+
+error_put:
+	dst_release(&rt->u.dst);
+out:
+	return NF_DROP;
+}
+
+#endif
+
+/* Response transmit icmp to client
+ * Used for NAT / local client / FULLNAT.
+ */
+int
+ip_vs_fnat_response_icmp_xmit(struct sk_buff *skb, struct ip_vs_protocol *pp,
+			      struct ip_vs_conn *cp, int offset)
+{
+	struct rtable *rt;	/* Route to the other host */
+	int mtu;
+	struct iphdr *iph = ip_hdr(skb);
+
+	/* lookup route table */
+	if (!(rt = ip_vs_get_rt(&cp->caddr, RT_TOS(iph->tos))))
+		goto tx_error_icmp;
+
+	/* MTU checking */
+	mtu = dst_mtu(&rt->u.dst);
+	if ((skb->len > mtu) && (iph->frag_off & htons(IP_DF))) {
+		ip_rt_put(rt);
+		IP_VS_DBG_RL_PKT(0, pp, skb, 0,
+				 "fnat_response_icmp(): frag needed for");
+		goto tx_error;
+	}
+
+	/* copy-on-write the packet before mangling it */
+	if (!skb_make_writable(skb, offset))
+		goto tx_error_put;
+
+	if (skb_cow(skb, rt->u.dst.dev->hard_header_len))
+		goto tx_error_put;
+
+	/* drop old route */
+	skb_dst_drop(skb);
+	skb_dst_set(skb, &rt->u.dst);
+
+	ip_vs_nat_icmp(skb, pp, cp, 1);
+
+	/* Another hack: avoid icmp_send in ip_fragment */
+	skb->local_df = 1;
+
+	IP_VS_XMIT(PF_INET, skb, rt);
+
+	return NF_STOLEN;
+
+      tx_error_icmp:
+	dst_link_failure(skb);
+      tx_error:
+	kfree_skb(skb);
+	return NF_STOLEN;
+      tx_error_put:
+	ip_rt_put(rt);
+	goto tx_error;
+}
+
+#ifdef CONFIG_IP_VS_IPV6
+
+int
+ip_vs_fnat_response_icmp_xmit_v6(struct sk_buff *skb, struct ip_vs_protocol *pp,
+				 struct ip_vs_conn *cp, int offset)
+{
+	struct rt6_info *rt;	/* Route to the other host */
+	int mtu;
+
+	/* lookup route table */
+	if (!(rt = ip_vs_get_rt_v6(&cp->caddr)))
+		goto tx_error_icmp;
+
+	/* MTU checking */
+	mtu = dst_mtu(&rt->u.dst);
+	if (skb->len > mtu) {
+		dst_release(&rt->u.dst);
+		IP_VS_DBG_RL("%s(): frag needed\n", __func__);
+		goto tx_error;
+	}
+
+	/* copy-on-write the packet before mangling it */
+	if (!skb_make_writable(skb, offset))
+		goto tx_error_put;
+
+	if (skb_cow(skb, rt->u.dst.dev->hard_header_len))
+		goto tx_error_put;
+
+	/* drop old route */
+	skb_dst_drop(skb);
+	skb_dst_set(skb, &rt->u.dst);
+
+	ip_vs_nat_icmp_v6(skb, pp, cp, 1);
+
+	/* Another hack: avoid icmp_send in ip_fragment */
+	skb->local_df = 1;
+
+	IP_VS_XMIT(PF_INET6, skb, rt);
+
+	return NF_STOLEN;
+
+      tx_error_icmp:
+	dst_link_failure(skb);
+      tx_error:
+	kfree_skb(skb);
+	return NF_STOLEN;
+      tx_error_put:
+	dst_release(&rt->u.dst);
+	goto tx_error;
+}
+
+#endif
+
+/* just for nat/fullnat mode */
+int
+ip_vs_fast_response_xmit(struct sk_buff *skb, struct ip_vs_protocol *pp,
+						struct ip_vs_conn *cp)
+{
+	struct ethhdr *eth;
+
+	if (!cp->indev)
+		goto err;
+	if (!gso_ok(skb, cp->indev) && (skb->len > cp->indev->mtu))
+		goto err;
+
+	/* Try to reuse skb */
+	if (unlikely(skb_shared(skb) || skb_cloned(skb))) {
+		struct sk_buff *new_skb = skb_copy(skb, GFP_ATOMIC);
+		if(unlikely(new_skb == NULL))
+			goto err;
+
+		/* Drop old skb */
+		kfree_skb(skb);
+		skb = new_skb;
+		IP_VS_INC_ESTATS(ip_vs_esmib, FAST_XMIT_SKB_COPY);
+	}
+
+	/* change ip, port. */
+	if (cp->flags & IP_VS_CONN_F_FULLNAT) {
+		if (pp->fnat_out_handler && !pp->fnat_out_handler(skb, pp, cp))
+			goto err;
+
+		ip_hdr(skb)->saddr = cp->vaddr.ip;
+		ip_hdr(skb)->daddr = cp->caddr.ip;
+	} else {
+		IP_VS_ERR_RL("L2 fast xmit support fullnat only!\n");
+		goto err;
+		/*if (pp->snat_handler && !pp->snat_handler(skb, pp, cp))
+			goto err;
+
+		ip_hdr(skb)->saddr = cp->vaddr.ip;*/
+	}
+
+	ip_send_check(ip_hdr(skb));
+
+	skb->dev = cp->indev;
+
+	if(unlikely(skb_headroom(skb) < LL_RESERVED_SPACE(skb->dev))){
+		struct sk_buff *skb2;
+
+		IP_VS_ERR_RL("need more headroom! realloc skb\n");
+		skb2 = skb_realloc_headroom(skb, LL_RESERVED_SPACE(skb->dev));
+		if (skb2 == NULL)
+			goto err;
+		kfree_skb(skb);
+		skb = skb2;
+	}
+
+	if(likely(skb_mac_header_was_set(skb))) {
+		eth = eth_hdr(skb);
+		memcpy(eth->h_dest, cp->src_hwaddr, ETH_ALEN);
+		memcpy(eth->h_source, cp->dst_hwaddr, ETH_ALEN);
+		skb->data = (unsigned char *)eth_hdr(skb);
+		skb->len += sizeof(struct ethhdr);
+	} else {
+		eth = (struct ethhdr *)skb_push(skb, sizeof(struct ethhdr));
+		skb_reset_mac_header(skb);
+		memcpy(eth->h_dest, cp->src_hwaddr, ETH_ALEN);
+		memcpy(eth->h_source, cp->dst_hwaddr, ETH_ALEN);
+	}
+	skb->protocol = eth->h_proto = htons(ETH_P_IP);
+	skb->pkt_type = PACKET_OUTGOING;
+
+	IP_VS_DBG_RL("%s: send skb to client!\n", __func__);
+
+	/* Send the packet out */
+	do {
+		int ret = dev_queue_xmit(skb);
+		if (ret != 0)
+			IP_VS_ERR_RL("dev_queue_xmit failed! code:%d\n", ret);
+	}while(0);
+
+	IP_VS_INC_ESTATS(ip_vs_esmib, FAST_XMIT_PASS);
+	return 0;
+err:
+	IP_VS_INC_ESTATS(ip_vs_esmib, FAST_XMIT_REJECT);
+	return 1;
+}
+
+#ifdef CONFIG_IP_VS_IPV6
+/* just for nat/fullnat mode */
+int
+ip_vs_fast_response_xmit_v6(struct sk_buff *skb, struct ip_vs_protocol *pp,
+						struct ip_vs_conn *cp)
+{
+	struct ethhdr *eth;
+
+	if (!cp->indev)
+		goto err;
+	if (!gso_ok(skb, cp->indev) && (skb->len > cp->indev->mtu))
+		goto err;
+
+	/* Try to reuse skb if possible */
+	if (unlikely(skb_shared(skb) || skb_cloned(skb))) {
+		struct sk_buff *new_skb = skb_copy(skb, GFP_ATOMIC);
+		if(unlikely(new_skb == NULL))
+			goto err;
+
+		/* Drop old skb */
+		kfree_skb(skb);
+		skb = new_skb;
+		IP_VS_INC_ESTATS(ip_vs_esmib, FAST_XMIT_SKB_COPY);
+	}
+
+	/* change ip, port. */
+	if (cp->flags & IP_VS_CONN_F_FULLNAT) {
+		if (pp->fnat_out_handler && !pp->fnat_out_handler(skb, pp, cp))
+			goto err;
+
+		ipv6_hdr(skb)->saddr = cp->vaddr.in6;
+		ipv6_hdr(skb)->daddr = cp->caddr.in6;
+	} else {
+		IP_VS_ERR_RL("L2 fast xmit support fullnat only!\n");
+		goto err;
+		/*if (pp->snat_handler && !pp->snat_handler(skb, pp, cp))
+			goto err;
+
+		ipv6_hdr(skb)->saddr = cp->vaddr.in6;*/
+	}
+
+	skb->dev = cp->indev;
+
+	if(unlikely(skb_headroom(skb) < LL_RESERVED_SPACE(skb->dev))){
+		struct sk_buff *skb2;
+
+		IP_VS_ERR_RL("need more headroom! realloc skb\n");
+		skb2 = skb_realloc_headroom(skb, LL_RESERVED_SPACE(skb->dev));
+		if (skb2 == NULL)
+			goto err;
+		kfree_skb(skb);
+		skb = skb2;
+	}
+
+	if(likely(skb_mac_header_was_set(skb))) {
+		eth = eth_hdr(skb);
+		memcpy(eth->h_dest, cp->src_hwaddr, ETH_ALEN);
+		memcpy(eth->h_source, cp->dst_hwaddr, ETH_ALEN);
+		skb->data = (unsigned char *)eth_hdr(skb);
+		skb->len += sizeof(struct ethhdr);
+	} else {
+		eth = (struct ethhdr *)skb_push(skb, sizeof(struct ethhdr));
+		skb_reset_mac_header(skb);
+		memcpy(eth->h_dest, cp->src_hwaddr, ETH_ALEN);
+		memcpy(eth->h_source, cp->dst_hwaddr, ETH_ALEN);
+	}
+	skb->protocol = eth->h_proto = htons(ETH_P_IPV6);
+	skb->pkt_type = PACKET_OUTGOING;
+
+	IP_VS_DBG_RL("%s: send skb to client!\n", __func__);
+	/* Send the packet out */
+	do {
+		int ret = dev_queue_xmit(skb);
+		if (ret != 0)
+			IP_VS_ERR_RL("dev_queue_xmit failed! code:%d\n", ret);
+	}while(0);
+
+	IP_VS_INC_ESTATS(ip_vs_esmib, FAST_XMIT_PASS);
+	return 0;
+err:
+	IP_VS_INC_ESTATS(ip_vs_esmib, FAST_XMIT_REJECT);
+	return 1;
+}
+#endif
+
+/* Response transmit to client
+ * Used for NAT/Local.
+ */
+int
+ip_vs_normal_response_xmit(struct sk_buff *skb, struct ip_vs_protocol *pp,
+			   struct ip_vs_conn *cp, int ihl)
+{
+	struct rtable *rt;
+	int mtu;
+
+	/* copy-on-write the packet before mangling it */
+	if (!skb_make_writable(skb, ihl))
+		goto drop;
+
+	/* mangle the packet */
+	if (pp->snat_handler && !pp->snat_handler(skb, pp, cp))
+		goto drop;
+
+	ip_hdr(skb)->saddr = cp->vaddr.ip;
+	ip_send_check(ip_hdr(skb));
+
+	/* For policy routing, packets originating from this
+	 * machine itself may be routed differently to packets
+	 * passing through.  We want this packet to be routed as
+	 * if it came from this machine itself.  So re-compute
+	 * the routing information.
+	 */
+//	if (ip_route_me_harder(skb, RTN_LOCAL) != 0)
+//		goto drop;
+
+	/* lookup route table */
+	if(!(rt = ip_vs_get_rt(&cp->caddr, RT_TOS(ip_hdr(skb)->tos))))
+		goto drop;
+
+	/* MTU checking */
+	mtu = dst_mtu(&rt->u.dst);
+	if ((skb->len > mtu) && (ip_hdr(skb)->frag_off & htons(IP_DF))) {
+		ip_rt_put(rt);
+		IP_VS_INC_ESTATS(ip_vs_esmib, XMIT_UNEXPECTED_MTU);
+		icmp_send(skb, ICMP_DEST_UNREACH, ICMP_FRAG_NEEDED, htonl(mtu));
+		IP_VS_DBG_RL_PKT(0, pp, skb, 0,
+				 "handle_nat_response(): frag needed for");
+		goto drop;
+	}
+
+	if (skb_cow(skb, rt->u.dst.dev->hard_header_len)) {
+		ip_rt_put(rt);
+		goto drop;
+	}
+
+	/* drop old route */
+	skb_dst_drop(skb);
+	skb_dst_set(skb, &rt->u.dst);
+
+	/* Another hack: avoid icmp_send in ip_fragment */
+	skb->local_df = 1;
+
+	IP_VS_XMIT(PF_INET, skb, rt);
+
+	return NF_STOLEN;
+
+drop:
+	kfree_skb(skb);
+	return NF_STOLEN;
+}
+
+#ifdef CONFIG_IP_VS_IPV6
+
+int
+ip_vs_normal_response_xmit_v6(struct sk_buff *skb, struct ip_vs_protocol *pp,
+			      struct ip_vs_conn *cp, int ihl)
+{
+	struct rt6_info *rt;
+	int mtu;
+
+	/* copy-on-write the packet before mangling it */
+	if (!skb_make_writable(skb, ihl))
+		goto drop;
+
+	/* mangle the packet */
+	if (pp->snat_handler && !pp->snat_handler(skb, pp, cp))
+		goto drop;
+
+	ipv6_hdr(skb)->saddr = cp->vaddr.in6;
+
+	/* For policy routing, packets originating from this
+	 * machine itself may be routed differently to packets
+	 * passing through.  We want this packet to be routed as
+	 * if it came from this machine itself.  So re-compute
+	 * the routing information.
+	 */
+//	if (ip6_route_me_harder(skb) != 0)
+//		goto drop;
+
+	/* lookup route table */
+	if (!(rt = ip_vs_get_rt_v6(&cp->caddr)))
+		goto drop;
+
+	/* MTU checking */
+	mtu = dst_mtu(&rt->u.dst);
+	if (skb->len > mtu) {
+		dst_release(&rt->u.dst);
+		IP_VS_INC_ESTATS(ip_vs_esmib, XMIT_UNEXPECTED_MTU);
+		icmpv6_send(skb, ICMPV6_PKT_TOOBIG, 0, mtu, skb->dev);
+		IP_VS_DBG_RL_PKT(0, pp, skb, 0,
+				 "handle_fnat_response_v6(): frag needed for");
+		goto drop;
+	}
+
+	if (skb_cow(skb, rt->u.dst.dev->hard_header_len)) {
+		dst_release(&rt->u.dst);
+		goto drop;
+	}
+
+	/* drop old route */
+	skb_dst_drop(skb);
+	skb_dst_set(skb, &rt->u.dst);
+
+	/* Another hack: avoid icmp_send in ip_fragment */
+	skb->local_df = 1;
+
+	IP_VS_XMIT(PF_INET6, skb, rt);
+
+	return NF_STOLEN;
+
+drop:
+	kfree_skb(skb);
+	return NF_STOLEN;
+}
+
+#endif
+
+/* Response transmit to client
+ * Used for FULLNAT.
+ */
+int
+ip_vs_fnat_response_xmit(struct sk_buff *skb, struct ip_vs_protocol *pp,
+			 struct ip_vs_conn *cp, int ihl)
+{
+	struct rtable *rt;	/* Route to the other host */
+	int mtu;
+	struct iphdr *iph = ip_hdr(skb);
+
+	if(sysctl_ip_vs_fast_xmit && !ip_vs_fast_response_xmit(skb, pp, cp))
+		return NF_STOLEN;
+
+	/* lookup route table */
+	if (!(rt = ip_vs_get_rt(&cp->caddr, RT_TOS(iph->tos))))
+		goto tx_error_icmp;
+
+	/* MTU checking */
+	mtu = dst_mtu(&rt->u.dst);
+	if (!gso_ok(skb, rt->u.dst.dev) && (skb->len > mtu) &&
+					(iph->frag_off & htons(IP_DF))) {
+		ip_rt_put(rt);
+		IP_VS_INC_ESTATS(ip_vs_esmib, XMIT_UNEXPECTED_MTU);
+		icmp_send(skb, ICMP_DEST_UNREACH, ICMP_FRAG_NEEDED, htonl(mtu));
+		IP_VS_DBG_RL_PKT(0, pp, skb, 0,
+				 "handle_fnat_response(): frag needed for");
+		goto tx_error;
+	}
+
+	/* copy-on-write the packet before mangling it */
+	if (!skb_make_writable(skb, ihl))
+		goto tx_error_put;
+
+	if (skb_cow(skb, rt->u.dst.dev->hard_header_len))
+		goto tx_error_put;
+
+	/* drop old route */
+	skb_dst_drop(skb);
+	skb_dst_set(skb, &rt->u.dst);
+
+	/* mangle the packet */
+	if (pp->fnat_out_handler && !pp->fnat_out_handler(skb, pp, cp))
+		goto tx_error;
+
+	ip_hdr(skb)->saddr = cp->vaddr.ip;
+	ip_hdr(skb)->daddr = cp->caddr.ip;
+	ip_send_check(ip_hdr(skb));
+
+	/* Another hack: avoid icmp_send in ip_fragment */
+	skb->local_df = 1;
+
+	IP_VS_XMIT(PF_INET, skb, rt);
+
+	return NF_STOLEN;
+
+      tx_error_icmp:
+	dst_link_failure(skb);
+      tx_error:
+	kfree_skb(skb);
+	return NF_STOLEN;
+      tx_error_put:
+	ip_rt_put(rt);
+	goto tx_error;
+}
+
+#ifdef CONFIG_IP_VS_IPV6
+
+int
+ip_vs_fnat_response_xmit_v6(struct sk_buff *skb, struct ip_vs_protocol *pp,
+			    struct ip_vs_conn *cp, int ihl)
+{
+	struct rt6_info *rt;	/* Route to the other host */
+	int mtu;
+
+	if(sysctl_ip_vs_fast_xmit && !ip_vs_fast_response_xmit_v6(skb, pp, cp))
+		return NF_STOLEN;
+
+	/* lookup route table */
+	if (!(rt = ip_vs_get_rt_v6(&cp->caddr)))
+		goto tx_error_icmp;
+
+	/* MTU checking */
+	mtu = dst_mtu(&rt->u.dst);
+	if (!gso_ok(skb, rt->u.dst.dev) && (skb->len > mtu)) {
+		dst_release(&rt->u.dst);
+		IP_VS_INC_ESTATS(ip_vs_esmib, XMIT_UNEXPECTED_MTU);
+		icmpv6_send(skb, ICMPV6_PKT_TOOBIG, 0, mtu, skb->dev);
+		IP_VS_DBG_RL_PKT(0, pp, skb, 0,
+				 "handle_fnat_response_v6(): frag needed for");
+		goto tx_error;
+	}
+
+	/* copy-on-write the packet before mangling it */
+	if (!skb_make_writable(skb, ihl))
+		goto tx_error_put;
+
+	if (skb_cow(skb, rt->u.dst.dev->hard_header_len))
+		goto tx_error_put;
+
+	/* drop old route */
+	skb_dst_drop(skb);
+	skb_dst_set(skb, &rt->u.dst);
+
+	/* mangle the packet */
+	if (pp->fnat_out_handler && !pp->fnat_out_handler(skb, pp, cp))
+		goto tx_error;
+
+	ipv6_hdr(skb)->saddr = cp->vaddr.in6;
+	ipv6_hdr(skb)->daddr = cp->caddr.in6;
+
+	/* Another hack: avoid icmp_send in ip_fragment */
+	skb->local_df = 1;
+
+	IP_VS_XMIT(PF_INET6, skb, rt);
+
+	return NF_STOLEN;
+
+      tx_error_icmp:
+	dst_link_failure(skb);
+      tx_error:
+	kfree_skb(skb);
+	return NF_STOLEN;
+      tx_error_put:
+	dst_release(&rt->u.dst);
+	goto tx_error;
+}
+
+#endif
 
 /*
  *      NULL transmitter (do nothing except return NF_ACCEPT)
@@ -247,6 +1120,7 @@ ip_vs_bypass_xmit(struct sk_buff *skb, struct ip_vs_conn *cp,
 	mtu = dst_mtu(&rt->u.dst);
 	if ((skb->len > mtu) && (iph->frag_off & htons(IP_DF))) {
 		ip_rt_put(rt);
+		IP_VS_INC_ESTATS(ip_vs_esmib, XMIT_UNEXPECTED_MTU);
 		icmp_send(skb, ICMP_DEST_UNREACH,ICMP_FRAG_NEEDED, htonl(mtu));
 		IP_VS_DBG_RL("%s(): frag needed\n", __func__);
 		goto tx_error;
@@ -311,6 +1185,7 @@ ip_vs_bypass_xmit_v6(struct sk_buff *skb, struct ip_vs_conn *cp,
 	mtu = dst_mtu(&rt->u.dst);
 	if (skb->len > mtu) {
 		dst_release(&rt->u.dst);
+		IP_VS_INC_ESTATS(ip_vs_esmib, XMIT_UNEXPECTED_MTU);
 		icmpv6_send(skb, ICMPV6_PKT_TOOBIG, 0, mtu, skb->dev);
 		IP_VS_DBG_RL("%s(): frag needed\n", __func__);
 		goto tx_error;
@@ -347,6 +1222,43 @@ ip_vs_bypass_xmit_v6(struct sk_buff *skb, struct ip_vs_conn *cp,
 }
 #endif
 
+void
+ip_vs_save_xmit_info(struct sk_buff *skb, struct ip_vs_protocol *pp,
+					struct ip_vs_conn *cp)
+{
+	if(!sysctl_ip_vs_fast_xmit)
+		return;
+
+	if(!skb->dev) {
+		IP_VS_INC_ESTATS(ip_vs_esmib, FAST_XMIT_DEV_LOST);
+		IP_VS_DBG_RL("save_xmit_info, skb->dev is NULL. \n");
+		return;
+	}
+	IP_VS_DBG_RL("save_xmit_info, netdevice:%s\n", netdev_name(skb->dev));
+
+	if(likely((skb->dev->type == ARPHRD_ETHER) &&
+					skb_mac_header_was_set(skb))) {
+		struct ethhdr *eth = (struct ethhdr *)skb_mac_header(skb);
+
+		if(unlikely(cp->indev == NULL)) {
+			cp->indev = skb->dev;
+			dev_hold(cp->indev);
+		}
+
+		if (unlikely(cp->indev != skb->dev)) {
+			dev_put(cp->indev);
+			cp->indev = skb->dev;
+			dev_hold(cp->indev);
+		}
+
+		memcpy(cp->src_hwaddr, eth->h_source, ETH_ALEN);
+		memcpy(cp->dst_hwaddr, eth->h_dest, ETH_ALEN);
+	} else {
+		IP_VS_INC_ESTATS(ip_vs_esmib, FAST_XMIT_NO_MAC);
+		IP_VS_DBG_RL("save dev and mac failed!\n");
+	}
+}
+
 /*
  *      NAT transmitter (only for outside-to-inside nat forwarding)
  *      Not used for related ICMP
@@ -378,8 +1290,10 @@ ip_vs_nat_xmit(struct sk_buff *skb, struct ip_vs_conn *cp,
 	mtu = dst_mtu(&rt->u.dst);
 	if ((skb->len > mtu) && (iph->frag_off & htons(IP_DF))) {
 		ip_rt_put(rt);
+		IP_VS_INC_ESTATS(ip_vs_esmib, XMIT_UNEXPECTED_MTU);
 		icmp_send(skb, ICMP_DEST_UNREACH,ICMP_FRAG_NEEDED, htonl(mtu));
-		IP_VS_DBG_RL_PKT(0, pp, skb, 0, "ip_vs_nat_xmit(): frag needed for");
+		IP_VS_DBG_RL_PKT(0, pp, skb, 0,
+				 "ip_vs_nat_xmit(): frag needed for");
 		goto tx_error;
 	}
 
@@ -454,6 +1368,7 @@ ip_vs_nat_xmit_v6(struct sk_buff *skb, struct ip_vs_conn *cp,
 	mtu = dst_mtu(&rt->u.dst);
 	if (skb->len > mtu) {
 		dst_release(&rt->u.dst);
+		IP_VS_INC_ESTATS(ip_vs_esmib, XMIT_UNEXPECTED_MTU);
 		icmpv6_send(skb, ICMPV6_PKT_TOOBIG, 0, mtu, skb->dev);
 		IP_VS_DBG_RL_PKT(0, pp, skb, 0,
 				 "ip_vs_nat_xmit_v6(): frag needed for");
@@ -502,6 +1417,170 @@ tx_error_put:
 }
 #endif
 
+/*
+ *      FULLNAT transmitter (only for outside-to-inside fullnat forwarding)
+ *      Not used for related ICMP
+ */
+int
+ip_vs_fnat_xmit(struct sk_buff *skb, struct ip_vs_conn *cp,
+		struct ip_vs_protocol *pp)
+{
+	struct rtable *rt;	/* Route to the other host */
+	int mtu;
+	struct iphdr *iph = ip_hdr(skb);
+
+	EnterFunction(10);
+
+	/* check if it is a connection of no-client-port */
+	if (unlikely(cp->flags & IP_VS_CONN_F_NO_CPORT)) {
+		__be16 _pt, *p;
+		p = skb_header_pointer(skb, iph->ihl * 4, sizeof(_pt), &_pt);
+		if (p == NULL)
+			goto tx_error;
+		ip_vs_conn_fill_cport(cp, *p);
+		IP_VS_DBG(10, "filled cport=%d\n", ntohs(*p));
+	}
+
+	if (!(rt = __ip_vs_get_out_rt(cp, RT_TOS(iph->tos))))
+		goto tx_error_icmp;
+
+	/* MTU checking */
+	mtu = dst_mtu(&rt->u.dst);
+	if (!gso_ok(skb, rt->u.dst.dev) && (skb->len > mtu) &&
+					(iph->frag_off & htons(IP_DF))) {
+		ip_rt_put(rt);
+		IP_VS_INC_ESTATS(ip_vs_esmib, XMIT_UNEXPECTED_MTU);
+		icmp_send(skb, ICMP_DEST_UNREACH, ICMP_FRAG_NEEDED, htonl(mtu));
+		IP_VS_DBG_RL_PKT(0, pp, skb, 0,
+				 "ip_vs_fnat_xmit(): frag needed for");
+		goto tx_error;
+	}
+
+	ip_vs_save_xmit_info(skb, pp, cp);
+
+	/* copy-on-write the packet before mangling it */
+	if (!skb_make_writable(skb, sizeof(struct iphdr)))
+		goto tx_error_put;
+
+	if (skb_cow(skb, rt->u.dst.dev->hard_header_len))
+		goto tx_error_put;
+
+	/* drop old route */
+	skb_dst_drop(skb);
+	skb_dst_set(skb, &rt->u.dst);
+
+	/* mangle the packet */
+	if (pp->fnat_in_handler && !pp->fnat_in_handler(&skb, pp, cp))
+		goto tx_error;
+	ip_hdr(skb)->saddr = cp->laddr.ip;
+	ip_hdr(skb)->daddr = cp->daddr.ip;
+	ip_send_check(ip_hdr(skb));
+
+	IP_VS_DBG_PKT(10, pp, skb, 0, "After FNAT-IN");
+
+	/* FIXME: when application helper enlarges the packet and the length
+	   is larger than the MTU of outgoing device, there will be still
+	   MTU problem. */
+
+	/* Another hack: avoid icmp_send in ip_fragment */
+	skb->local_df = 1;
+
+	IP_VS_XMIT(PF_INET, skb, rt);
+
+	LeaveFunction(10);
+	return NF_STOLEN;
+
+      tx_error_icmp:
+	dst_link_failure(skb);
+      tx_error:
+	LeaveFunction(10);
+	kfree_skb(skb);
+	return NF_STOLEN;
+      tx_error_put:
+	ip_rt_put(rt);
+	goto tx_error;
+}
+
+#ifdef CONFIG_IP_VS_IPV6
+int
+ip_vs_fnat_xmit_v6(struct sk_buff *skb, struct ip_vs_conn *cp,
+		   struct ip_vs_protocol *pp)
+{
+	struct rt6_info *rt;	/* Route to the other host */
+	int mtu;
+
+	EnterFunction(10);
+
+	/* check if it is a connection of no-client-port */
+	if (unlikely(cp->flags & IP_VS_CONN_F_NO_CPORT)) {
+		__be16 _pt, *p;
+		p = skb_header_pointer(skb, sizeof(struct ipv6hdr),
+				       sizeof(_pt), &_pt);
+		if (p == NULL)
+			goto tx_error;
+		ip_vs_conn_fill_cport(cp, *p);
+		IP_VS_DBG(10, "filled cport=%d\n", ntohs(*p));
+	}
+
+	rt = __ip_vs_get_out_rt_v6(cp);
+	if (!rt)
+		goto tx_error_icmp;
+
+	/* MTU checking */
+	mtu = dst_mtu(&rt->u.dst);
+	if (!gso_ok(skb, rt->u.dst.dev) && (skb->len > mtu)) {
+		dst_release(&rt->u.dst);
+		IP_VS_INC_ESTATS(ip_vs_esmib, XMIT_UNEXPECTED_MTU);
+		icmpv6_send(skb, ICMPV6_PKT_TOOBIG, 0, mtu, skb->dev);
+		IP_VS_DBG_RL_PKT(0, pp, skb, 0,
+				 "ip_vs_fnat_xmit_v6(): frag needed for");
+		goto tx_error;
+	}
+
+	ip_vs_save_xmit_info(skb, pp, cp);
+
+	/* copy-on-write the packet before mangling it */
+	if (!skb_make_writable(skb, sizeof(struct ipv6hdr)))
+		goto tx_error_put;
+
+	if (skb_cow(skb, rt->u.dst.dev->hard_header_len))
+		goto tx_error_put;
+
+	/* drop old route */
+	skb_dst_drop(skb);
+	skb_dst_set(skb, &rt->u.dst);
+
+	/* mangle the packet */
+	if (pp->fnat_in_handler && !pp->fnat_in_handler(&skb, pp, cp))
+		goto tx_error;
+	ipv6_hdr(skb)->saddr = cp->laddr.in6;
+	ipv6_hdr(skb)->daddr = cp->daddr.in6;
+
+	IP_VS_DBG_PKT(10, pp, skb, 0, "After FNAT-IN");
+
+	/* FIXME: when application helper enlarges the packet and the length
+	   is larger than the MTU of outgoing device, there will be still
+	   MTU problem. */
+
+	/* Another hack: avoid icmp_send in ip_fragment */
+	skb->local_df = 1;
+
+	IP_VS_XMIT(PF_INET6, skb, rt);
+
+	LeaveFunction(10);
+	return NF_STOLEN;
+
+      tx_error_icmp:
+	dst_link_failure(skb);
+      tx_error:
+	LeaveFunction(10);
+	kfree_skb(skb);
+	return NF_STOLEN;
+      tx_error_put:
+	dst_release(&rt->u.dst);
+	goto tx_error;
+}
+#endif
 
 /*
  *   IP Tunneling transmitter
@@ -563,6 +1642,7 @@ ip_vs_tunnel_xmit(struct sk_buff *skb, struct ip_vs_conn *cp,
 
 	if ((old_iph->frag_off & htons(IP_DF))
 	    && mtu < ntohs(old_iph->tot_len)) {
+		IP_VS_INC_ESTATS(ip_vs_esmib, XMIT_UNEXPECTED_MTU);
 		icmp_send(skb, ICMP_DEST_UNREACH,ICMP_FRAG_NEEDED, htonl(mtu));
 		ip_rt_put(rt);
 		IP_VS_DBG_RL("%s(): frag needed\n", __func__);
@@ -672,6 +1752,7 @@ ip_vs_tunnel_xmit_v6(struct sk_buff *skb, struct ip_vs_conn *cp,
 		skb_dst(skb)->ops->update_pmtu(skb_dst(skb), mtu);
 
 	if (mtu < ntohs(old_iph->payload_len) + sizeof(struct ipv6hdr)) {
+		IP_VS_INC_ESTATS(ip_vs_esmib, XMIT_UNEXPECTED_MTU);
 		icmpv6_send(skb, ICMPV6_PKT_TOOBIG, 0, mtu, skb->dev);
 		dst_release(&rt->u.dst);
 		IP_VS_DBG_RL("%s(): frag needed\n", __func__);
@@ -761,6 +1842,7 @@ ip_vs_dr_xmit(struct sk_buff *skb, struct ip_vs_conn *cp,
 	/* MTU checking */
 	mtu = dst_mtu(&rt->u.dst);
 	if ((iph->frag_off & htons(IP_DF)) && skb->len > mtu) {
+		IP_VS_INC_ESTATS(ip_vs_esmib, XMIT_UNEXPECTED_MTU);
 		icmp_send(skb, ICMP_DEST_UNREACH,ICMP_FRAG_NEEDED, htonl(mtu));
 		ip_rt_put(rt);
 		IP_VS_DBG_RL("%s(): frag needed\n", __func__);
@@ -814,6 +1896,7 @@ ip_vs_dr_xmit_v6(struct sk_buff *skb, struct ip_vs_conn *cp,
 	/* MTU checking */
 	mtu = dst_mtu(&rt->u.dst);
 	if (skb->len > mtu) {
+		IP_VS_INC_ESTATS(ip_vs_esmib, XMIT_UNEXPECTED_MTU);
 		icmpv6_send(skb, ICMPV6_PKT_TOOBIG, 0, mtu, skb->dev);
 		dst_release(&rt->u.dst);
 		IP_VS_DBG_RL("%s(): frag needed\n", __func__);
@@ -869,7 +1952,8 @@ ip_vs_icmp_xmit(struct sk_buff *skb, struct ip_vs_conn *cp,
 	/* The ICMP packet for VS/TUN, VS/DR and LOCALNODE will be
 	   forwarded directly here, because there is no need to
 	   translate address/port back */
-	if (IP_VS_FWD_METHOD(cp) != IP_VS_CONN_F_MASQ) {
+	if ((IP_VS_FWD_METHOD(cp) != IP_VS_CONN_F_MASQ) &&
+	    (IP_VS_FWD_METHOD(cp) != IP_VS_CONN_F_FULLNAT)) {
 		if (cp->packet_xmit)
 			rc = cp->packet_xmit(skb, cp, pp);
 		else
@@ -890,6 +1974,7 @@ ip_vs_icmp_xmit(struct sk_buff *skb, struct ip_vs_conn *cp,
 	mtu = dst_mtu(&rt->u.dst);
 	if ((skb->len > mtu) && (ip_hdr(skb)->frag_off & htons(IP_DF))) {
 		ip_rt_put(rt);
+		IP_VS_INC_ESTATS(ip_vs_esmib, XMIT_UNEXPECTED_MTU);
 		icmp_send(skb, ICMP_DEST_UNREACH, ICMP_FRAG_NEEDED, htonl(mtu));
 		IP_VS_DBG_RL("%s(): frag needed\n", __func__);
 		goto tx_error;
@@ -943,7 +2028,8 @@ ip_vs_icmp_xmit_v6(struct sk_buff *skb, struct ip_vs_conn *cp,
 	/* The ICMP packet for VS/TUN, VS/DR and LOCALNODE will be
 	   forwarded directly here, because there is no need to
 	   translate address/port back */
-	if (IP_VS_FWD_METHOD(cp) != IP_VS_CONN_F_MASQ) {
+	if ((IP_VS_FWD_METHOD(cp) != IP_VS_CONN_F_MASQ) &&
+	    (IP_VS_FWD_METHOD(cp) != IP_VS_CONN_F_FULLNAT)) {
 		if (cp->packet_xmit)
 			rc = cp->packet_xmit(skb, cp, pp);
 		else
@@ -965,6 +2051,7 @@ ip_vs_icmp_xmit_v6(struct sk_buff *skb, struct ip_vs_conn *cp,
 	mtu = dst_mtu(&rt->u.dst);
 	if (skb->len > mtu) {
 		dst_release(&rt->u.dst);
+		IP_VS_INC_ESTATS(ip_vs_esmib, XMIT_UNEXPECTED_MTU);
 		icmpv6_send(skb, ICMPV6_PKT_TOOBIG, 0, mtu, skb->dev);
 		IP_VS_DBG_RL("%s(): frag needed\n", __func__);
 		goto tx_error;
